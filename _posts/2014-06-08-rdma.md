---
layout: post
title: Remote Direct Memory Access
date: 2014-06-08
categories: [computer science]
tags: [parallel computing]

---

## Article Source
* Title: [Remote Direct Memory Access](http://en.wikipedia.org/wiki/Remote_direct_memory_access)
* Authors:  Wikipedia, the free encyclopedia
* 
[![](http://sungsoo.github.com/images/rdma.png)](http://sungsoo.github.com/images/rdma.png)

---

# Accelerating Big Data over RDMA

In this video from the 2013 Open Fabrics Developer Workshop, Sreev Doddabalapur from Mellanox presents: Accelerating Big Data over RDMA.

<iframe width="600" height="400" src="//www.youtube.com/embed/n-FInuf0GB0" frameborder="0" allowfullscreen></iframe>

---

# Remote Direct Memory Access

In [computing](http://en.wikipedia.org/wiki/Computing "Computing"), **remote direct memory
access** (**RDMA**) is a [direct memory
access](http://en.wikipedia.org/wiki/Direct_memory_access "Direct memory access") from the
[memory](http://en.wikipedia.org/wiki/Main_memory "Main memory") of one computer into that of
another without involving either one's [operating
system](http://en.wikipedia.org/wiki/Operating_system "Operating system"). This permits
high-throughput,
low-[latency](http://en.wikipedia.org/wiki/Latency_(engineering) "Latency (engineering)")
networking, which is especially useful in massively parallel [computer
clusters](http://en.wikipedia.org/wiki/Computer_cluster "Computer cluster").


## Advantages


RDMA supports [zero-copy](http://en.wikipedia.org/wiki/Zero-copy "Zero-copy") networking by
enabling the [network adapter](http://en.wikipedia.org/wiki/Network_adapter "Network adapter")
to transfer data directly to or from application memory, eliminating the
need to copy data between application memory and the data buffers in the
operating system. Such transfers require no work to be done by
[CPUs](http://en.wikipedia.org/wiki/Central_processing_unit "Central processing unit"),
[caches](http://en.wikipedia.org/wiki/CPU_cache "CPU cache"), or [context
switches](http://en.wikipedia.org/wiki/Context_switch "Context switch"), and transfers continue
in parallel with other system operations. When an application performs
an RDMA Read or Write request, the application data is delivered
directly to the network, reducing latency and enabling fast message
transfer.


## Disadvantages

This strategy presents several problems related to the fact that the
target node is not notified of the completion of the request (1-sided
communications). The common way to notify it is to change a memory byte
when the data has been delivered, but it requires the target to poll on
this byte. Not only does this polling consume CPU cycles, but the memory
footprint and the latency increases linearly with the number of possible
other nodes, which limits use of RDMA in [High-Performance Computing
(HPC)](http://en.wikipedia.org/wiki/High-performance_computing "High-performance computing").

RDMA reduces [network
protocol](http://en.wikipedia.org/wiki/Protocol_(computing) "Protocol (computing)") overhead,
leading to improvements in communication latency. Reductions in protocol
overhead can increase a network's ability to [move data
quickly](http://en.wikipedia.org/wiki/Network_performance "Network performance"), allowing
applications to get the data they need faster, in turn leading to more
scalable [clusters](http://en.wikipedia.org/wiki/Computer_cluster "Computer cluster"). However,
one must be aware of the tradeoff between this reduction in network
protocol overhead and additional overhead that may be incurred on each
node due to the need for [pinning virtual memory
pages](http://en.wikipedia.org/wiki/Virtual_memory#Permanently_resident_pages "Virtual memory").
In particular, [zero-copy](http://en.wikipedia.org/wiki/Zero-copy "Zero-copy") RDMA protocols
require that the memory pages involved in a transaction be pinned, at
least for the duration of the transfer. If this is not done, RDMA pages
might be [paged out to disk](http://en.wikipedia.org/wiki/Paging "Paging") and replaced with
other data by the operating system, causing the
[DMA](http://en.wikipedia.org/wiki/Direct_memory_access "Direct memory access") engine (which
knows nothing of the virtual memory system maintained by the operating
system) to send the wrong data. The net result of not pinning the pages
in a zero-copy RDMA system can be corruption of the contents of memory
in the distributed system. Pinning memory takes time and additional
memory to set up, reduces the quantity of memory the operating system
can allocate to processes, limits the overall flexibility of the memory
system to adapt over time, and could even lead to underutilization of
memory if processes unnecessarily pin pages. The net result is the
introduction of latency, sometimes in linear proportion to the number of
pages of data pinned in memory. In order to mitigate these problems,
several techniques for interfacing with RDMA devices were developed:

-   using caching techniques to keep data pinned as long as possible,
    producing overhead reductions for applications that repeatedly
    communicate in the same memory area
-   pipelining memory pinning operations and data transfer (as done on
    [Infiniband](http://en.wikipedia.org/wiki/Infiniband "Infiniband") or
    [Myrinet](http://en.wikipedia.org/wiki/Myrinet "Myrinet"))
-   deferring memory pinning out of the critical path, thus hiding the
    latency increase
-   entirely removing the need for pinning (as
    [Quadrics](http://en.wikipedia.org/wiki/Quadrics "Quadrics") does)

In contrast, the [Send/Recv
model](/w/index.php?title=Send/Recv_model&action=edit&redlink=1 "Send/Recv model (page does not exist)")
used by other [zero-copy](http://en.wikipedia.org/wiki/Zero-copy "Zero-copy") HPC
interconnects, such as [Myrinet](http://en.wikipedia.org/wiki/Myrinet "Myrinet") or
[Quadrics](http://en.wikipedia.org/wiki/Quadrics "Quadrics"), does not have the 1-sided
communication problem or the memory paging problem described above, yet
provides comparable reductions in latency when used in conjunction with
HPC communication frameworks that expose the Send/Recv model to the
programmer (such as
[MPI](http://en.wikipedia.org/wiki/Message_Passing_Interface "Message Passing Interface")).


## Acceptance
Much like other HPC interconnects, RDMA has achieved limited acceptance
as of
2013
due to the need to install a different networking infrastructure.
However, new
standards
enable [Ethernet](http://en.wikipedia.org/wiki/Ethernet "Ethernet") RDMA implementation at the
physical layer using
[TCP](http://en.wikipedia.org/wiki/Transmission_Control_Protocol "Transmission Control Protocol")/[IP](http://en.wikipedia.org/wiki/Internet_Protocol "Internet Protocol")
as the transport, thus combining the performance and latency advantages
of RDMA with a low-cost, standards-based
solution.
The RDMA Consortium and the DAT Collaborative have
played key roles in the development of RDMA protocols and
[APIs](http://en.wikipedia.org/wiki/Application_programming_interface "Application programming interface")
for consideration by standards groups such as the [Internet Engineering
Task
Force](http://en.wikipedia.org/wiki/Internet_Engineering_Task_Force "Internet Engineering Task Force")
and the Interconnect Software Consortium.

Hardware vendors have started working on higher-capacity RDMA-based
network adapters, with rates of 40Gbit/s
reported. Software vendors
such as [Red Hat](http://en.wikipedia.org/wiki/Red_Hat "Red Hat") and [Oracle
Corporation](http://en.wikipedia.org/wiki/Oracle_Corporation "Oracle Corporation") support
these APIs in their latest products, and as of
2013
engineers have started developing network adapters that implement RDMA
over
Ethernet.
Both [Red Hat Enterprise
Linux](http://en.wikipedia.org/wiki/Red_Hat_Enterprise_Linux "Red Hat Enterprise Linux") and
[Red Hat Enterprise
MRG](http://en.wikipedia.org/wiki/Red_Hat_Enterprise_MRG "Red Hat Enterprise MRG")
have support for RDMA. Microsoft supports RDMA in Windows Server 2012
via [SMB Direct](http://en.wikipedia.org/wiki/Server_Message_Block "Server Message Block").

Common RDMA implementations include the [Virtual Interface
Architecture](http://en.wikipedia.org/wiki/Virtual_Interface_Architecture "Virtual Interface Architecture"),
[InfiniBand](http://en.wikipedia.org/wiki/InfiniBand "InfiniBand"), and
[iWARP](http://en.wikipedia.org/wiki/IWARP "IWARP").

## Notes
1. [DAT Collaborative
    website](http://www.datcollaborative.org/)
2.  [The Interconnect Software Consortium
    website](http://www.opengroup.org/icsc/)
3.  [http://www.mellanox.com/page/file\_storage/](http://www.mellanox.com/page/file_storage/)
4.  [http://www.chelsio.com/chelsio-to-demonstrate-40g-smb-direct-rdma-over-ethernet-for-windows-server-2012/](http://www.chelsio.com/chelsio-to-demonstrate-40g-smb-direct-rdma-over-ethernet-for-windows-server-2012/)
5.  [http://www.redhat.com/mrg](http://www.redhat.com/mrg)

## External links
-   [RDMA Consortium](http://www.rdmaconsortium.org/home)
-   [InfiniBand Trade Association](http://www.infinibandta.org/home)
-   [A Tutorial of the RDMA
    Model](http://www.hpcwire.com/features/17887604.html)
-   [RDMA usage](http://www.hpcwire.com/features/17888274.html)
-   [A Critique of
    RDMA](http://www.hpcwire.com/hpcwire/2006-08-18/a_critique_of_rdma-1.html)
    for High-Performance Computing
-   [OpenFabrics Alliance](http://www.openfabrics.org)