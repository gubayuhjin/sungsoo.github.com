---
layout: post
title: Learning Sound Representations from Unlabeled Video
date: 2017-03-15
categories: [computer science]
tags: [machine learning]

---


## Article Source
* Title: [SoundNet: Learning Sound Representations from Unlabeled Video](https://projects.csail.mit.edu/soundnet/)
* Authors: Yusuf Aytar, Carl Vondrick, Antonio Torralba (MIT)

---





SoundNet: Learning Sound Representations from Unlabeled Video
====================================

<span>[Yusuf Aytar](http://people.csail.mit.edu/yusuf/) *</span>
<span>[Carl Vondrick](http://mit.edu/vondrick) *</span> <span>[Antonio
Torralba](http://web.mit.edu/torralba/www/)</span>
Massachusetts Institute of Technology
NIPS 2016
* contributed equally


[![](https://projects.csail.mit.edu/soundnet/paper.png) **Download Paper**](https://arxiv.org/pdf/1610.09001.pdf)



Abstract
--------

We learn *rich natural sound representations* by capitalizing on large
amounts of unlabeled sound data collected in the wild. We leverage the
natural synchronization between vision and sound to learn an acoustic
representation using two-million unlabeled videos. Unlabeled video has
the advantage that it can be economically acquired at massive scales,
yet contains useful signals about natural sound. We propose a
student-teacher training procedure which transfers discriminative visual
knowledge from well established visual recognition models into the sound
modality using unlabeled video as a bridge. Our sound representation
yields significant performance improvements over the state-of-the-art
results on standard benchmarks for acoustic scene/object classification.
Visualizations suggest some high-level semantics automatically emerge in
the sound network, even though it is trained without ground truth
labels.

