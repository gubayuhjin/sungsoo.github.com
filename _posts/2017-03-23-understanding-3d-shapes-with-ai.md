---
layout: post
title: Understanding 3D Shapes With AI
date: 2017-03-23
categories: [computer science]
tags: [machine learning]

---


## Article Source
* Title: [Shape2vec: Understanding 3D Shapes With AI](https://www.youtube.com/watch?v=bB54Wz4kq0E&spfreload=10)

---


Shape2vec: Understanding 3D Shapes With AI 
==============

The paper "Shape2Vec: semantic-based descriptors for 3D shapes, sketches and images" is available here:

* [http://www.cl.cam.ac.uk/research/rainbow/projects/shape2vec/](http://www.cl.cam.ac.uk/research/rainbow/projects/shape2vec/)

Code (coming soon according to the authors): [https://github.com/ftasse/Shape2Vec](https://github.com/ftasse/Shape2Vec)

<iframe width="600" height="400" src="https://www.youtube.com/embed/bB54Wz4kq0E" frameborder="0" allowfullscreen></iframe>


## Shape2Vec: semantic-based descriptors for 3D shapes, sketches and images

## Abstract

Convolutional neural networks have been successfully used to compute shape descriptors, or jointly embed shapes and sketches in a common vector space. We propose a novel approach that leverages both *labeled 3D shapes* and *semantic information* contained in the labels, to generate semantically-meaningful shape descriptors. A neural network is trained to generate shape descriptors that lie close to a *vector representation* of the shape class, given a vector space of words. This method is easily extendable to *range scans*, *hand-drawn sketches* and *images*. This makes *cross-modal retrieval* possible, without a need to design different methods depending on the query type. We show that sketch-based shape retrieval using semantic-based descriptors outperforms the state-of-the-art by large margins, and mesh-based retrieval generates results of higher relevance to the query, than current deep shape descriptors.

<iframe width="600" height="400" src="https://www.youtube.com/embed/oVR4af9UWio" frameborder="0" allowfullscreen></iframe>


