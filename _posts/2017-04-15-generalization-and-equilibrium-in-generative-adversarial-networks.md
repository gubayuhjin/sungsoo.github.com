---
layout: post
title:  Generalization and Equilibrium in Generative Adversarial Networks
date: 2017-04-15
categories: [computer science]
tags: [machine learning]

---

## Article Source
* Title: [Generalization and Equilibrium in Generative Adversarial Nets](https://www.youtube.com/watch?v=V7TliSCqOwI&spfreload=10)
* Authors: [Sanjeev Arora, Princeton University](http://www.cs.princeton.edu/~arora/)

---


# Generalization and Equilibrium in Generative Adversarial Networks

## Abstract

This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective).

Finally, the above theoretical ideas lead us to propose a new training protocol, MIX+GAN, which can be combined with any existing method. We present experiments showing that it stabilizes and improves some existing methods.

<iframe width="600" height="400" src="https://www.youtube.com/embed/V7TliSCqOwI" frameborder="0" allowfullscreen></iframe>


Joint work with Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang. 

[Download Video [2.2 GB .mp4]](https://video.simons.berkeley.edu/2017/spring/ml/2/23-Arora.mp4)