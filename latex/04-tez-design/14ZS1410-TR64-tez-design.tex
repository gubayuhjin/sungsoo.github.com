
\documentclass[twocolumn]{article}
\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{times}
\usepackage{titlesec} % 1
%\usepackage{sectsty} % "제 1 절" ...

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %                              My Commands
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\ii}{\item}
\newtheorem{Def}{Definition}
\newtheorem{Lem}{Lemma}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{graphicx}
\graphicspath{%
        {converted_graphics/}
        {./images/}
}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{longtable}
% the following is needed for syntax highlighting
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{ %
  language=Java,                  % the language of the code
  basicstyle=\scriptsize,       % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=3.8pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=false,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  stringstyle=\color{mauve},         % string literal style
  escapeinside={\%*}{*)},            % if you want to add a comment within your code
  morekeywords={*,...}               % if you want to add more keywords to the set
}

%\usepackage[hangul,nonfrench,finemath]{kotex}
    
\setlength\textwidth{7in} 
\setlength\textheight{9.5in} 
\setlength\oddsidemargin{-0.25in} 
\setlength\topmargin{-0.25in} 
\setlength\headheight{0in} 
\setlength\headsep{0in} 
\setlength\columnsep{9pt}
\sloppy 
 
\begin{document}

\title{
\vspace{-0.5in}\rule{\textwidth}{2pt}
\begin{tabular}{ll}\begin{minipage}{4.75in}\vspace{6px}
\noindent\large {\it KIWI Project}@Data Management Research Section\\
\vspace{-12px}\\
\noindent\LARGE ETRI\qquad  \large Technical Report 14ZS1410-TR-65
\end{minipage}&\begin{minipage}{2in}\vspace{6px}\small
218 Gajeong-ro, Yuseong-gu\\
Daejeon, 305-700, South Korea\\
http:/$\!$/www.etri.re.kr/\\
http:/$\!$/sungsoo.github.com/\quad 
\end{minipage}\end{tabular}
\rule{\textwidth}{2pt}\vspace{0.25in}
\LARGE \bf Analysis of the Apache Tez Design
}

\date{}

\author{
{\bf Sung-Soo Kim}\\
\it{sungsoo@etri.re.kr}
}

\maketitle

\begin{abstract}
{\small
MapReduce has served us well. For years it has been the processing
engine for Hadoop and has been the backbone upon which a huge amount of
value has been created. While it is here to stay, new paradigms are also
needed in order to enable Hadoop to serve an even greater number of
usage patterns. A key and emerging example is the need for
\emph{interactive query}, which today is challenged by the
\emph{batch-oriented nature} of MapReduce. A key step to enabling this
new world was \emph{Apache YARN} and today the community proposes the
next step\ldots{} \emph{Tez}.

This report amis to analyze the insights of the design aspects of the Apache Tez.
In exploring the questions of the designing an efficient distributed execution engine such as the Tez, this technical report will be limited to consideration of major capabilities and  their application programming interface (API) structures of the DAG execution engine. 
}
\end{abstract}

\section{Introduction}
Apache Hadoop 2.0 (aka \textbf{YARN}) continues to make its way through the open source community process at the Apache Software Foundation and is getting closer to being declared “ready” from a community development perspective.  YARN on its own provides many benefits over Hadoop 1.x and its Map-Reduce job execution engine:
\bi\ii Concurrent cluster applications via \textit{multiple independent AppMasters}\ii Reduced job \textit{startup overheads}\ii \textit{Pluggable} scheduling policy framework\ii Improved \textit{security} framework\ei

\subsection{Motivation}

\emph{Distributed data processing} is the core application that Apache
Hadoop is built around \cite{FOWLER:2013}.  Storing and analyzing \emph{large volumes} and
\emph{variety} of data efficiently has been the cornerstone use case
that has driven large scale adoption of Hadoop, and has resulted in
creating enormous value for the Hadoop adopters. Over the years, while
building and running data processing applications based on MapReduce, we
have understood a lot about the strengths and weaknesses of this
framework and how we would like to evolve the \emph{Hadoop data
processing framework} to meet the evolving needs of Hadoop users. 

The common behavior of a Map-Reduce job under Hadoop 1.x is as follows.
\be
\ii Client-side determination of input pieces\ii Job startup\ii Map phase, with optional in-process combiner \\Each mapper reads input from durable storage\ii Hash partition with local per-bucket sort.\ii Data movement via framework initiated by reduce-side pull mechanism\ii Ordered merge\ii Reduce phase\ii Write to durable storage
\ee

As the Hadoop compute platform moves into its next phase with \textbf{YARN}, it
has decoupled itself from MapReduce being the only application, and
opened the opportunity to create a new data processing framework to meet
the new challenges. Apache Tez aspires to live up to these lofty goals.
Fundamentally, YARN resource scheduling is a \textbf{2-step framework} with \textit{resource allocation} done by YARN and \textit{task scheduling} done by the application. This allows YARN to be a generic compute platform while still allowing flexibility of scheduling strategies. An analogy would be general purpose operating systems that allocate computer resources among concurrent processes.

The support for third party AppMasters is the crucial aspect to flexibility in YARN.  It permits new job runtimes in addition to classical map-reduce, whilst still keeping M/R available and allowing both the old and new to co-exist on a single cluster.  \textbf{Apache Tez} is one such job runtime that provides richer capabilities than traditional map-reduce \cite{hortonworks}.  The motivation is to provide a better runtime for scenarios such as relational-querying that do not have a strong affinity for the map-reduce primitive.   This need arises because the Map-Reduce primitive mandates a very particular shape to every job and although this mandatory shape is very general and can be used to implement essentially any batch-oriented data processing job, it conflates too many details and provides too little flexibility.

The map-reduce primitive has proved to be very useful as the basis of a reliable cluster computation runtime and it is well suited to data processing tasks that involve a small number of jobs that benefit from the standard behavior.  However, algorithms that require \textit{many iterations} suffer from the \textit{high overheads of job startup} and from frequent reads and writes to durable storage.  \textit{Relation query languages} such as Hive suffer from those issues and from the need to massage multiple datasets into homogeneous inputs as a M/R job can only consume one physical dataset (excluding support for side-data channels such as \textit{distributed cache}).  

The \textbf{YARN Resource Manager} service is the central controlling authority for resource management and makes allocation decisions as shown in Figure \ref{yarn}. It exposes a Scheduler API that is specifically designed to negotiate resources and not schedule tasks. Applications can request resources at different layers of the cluster topology such as nodes, racks etc. The scheduler determines how much and where to allocate based on resource availability and the configured sharing policy.
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{rm}
        \caption{The resource manager in the YARN.}
        \label{yarn}
\end{figure}

Currently, there are two sharing policies – \textit{fair scheduling} and \textit{capacity scheduling}. Thus, the API reflects the Resource Manager’s role as the resource allocator. This API design is also crucial for Resource Manager scalability because it limits the complexity of the operations to the size of the cluster and not the size of the tasks running on the cluster.The actual task scheduling decisions are delegated to the application manager that runs the application logic. It decides when, where and how many tasks to run within the resources allocated to it. It has the flexibility to choose its locality, co-scheduling, co-location and other scheduling strategies.

\subsection{What is Tez?}

\textbf{Tez} -- Hindi for ``\emph{speed}'' provides a general-purpose, highly customizable
framework that creates simplifies data-processing tasks across both
small scale (low-latency) and large-scale (high throughput) workloads in
Hadoop \cite{Saha:2013-01}. It generalizes the MapReduce paradigm to a more powerful
framework by providing the ability to execute a complex \textbf{DAG}
(\emph{directed acyclic graph}) of tasks for a single job so that
projects in the Apache Hadoop ecosystem such as Apache Hive, Apache Pig
and Cascading can meet requirements for human-interactive response times
and extreme throughput at petabyte scale (clearly MapReduce has been a
key driver in achieving this).

%\subsection{What Tez Does}
Tez is the logical next step for Apache Hadoop after \textbf{Apache
Hadoop YARN}. With YARN the community generalized Hadoop MapReduce to
provide a \emph{general-purpose resource management framework} wherein
MapReduce became merely one of the applications that could process data
in a Hadoop cluster. Tez provides a more general data-processing
application to the benefit of the entire ecosystem.

Tez will speed Pig and Hive workloads by an order of magnitude. By
eliminating unnecessary tasks, synchronization barriers, and reads from
and write to HDFS, Tez speeds up data processing across both
small-scale, low-latency and large-scale, high-throughput workloads.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{hadoopstack}
        \caption{\textit{Monolithic} Hadoop 1.0 vs. \textit{Layered} Hadoop 2.0}
        \label{fig01}
\end{figure}

With the emergence of Apache Hadoop YARN as the basis of next generation
data-processing architectures, there is a strong need for an application
which can execute a complex DAG of tasks which can then be shared by
Apache Pig, Apache Hive, Cascading and others as shown in Figure \ref{fig01}. The constrained DAG
expressible in MapReduce (one set of maps followed by one set of
reduces) often results in multiple MapReduce jobs which harm latency for
short queries (overhead of launching multiple jobs) and throughput for
large-scale queries (too much overhead for materializing intermediate
job outputs to the filesystem). With Tez, we introduce a more expressive
DAG of tasks, within a single application or job, that is better aligned
with the required processing task -- thus, for e.g., any \emph{given SQL
query can be expressed as a single job} using Tez.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{pighivetez.png}
        \caption{General example of a Map-Reduce execution plan compared to DAG execution plan}
        \label{fig02}
\end{figure}

The usefulness of DAG plans to \textit{relational query processing} is well known and has been explored in research and mission-critical systems (various SQL engines, Dryad, etc.).   The flexibility of\textit{ DAG-style execution} plans makes them useful for various data processing tasks such as iterative batch computing.
Figure \ref{fig02} illustrates the advantages provided by Tez for complex
SQL queries in Apache Hive or complex Apache Pig scripts.



Tez is critical to the
\href{http://hortonworks.com/blog/100x-faster-hive}{\textbf{Stinger
Initiative}} and goes a long way in helping Hive support both
interactive queries and batch queries. Tez provides a single underlying
framework to support both latency and throughput sensitive applications,
there-by obviating the need for multiple frameworks and systems to be
installed, maintained and supported, a \emph{key advantage to
enterprises looking to rationalize their data architectures}.

Essentially, Tez is the logical next step for Apache Hadoop after Apache
Hadoop YARN. With YARN the community generalized Hadoop MapReduce to
provide a
\href{http://hortonworks.com/blog/introducing-apache-hadoop-yarn/}{general-purpose
resource management framework}(YARN) where-in MapReduce became merely
\emph{one of the applications} that could process data in your Hadoop
cluster. With Tez, we build on YARN and our experience with the
MapReduce to provide a more general data-processing application to the
benefit of the entire ecosystem i.e. Apache Hive, Apache Pig etc.

Major community achievements in the recent Tez  were:
\begin{itemize}
\item
  \textbf{Application Recovery} -- This is a major improvement to the
  Tez framework that preserves work when the job controller (YARN Tez
  Application Master) gets restarted due to node loss or cluster
  maintenance. When the Tez Application Master restarts, it will recover
  all the work that was already completed by the previous master. This
  is especially useful for long running jobs where restarting from
  scratch would waste work already completed.
\item
  \textbf{Stability for Hive on Tez} -- We did considerable testing with
  the Apache Hive community to make sure the imminent release of Hive
  0.13 is stable on Tez. We appreciate the great partnership.
\item
  \textbf{Data Shuffle Improvements}-- Data shuffling re-partitions and
  re-distributes data across the cluster. This is a major operation in
  distributed data processing, so performance and stability are
  important. Tez 0.4 includes improvements in memory consumption,
  connection management, and in the handling of errors and empty
  partitions.
\item
  \textbf{Windows Support} -- The community fixed bugs and made changes
  to Tez so that it runs as smoothly on Windows as it does on Linux. We
  hope this will encourage adoption of Tez on Windows-based systems.
\end{itemize}

\section{Key Design Themes}

Higher-level data processing applications like Hive and Pig need an
\textit{execution framework} that can express their complex query logic in an
efficient manner and then execute it with \textit{high performance}. 
Apache Tez has been built around the following main design themes that solve these
key challenges in the Hadoop data processing domain.

%\subsection{Ability to express, model and execute data processing logic}
\subsection{Dataflow Graph Representation}
\textbf{Expressing the Computation:} Tez models data processing as a \emph{dataflow graph} with vertices in
the graph representing \emph{application logic} and edges representing
\emph{movement of data}. A rich dataflow definition API allows users to
express \emph{complex query logic} in an intuitive manner and it is a
natural fit for \emph{query plans} produced by higher-level declarative
applications like \emph{Hive} and \emph{Pig}. 

As an example, Figure \ref{fig03} shows how to model an \emph{ordered distributed sort} using
\emph{range partitioning}. The \emph{Preprocessor} stage sends samples
to a \emph{Sampler} that calculates sorted data ranges for each data
partition such that the work is \emph{uniformly distributed}. The ranges
are sent to \emph{Partition} and \emph{Aggregate} stages that read
their assigned ranges and perform the data \emph{scatter-gather}. 

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez1}
        \caption{The representation of the data processing flow}
        \label{fig03}
\end{figure}


This dataflow pipeline can be expressed as a single Tez job that will run the
entire computation. Expanding this logical graph into a physical graph
of tasks and executing it is taken care of by Tez.
Tez provides the following APIs to define the processing.

\bi
\ii \textbf{DAG API}
\bi
\ii Defines the structure of the data processing and the relationship between producers and consumers.
\ii Enable definition of complex data flow pipelines using simple graph connection API’s. Tez expands the logical DAG at runtime.
\ii This is how all the tasks in the job get specified.
\ei
\ii \textbf{Runtime API}
\bi
\ii Defines the interfaces using which the framework and application code interact with each other.
\ii Application code transforms data and moves it between tasks.
\ii This is how we specify what actually executes in each task on the cluster nodes.
\ei
\ei

\subsection{Flexible Task Model}
Tez models the user logic running in each vertex of the dataflow graph
as a composition of \emph{Input}, \emph{Processor} and
\emph{Output} modules. Input \& Output determine the \emph{data
format} and how and where it is read/written. \emph{Processor} holds the
\emph{data transformation} logic. Tez does not impose any data format
and only requires that a combination of Input, Processor and Output must
be compatible with each other with respect to their formats when they
are composed to instantiate a \emph{vertex task}. Similarly, an Input
and Output pair connecting two tasks should be compatible with each
other. In Figure \ref{fig04}, we can see how composing different Inputs,
Outputs and Processors can produce different tasks.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez2}
        \caption{Flexible Input-Processor-Output runtime model}
        \label{fig04}
\end{figure}

\subsection{Dynamic Graph Reconfiguration}

\textbf{Late Binding:} Distributed data processing is \emph{dynamic} by nature and it is
extremely difficult to statically determine \emph{optimal concurrency}
and \emph{data movement methods} a priori. More information is available
during runtime, like data samples and sizes, which may help optimize the
\emph{execution plan} further. We also recognize that Tez by itself
cannot always have the smarts to perform these \emph{dynamic
optimizations}. 

The design of Tez includes support for \textit{pluggable} vertex
management modules to collect relevant information from tasks and change
the dataflow graph at runtime to optimize for performance and resource
usage. Figure \ref{fig05} shows how Tez can determine an appropriate number of
reducers in a MapReduce like job by observing the actual data output
produced and the desired load per reduce task.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez3}
        \caption{Plan Reconfiguration at Runtime}
        \label{fig05}
\end{figure}

\subsection{Optimal Resource Management}

Resources acquisition in a \emph{distributed multi-tenant} environment
is based on cluster capacity, load and other quotas enforced by the
\emph{resource management framework} like \textbf{YARN}. Thus resource
available to the user may vary over time and over different executions
of the job. It becomes paramount to be able to efficiently use all
available resources to run a job as fast as possible during one instance
of execution and predictably over different instances of execution. The

Tez execution engine framework allows for efficient acquisition of
resources from YARN along with \emph{extensive reuse} of every component
in the pipeline such that no operation is duplicated unnecessarily.
These efficiencies are exposed to user logic, where possible, such that
users may also leverage this for \emph{efficient caching} and avoid
\emph{work duplication}. Figure \ref{fig06} shows how Tez runs multiple
containers within the same YARN container host and how users can
leverage that to store their own objects that may be shared across
tasks.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez4}
        \caption{Optimal Resource Management}
        \label{fig06}
\end{figure}


%We hope this brief overview about the philosophy and design of
%\textbf{Apache Tez} will throw some light on the aspirations of the
%project and how we hope to work with the Apache Hadoop community to
%bring them to life. \textbf{Apache Hive} and \textbf{Apache Pig}
%projects have already show deep interest in integrating with Tez.

%In the next posts in this series, we'll dive further into the
%\textbf{DAG execution} architecture, and look at MapReduce atop Tez
%along with the associated performance benefits of that model.


\section{Data Processing API}
%\subsection{Overview}

Apache Tez models data processing as a \emph{dataflow graph}, with the
\textbf{vertices} in the graph representing \emph{processing of data}
and \textbf{edges} representing \emph{movement of data} between the
processing. Thus \emph{user logic}, that analyses and modifies the data,
sits in the \textbf{vertices}. Edges determine the consumer of the data,
how the data is transferred and the \emph{dependency} between the
\emph{producer} and \emph{consumer} vertices. This model concisely
captures the \emph{logical definition of the computation}. 

When the Tez
job executes on the cluster, it expands this \emph{logical graph} into a
\emph{physical graph} by adding parallelism at the vertices to scale to
the data size being processed. Multiple tasks are created per logical
vertex to perform the computation in parallel.

\noindent
\\
\textbf{DAG Definition API:} More technically, the data processing is expressed in the form of a
\emph{directed acyclic graph} (\textbf{DAG}). The processing starts at
the root vertices of the DAG and continues down the \emph{directed
edges} till it reaches the leaf vertices. When all the vertices in the
DAG have completed then the data processing job is done. The graph does
not have cycles because the \emph{fault tolerance mechanism} used by Tez
is \textbf{re-execution} of failed tasks. When the input to a task is
lost then the producer task of the input is re-executed and so Tez needs
to be able to \emph{walk up} the graph edges to locate a non-failed task
from which to re-start the computation. \emph{Cycles} in the graph can
make this work \emph{difficult} to perform. In some cases, cycles may be
handled by \emph{unrolling} them to create a DAG.

Tez defines a simple Java API to express a DAG of data processing \cite{Saha:2013-02}. 
The API has three components

\begin{itemize}
\item
  \textbf{DAG.} this defines the overall job. The user creates a DAG
  object for each data processing job.
\item
  \textbf{Vertex.} this defines the user logic and the resources \&
  environment needed to execute the user logic. The user creates a
  Vertex object for each step in the job and adds it to the DAG.
\item
  \textbf{Edge.} this defines the connection between producer and
  consumer vertices. The user creates an Edge object and connects the
  producer and consumer vertices using it.
\end{itemize}

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.45\textwidth]{tez11}
        \caption{A dataflow logical graph using the DAG API}
        \label{fig07}
\end{figure}

Figure \ref{fig07} shows a \emph{dataflow graph} and its definition using the
DAG API (simplified). The job consists of 2 vertices performing a
``\textbf{Map}'' operation on 2 datasets. Their output is consumed by 2
vertices that do a ``\textbf{Reduce}'' operation. Their output is
brought together in the last vertex that does a ``\textbf{Join}''
operation.

Tez handles expanding this \emph{logical graph} at runtime to perform
the operations \emph{in parallel} using multiple tasks. Figure \ref{fig08}
shows a runtime expansion in which the first M-R pair has a parallelism
of 2 while the second has a parallelism of 3. Both branches of
computation merge in the \textbf{Join operation} that has a parallelism
of 2. \emph{Edge properties} are at the heart of this runtime activity.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.49\textwidth]{tez21}
        \caption{Expanding the logical graph at runtime}
        \label{fig08}
\end{figure}

\subsection{Edge Properties}
The following edge properties enable Tez to instantiate the tasks,
configure their inputs and outputs, schedule them appropriately and help
\emph{route} the data between the tasks. The parallelism for each vertex
is determined based on \emph{user guidance}, \emph{data size} and
\emph{resources}.

\begin{itemize}
\item
  \textbf{Data movement.} Defines \emph{routing} of data between tasks
  \begin{itemize}
  \item
    \emph{One-To-One}: Data from the \emph{i}-th producer task routes to
    the \emph{i}-th consumer task.
  \item
    \emph{Broadcast}: Data from a producer task routes to \emph{all}
    consumer tasks.
  \item
    \emph{Scatter--Gather}: Producer tasks \emph{scatter} data into
    \emph{shards} and consumer tasks \emph{gather} the \emph{shards}.
    The \emph{i}-th shard from all producer tasks routes to the
    \emph{i}-th consumer task.
  \end{itemize}
\item
  \textbf{Scheduling.} Defines when a \emph{consumer} task is
  scheduled
  \begin{itemize}
  \item
    \emph{Sequential}: Consumer task may be scheduled after a
    \emph{producer task} completes.
  \item
    \emph{Concurrent}: Consumer task must be \emph{co-scheduled} with a
    producer task.
  \end{itemize}
\item
  \textbf{Data source.} Defines the \emph{lifetime}/\emph{reliability}
  of a task output
  \begin{itemize}
  \item
    \emph{Persisted}: Output will be available after the task exits.
    Output may be lost later on.
  \item
    \emph{Persisted-Reliable}: Output is reliably stored and will always
    be available
  \item
    \emph{Ephemeral}: Output is available only while the producer task
    is running
  \end{itemize}
\end{itemize}

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.18\textwidth]{tez31}
        \caption{Data movement patterns (\textit{one-to-one, broadcast, scatter-gather})}
        \label{fig09}
\end{figure}

Some real life use cases will help in clarifying the edge properties.
\emph{Mapreduce} would be expressed with the \emph{scatter-gather},
\emph{sequential} and \emph{persisted} edge properties. Map
tasks \emph{scatter} partitions and reduce tasks \emph{gather} them.
Reduce tasks are \emph{scheduled} after the map tasks complete
and the map task outputs are written to local disk and hence available
after the map tasks have completed. 

When a vertex \emph{checkpoints} its output into HDFS then its \emph{output edge} has a
\emph{persisted-reliable} property. If a producer vertex is
\emph{streaming data} directly to a consumer vertex then the edge
between them has \emph{ephemeral} and \emph{concurrent} properties. A
\emph{broadcast} property is used on a \emph{sampler vertex} that
produces a global histogram of data ranges for \emph{range
partitioning}.


\subsubsection*{DAG Topologies and Scenarios}
There are various DAG \textit{topologies} and associated dynamic strategies that aid efficient execution of jobs.   

\noindent
\\
\textbf{One-to-One Edge:}  A basic connection between job vertices that indication each Task in first stage has a 1:1 connection to Tasks in subsequent stage.This type of edge appears in a variety of scenarios such as in the \textit{hash-join plan}.  Job composition may also lead to graphs with 1-to-1 edges.  

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{one-to-one}
        \caption{One-to-One edge}
        \label{one-to-one}
\end{figure}

A \textit{One-to-All} edge indicates that each source task will produce data that is read by all of the destination tasks.  
There are a variety of ways to make this happen and so a One-to-All edge may have properties describing its exact behavior.Two important variants are described below.
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{one-to-all}
        \caption{One-to-All basic edge}
        \label{one-to-one}
\end{figure}
\noindent
\textbf{One-to-All (Basic):} 
In a graph of two job vertices having cardinality N and M, a basic 1-to-All edge has each task produce M outputs, one for each of the M destination tasks.  Each destination task receives one input from each of the source tasks.This shape is also known as \textit{complete bipartite}.
The primary point to note is that the tasks in the first vertex must open M output targets and write to them individually.  If implemented with real files or network ports this may not scale to support thousands of destination tasks.  

\noindent
\\
\textbf{One-to-All (Shared):} 
A “1-to-All shared” edge has each task produce one output file and that file is made available to all the destination tasks.  The primary use of this edge is to implement \textit{hash-partitioning} without the need to open many outputs per task that would be required with the basic 1-to-All edge.  The strategy is to have each source task produce a single output file that comprises a packed and indexed representation of M partitions.  The destination tasks receive an identifier for each output, read the index,  then only transfer their portion of the output.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{one-to-all-shared}
        \caption{One-to-All shared edge}
        \label{one-to-one}
\end{figure}

\subsubsection*{Dynamic Task Cardinality}For relational query processing it is frequently necessary to support \textit{runtime decisions} for\textit{ task-cardinality} \cite{hortonworks}.  Some scenarios include:\be\ii A standard container might be known to handle $X$ bytes of data without suffering OOM or taking excessively long.  
At runtime, each stage should create sufficient tasks such that each receives no more than $X$ bytes of input.\ii A job hint might specify resource usage guidelines such as "Use as few resources as required to make progress" or "Use as many cluster resources as makes sense".   \ii Dynamic decision making may also involve how to schedule tasks against available container slots.  
If there are $N$ files requiring processing but only $M$ slots available, we have choices:
\be\ii run $M$ tasks, and provide groups of files to each\ii run $N$ tasks but only activate $M$ at a time\ii file-sized based dynamic decisions.
\ee\eeTez expects to supports these types of data-size constraints as needed by adopters. 

\subsubsection*{Skew Handling}\textit{Data skew} is an ever present issue for relational queries as it routinely leads to a vertex in which the tasks have unequal data sizes to process.  Various strategies such as \textit{over-partitioning} and \textit{dynamic task cardinality} are useful to resolve data-skew situations.  A particular cause of concern is a data-skew in which a large portion of rows share a common key.  For example, if a key column contains many NULLs, or perhaps simply a frequent value such as “en-US”, then a \textit{hash-partitioning} operation may yield very uneven buckets.
\section{Runtime API}
Apache Tez models data processing as a \emph{dataflow graph}, with the
\textbf{vertices} in the graph representing \emph{processing of data}
and \textbf{edges} representing \emph{movement of data} between the
processing \cite{Saha:2013-03}. Thus \emph{user logic}, that analyses and modifies the data,
sits in the \textbf{vertices}. Edges determine the consumer of the data,
how the data is transferred and the \emph{dependency} between the
\emph{producer} and \emph{consumer} vertices.

For users of \textbf{MapReduce} (\textbf{MR}), the most primitive
functionality that Tez can provide is an ability to run a \emph{chain of
Reduce stages} as compared to a \emph{single} Reduce stage in the
current MR implementation. Via the Task API, Tez can do this and much
more by facilitating execution of any form of processing logic that does
not need to be retrofitted into a Map or Reduce task and also by
supporting multiple options of data transfer between different vertices
that are not restricted to the \textbf{MapReduce} \emph{shuffle
transport mechanism}.

\subsection{The Building Blocks}

The \textit{Task API} provides the building blocks for a user to plug-in their
logic to analyze and modify data into the \textit{vertex} and augment this
processing logic with the necessary plugins to \emph{transfer} and
\emph{route} data between vertices.

Tez models the user logic running in each vertex as a composition of a
set of \textit{Inputs}, a \textit{Processor} and \textit{a set of Outputs}.

\begin{itemize}
\item
  \textbf{Input:} An input represents a pipe through which a processor
  can accept input data from a \emph{data source} such as HDFS or the
  output generated by another vertex.
\item
  \textbf{Processor:} The entity responsible for \emph{consuming} one or
  more Inputs and \emph{producing} one or more Outputs.
\item
  \textbf{Output:} An output represents a pipe through which a processor
  can generate output data for another vertex to consume or to a
  \emph{data sink} such as HDFS.
\end{itemize}

Given that an edge in a DAG is a \textit{logical entity} that represents a number
of \textit{physical connections} between the tasks of 2 connected vertices, to
improve ease of programmability for a developer implementing a new
Processor, there are 2 kinds of Inputs and Outputs to either expose or
hide the level of complexity:

\begin{itemize}
\item
  \textbf{Logical:} A corresponding pair of a \emph{LogicalInput} and a
  \emph{LogicalOutput} represent the \emph{logical edge} between 2
  vertices. The implementation of Logical objects hides all the
  underlying physical connections and exposes a single view to the
  data.
\item
  \textbf{Physical:} The pair of \textit{Physical Input} and \textit{Output} represents
  the \emph{connection} between a task of the \emph{Source vertex} and a
  task of a \emph{Destination vertex}.
\end{itemize}

An example of the \emph{Reduce stage} within an MR job as shown in Figure \ref{fig10} would be a
\emph{Reduce Processor} that receives data from the maps via
\emph{ShuffleInput} and generates output to HDFS. Likewise, an
intermediate Reduce stage in an \emph{MRR chain} would be quite
similar to the final Reduce stage except for the difference in the
Output type.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez12}
        \caption{An example of the Reduce stage within an MR jobs}
        \label{fig10}
\end{figure}


\subsection{Tez Runtime API}

To implement a new \textit{Input, Processor} or \textit{Output}, a user to implement the
appropriate interfaces mentioned above. All objects are given a \textit{Context}
object in their initialize functions. This context is the hook for these
objects to communicate to the Tez framework. 

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez32}
        \caption{Tez runtime API (Input, Processor and Output)}
        \label{fig11}
\end{figure}

The Inputs and Outputs are expected to provide implementations for their respective
\textit{Readers} and \textit{Writers} which are then used by the \textit{Processor} to read/write
data. In a task, after the Tez framework has initialized all the
necessary Inputs, Outputs and the Processor, the Tez framework invokes
the Processor's run function and passes the appropriate \textit{handles} to all
the Inputs and Outputs for that particular task.

Tez allows all inputs and outputs to be \emph{pluggable}. This requires
support for passing of information from the Output of a source vertex to
the Input of the destination vertex. For example, let us assume that the
Output of a source vertex writes all of its data to a \emph{key-value
store}. The Output would need to communicate the ``\emph{key}'' to the
Input of the next stage so that the Input can retrieve the correct data
from the key-value store. To facilitate this, Tez uses \emph{Events}.

\subsection{Events in Tez}

\textit{Events} used to communicate between the tasks and between ApplicationMaster (AM). 
\textit{Data Movement Event} used by producer task to inform the consumer task about data location, size etc.
\textit{Input Error event }sent by task to AM to inform about errors in reading input. AM then takes action by re-generating the input. Other events to send task completion notification, data statistics and other control plane information.
Similarly, \emph{Events} in Tez are a way to pass information amongst different components.

\begin{itemize}
\item
  The Tez framework uses Events to pass information of system events
  such as \emph{task failures} to the required components.
\item
  Inputs of a vertex can inform the framework of any failures
  encountered when trying to retrieve data from the source vertex's
  Output that in turn can be used by the framework to take \textit{failure
  recovery} measures.
\item
  An Output can pass information of the location of the data, which it
  generates, to the Inputs of the destination vertex. An example of this
  is described in the \textit{Shuffle Event} as shown in Figure \ref{fig12} which shows how the output
  of a Map stage informs the Shuffle Input of the Reduce stage of the
  location of its output via a \textit{Data Movement Event}.
\end{itemize}

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.3\textwidth]{tez22}
        \caption{An example of the Suhffle Event}
        \label{fig12}
\end{figure}

Another use of Events is to enable \textit{run-time changes} to the DAG execution
plan. For example, based on the amount of the data being generated by a
Map stage, it may be more optimal to run less reduce tasks within the
following Reduce stage. Events generated by Outputs are routed to the
\textit{pluggable} Vertex/Edge management modules, allowing them to make the
necessary decisions to modify some run-time parameters as needed.

\subsection{Implementations}

The \emph{flexibility} of Tez allows anyone to implement their Inputs
and Outputs, whether they use \emph{blocking/non-blocking transport
protocols}, handle data in the form of raw bytes/records/key-value pairs
etc., and build Processors to handle these variety of Inputs and
Outputs.

There is already a small repository of various implementations of
\textit{Inputs/Outputs/Processors}:

\begin{itemize}
\item
  \emph{MRInput} and \emph{MROutput}: Basic input and outputs to
  handle data to/from HDFS that are MapReduce compatible as they use
  MapReduce constructs such as \textit{InputFormat}, \textit{RecordReader}, \textit{OutputFormat}
  and \textit{RecordWriter}.
\item
  \emph{OnFileSortedOutput} and \emph{ShuffleMergedInput}: A pair of  key-value based Input and Output that use the local disk for all I/O
  and provide the same \textit{sort+merge} functionality that is required for the
  \textit{``shuffle'' edge} between the Map and Reduce stages in a MapReduce job.
\item
  \emph{OnFileUnorderedKVOutput} and
  \emph{ShuffledUnorderedKVInput}: These are similar to the shuffle
  pair mentioned earlier except that the data is not sorted implicitly.
  This can be a big performance boost in various situations.
\item
  \emph{MapProcessor} and \emph{ReduceProcessor}: As the names
  suggest, these processors are available for anyone trying to run a
  MapReduce job on the Tez execution framework. They can be used to run
  an \textit{MRR chain} too.
\end{itemize}

% We should move the below statements to the conclusions section.
%As the Hive and Pig projects adapt to use Tez, we hope this repository
%will grow to house a common set of building blocks for use across the
%different projects.

\section{Writing a Tez Input, Processor and Output}

\subsection{Tez Task}
Tez task is constituted of all the \textit{Inputs} on its incoming edges, the
\textit{Processor} configured for the \textit{Vertex}, and all the \textit{Output}(s) on it's
outgoing edge \cite{Saha:2013-04}.

The number of tasks for a vertex is equal to the \textit{parallelism set} for
that vertex -- which is set at DAG construction time, or modified during
runtime via user plugins running in the AM.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.3\textwidth]{tez01}
        \caption{A single Tez task}
        \label{fig13}
\end{figure}

Figure \ref{fig13} shows a single task. The vertex is configured to run
\textit{Processor1} -- has two incoming edges -- with the output of the edge
specified as \textit{Input1} and \textit{Input2} respectively, and has a single outgoing
edge -- with the input to this edge configured as \textit{Output1}. There will be
$n$ such Task instances created per Vertex -- depending on the
\textit{parallelism}.

\subsection{Initialization of a Tez task}

The following steps are followed to initialize and run a Tez task.

The Tez framework will first construct instances of the specified
Input(s), Processor, Output(s) using a 0 argument constructor.

For a LogicalInput and a LogicalOutput -- the Tez framework will set the
number of physical connections using the respective \textit{setNumPhysicalInputs}
and \textit{setNumPhysicalOutputs} methods.

The Input(s), Processor and Output(s) will then be initialized via their
respective \textit{initialize} methods. Configuration and context information is
made available to the Is/P/Os via this call. More information on the
\textit{Context} classes is available in the JavaDoc for \textit{TezInputContext},
\textit{TezProcessorContext} and \textit{TezOutputContext}.

The Processor \textit{run} method will be called with the initialized Inputs and
Outputs passed in as arguments (as a Map -- connected vertexName to
Input/Output).
Once the run method completes, the Input(s), Processor and Output(s)
will be closed, and the task is considered to be complete.
\\
Notes for I/P/O writers:
\bi
\ii Each Input / Processor / Output must provide a 0 argument constructor.
\ii No assumptions should be made about the order in which the Inputs,
Processor and Outputs will be initialized, or closed. 
\ii Assumptions should also not be made about how the Initialization, Close and Processor run
will be invoked -- i.e. on the same thread or multiple threads.
\ei

\subsubsection*{Common Interfaces to be implemented by
Input/Processor/Output}

\bi
\ii \textbf{List initialize(Tez*Context)} -This is where I/P/O receive their corresponding context objects. They can, optionally, return a list of
events. 
\ii \textbf{handleEvents(List events)} -- Any events generated for the specific I/P/O will be passed in via this interface. Inputs receive DataMovementEvent(s) generated by corresponding Outputs on this interface -- and will need to interpret them to retrieve data. At the moment, this can be ignored for Outputs and Processors. 
\ii \textbf{List close()} --
Any cleanup or final commits will typically be implemented in the close
method. This is generally a good place for Outputs to generate
DataMovementEvent(s). More on these events later.
\ei 
\subsubsection*{Providing User Information to an Input/Processor/Output}
Information specified in the bytePayload associated with an
Input/Processor/Output is made available to the respective I/P/O via
their \textit{context} objects.

Users provide this information as a byte array -- and can specify any
information that may be required at runtime by the I/P/O. This could
include \textit{configuration}, \textit{execution plans} for Hive/PIG, etc. As an example,
the current inputs use a Hadoop Configuration instance for backward
compatibility. Hive may choose to send it's \textit{vertex execution plan} as
part of this field instead of using the distributed cache provided by
YARN.

Typically, \textit{Inputs} and \textit{Outputs} exist as a pair -- the Input knows how to
process \textit{DataMovementEvent}(s) generated by the corresponding \textit{Output}, and
how to interpret the data. This information will generally be encoded
into some form of configuration (specified via the userPayload) used by
the Output-Input pair, and should match. As an example -- the output Key
type configured on an Output should match the Input key type on the
corresponding Input.

\subsection{Writing a Tez LogicalOutput}

A \textit{LogicalOutput} can be considered to have two main responsibilities.

\be
\ii dealing with the actual data provided by the \textit{Processor} --
partitioning it for the `\textit{physical}' edges, serializing it etc.
\ii Providing information to Tez (in effect the subsequent Input) on where
this data is available.
\ee
\subsubsection*{Processing the Data}
Depending on the connection pattern being used -- an Output will
generate data to a single `\textit{physical}' edge or multiple `physical' edges.
A \textit{LogicalOutput} is responsible for partitioning the data into these
`physical' edges.

It would typically work in conjunction with the configured downstream
Input to write data in a specific data format understood by the
downstream Input. This includes a serialization mechanism, compression
etc.

As an example: \textit{OnFileSortedOutput} which is the Output used for a
MapReduce shuffle makes use of a \textit{Partitioner} to partition the data into
n partitions (`physical' edges) -- where n corresponds to the number of
downstream tasks. It also sorts the data per partition, and writes it
out as Key-Value pairs using Hadoop serialization which is understood by
the downstream Input (\textit{ShuffledMergedInput} in this case).

\subsubsection*{Providing information on how the data is to be retrieved}
A LogicalOutput needs to send out information on how data is to be
retrieved by the corresponding downstream Input defined on an edge. This
is done by generating \textit{DataMovementEvent}(s). These events are routed by
the AM, based on the connection pattern, to the relevant LogicalInputs.

These events can be sent at anytime by using the \textit{TezOutputContext} with
which the Output was initialized. Alternately, they can be returned as
part of the \textit{initialize()} or \textit{close()} calls. More on \textit{DataMovementEvent}(s)
further down.

Continuing with the \textit{OnFileSortedOutput} example: This will generate one
event per partition -- the sourceIndex for each of these events will be
the partition number. This particular Output makes use of the MapReduce
ShuffleHandler, which requires downstream Inputs to pull data over HTTP.
The payload for these events contains the host name and port for the
http server, as well as an identifier which uniquely identifies the
specific task and Input instance running this output.

In case of \textit{OnFileSortedOutput} -- these events are generated during the
\textit{close()} call.
For more detail, see Listing \ref{sorted-output-code} (OnFileSortedOutput.java) in the Appendix.

\subsubsection*{Specific interface for a LogicalOutput}

\bi
\ii \textbf{setNumPhysicalOutputs(int)} -- This is where a Logical Output is informed
about the number of physical outgoing edges for the output. Writer
\ii \textbf{getWriter()} -- An implementation of the \textit{Writer} interface, which can be
used by a Processor to write to this Output.
\ei

\subsection{Writing a Tez LogicalInput}

The main responsibilities of a Logical Input are
\be
\ii Obtaining the actual data over the `\textit{physical}' edges, and 
\ii Interpreting the data, and providing a single `\textit{Logical}' view of this data to the Processor.
\ee

\subsubsection*{Obtaining the Data:}
A LogicalInput will receive \textit{DataMovementEvent}(s) generated by the
corresponding \textit{LogicalOutput} which generated them. It needs to interpret
these events to get hold of the data. The number of \textit{DataMovementEvent}(s)
a LogicalInput receives is typically equal to the number of physical
edges it is configured with, and is used as a termination condition.

As an example: \textit{ShuffledMergedInput} (which is the Input on the
\textit{OnFileSortedOutput-ShuffledMergedInput} O-I edge) would fetch data from
the ShuffleHandler by interpretting the host, port and identifier from
the \textit{DataMovementEvent}(s) it receives.

\subsubsection*{Providing a view of the data to the Processor} 
A LogicalInput will typically expose the data to the Processor via a
Reader interface. This would involve interpreting the data, manipulating
it if required -- decompression, ser-de etc.

Continuing with the \textit{ShuffledMergedInput} example: This input fetches all
the data -- one chunk per source task and partition -- each of which is
sorted. It then proceeds to merge the sorted chunks and makes the data
available to the Processor only after this step -- via a KeyValues
reader implementation.
For more detail, see Listing \ref{merged-input-code} (ShuffledMergedInput.java) and Listing \ref{kv-input-code} (ShuffledUnorderedKVInput.java) in the Appendix.

\subsubsection*{Specific interface for a LogicalInput}
\bi
\ii \textbf{setNumPhysicalInputs(int)} -- This is where a LogicalInput is informed
about the number of physical incoming edges. 
\ii \textbf{Reader getReader()} -- An
implementation of the \textit{Reader} interface, which can be used by a Processor
to read from this Input
\ei

\subsection{Writing a Tez LogicalIOProcessor}

A logical processor receives configured \textit{LogicalInput}(s) and
\textit{LogicalOutput}(s). It is responsible for reading source data from the
Input(s), processing it, and writing data out to the configured
Output(s).

A processor is aware of which vertex (vertex-name) a specific Input is
from. Similarly, it is aware of the output vertex (via the vertex-name)
associated with a specific Output. It would typically validate the Input
and Output types, process the Inputs based on the \textit{source} vertex and
generate output for the various \textit{destination} vertices.

As an example: The \textit{MapProcessor} validates that it is configured with
only a single Input of type \textit{MRInput} -- since that is the only input it
knows how to work with. It also validates the Output to be an
\textit{OnFileSortedOutput} or a \textit{MROutput}. It then proceeds to obtain a KeyValue
reader from the \textit{MRInput}, and KeyValueWriter from the \textit{OnFileSortedOutput}
or MROutput. The \textit{KeyvalueReader} instance is used to walk all they keys
in the input -- on which the user configured map function is called,
with a MapReduce output collector backed by the \textit{KeyValue} writer
instance.

\subsubsection*{Specific interface for a LogicalIOProcessor}

\bi
\ii \textbf{run(Map inputs, Map outputs)} -- This is where a processor should
implement it's compute logic. It receives initialized Input(s) and
Output(s) along with the vertex names to which the Input(s) and
Output(s) are connected.
\ei
\subsection{DataMovementEvent}

A \textit{DataMovementEvent} is used to communicate between Outputs and Inputs to
specify location information. A byte payload field is available for this
-- the contents of which should be understood by the communicating
Outputs and Inputs. This byte payload could be interpreted by
user-plugins running within the AM to modify the DAG (\textit{Auto
reduce-parallelism} as an example).

\textit{DataMovementEvent}(s) are typically generated per physical edge between
the Output and Input. The event generator needs to set the sourceIndex
on the event being generated -- and this matches the physical
Output/Input that generated the event. Based on the ConnectionPattern
specified for the DAG -- Tez sets the targetIndex, so that the event
receiver knows which physical Input/Output the event is meant for. An
example of data movement events generated by a ScatterGather connection
pattern (Shuffle) follows, with values specified for the source and
target Index.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{tez02}
        \caption{An example of data movement events}
        \label{fig14}
\end{figure}

In this case the Input has 3 tasks, and the output has 2 tasks. Each
input generates 1 partition (physical output) for the downstream tasks,
and each downstream task consumes the same partition from each of the
upstream tasks.

\be
\ii Vertex1, Task1 will generate two DataMovementEvents -- E1 and E2.
\bi
\ii E1, sourceIndex = 0 (since it is generated by the 1st physical output) 
\ii E2, sourceIndex = 1 (since it is generated by the 2nd physical output)
\ei
\ii Similarly Vertex1, Task2 and Task3 will generate two data movement
events each. 
\bi
\ii E3 and E5, sourceIndex=0 
\ii E4 and E6, sourceIndex=1
\ei
\ii Based on the ScatterGather ConnectionPattern, the AM will rout the
events to respective tasks. 
\bi
\ii E1, E3, E5 with sourceIndex 1 will be sent
to Vertex2, Task1 
\ii E2, E4, E6 with sourceIndex 2 will be sent to Vertex2,
Task2
\ei
\ii The destination will see the following targetIndex (based on the
physical edges between the tasks (arrows)) 
\bi
\ii E1, targetIndex=0 -- first
physical input to V2, Task1 
\ii E3, targetIndex=1 -- second physical input
to V2, Task1 
\ii E5, targetIndex=5 -- third physical input to V2, Task1
\ii Similarly, E2, E4, E6 will have target indices 0,1 and 2 respectively --
i.e. first, second and third physical input to V2 Task2.
\ei
\ee
DataMovement events generated by an Input are routed to the
corresponding upstream Input defined on the edge. Similarly data
movement events generated by an Output are routed to the corresponding
downstream Input defined on the edge.
If the Output is one of the Leaf Outputs for a DAG -- it will typically
not generate any events.

\subsection{Error Handling}
\subsubsection*{Reporting errors from an Input/Processor/Output}
\bi
\ii \textbf{Fatal Errors} -- fatal errors can be reported to Tez via the \textit{fatalError} method available on the
context instances, with which the I/P/O was initialized. Alternately,
throwing an Exception from the \textit{initialize}, \textit{close} or \textit{run} methods are
considered to be fatal. Fatal errors cause the current running task to
be killed. 
\ii \textbf{Actionable Non Fatal Errors} -- Inputs can report the failure
to obtain data from a specific Physical connection by sending an
\textit{InputReaderErrorEvent} via the InputContext. Depending on the Edge
configuration, this may trigger a retry of the previous stage task which
generated this data. 
\ei
\subsubsection*{Errors reported to an Input} 
If the AM determines
that data generated by a previous task is no longer available, Inputs
which require this data are informed via an \textit{InputFailedEvent}. The
sourceIndex, targetIndex and attemptNumber information on this event
would correspond to the \textit{DataMovementEvent} event with the same values.
The Input will typically handle this event by not attempting to obtain
data based on the specific DataMovement event, and would wait for an
updated DataMovementEvent for the same data.
\\
\textbf{Notes:} Tez does not enforce any interface on the Reader and Writer to stay data format agnostic. Specific Writers and
Readers can be implemented for Key-Value, Record or other data formats.
A KeyValue and KeyValues Reader/Writer interface and implementation,
based on Hadoop serialization, is used by the Shuffle Input/Output
provided by the Tez Runtime library.

\section{Dynamic Graph Reconfiguration}
\subsection{Case Study: Automatic Reduce Parallelism}
%\textbf{Motivation:} 
%\emph{Distributed data processing} is dynamic by nature and it is
%extremely difficult to statically determine \emph{optimal concurrency}
%and \emph{data movement methods} a priori. More information is available
%during runtime, like \emph{data samples and sizes}, which may help
%optimize the execution plan further. We also recognize that Tez by
%itself cannot always have the smarts to perform these \emph{dynamic
%optimizations}. The design of Tez includes support for \emph{pluggable
%vertex management} modules to collect relevant information from tasks
%and change the \emph{dataflow graph} at runtime to optimize for
%\emph{performance} and \emph{resource usage} \cite{Saha:2013-05}. 
%
%Figure \ref{fig15} shows how we can determine an appropriate number of reducers in a MapReduce like
%job by observing the actual data output produced and the \emph{desired
%load} per reduce task.
%
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{dtez11}
%        \caption{tez01}
%        \label{fig15}
%\end{figure}

\subsubsection*{Performance and Efficiency} 
Tez envisions running computation by the most \emph{resource efficient}
and \emph{high-performance} means possible given the runtime conditions in the cluster and the results of the previous steps of the computation. This functionality is constructed using a couple of basic building blocks

\begin{itemize}
\item
  \textbf{Pluggable Vertex Management Modules}: The control flow
  architecture of Tez incorporates a \emph{per-vertex pluggable module}
  for user logic that deeply understands the data and computation. The
  vertex state machine invokes this user module at significant
  transitions of the state machine such as vertex start, source task
  completion etc. At these points the user logic can examine the runtime
  state and provide hints to the main Tez execution engine on attributes
  like vertex task parallelism.
\item
  \textbf{Event Flow Architecture}: Tez defines a set of events by which
  different components like vertices, tasks etc. can pass information to
  each other. These events are routed from source to destination
  components based on a \emph{well-defined routing logic} in the Tez
  control plane. One such event is the \textbf{VertexManager} event that
  can be used to send any kind of user-defined payload to the
  VertexManager of a given vertex.
\end{itemize}

\subsection{Case Study: Reduce Task Parallelism and Reduce Slow-Start}
%\textbf{Reduce Task Parallelism and Reduce Slow-Start}

Determining the correct number of reduce tasks has been a long standing
issue for Map Reduce jobs. The output produced by the map tasks is not
known a priori and thus determining that number before job execution is
hard. This becomes even more difficult when there are several stages of
computation and the reduce parallelism needs to be determined for each
stage. We take that as a case study to demonstrate the graph
reconfiguration capabilities of Tez.

\paragraph{Reduce Task Parallelism:}

Tez has a \textbf{ShuffleVertexManager} that understands the semantics
of \emph{hash based partitioning} performed over a \emph{shuffle
transport layer} that is used in MapReduce. Tez defines a
\textbf{VertexManager} event that can be used to send an arbitrary user
payload to the vertex manager of a given vertex. The \emph{partitioning
tasks} (say the \textbf{Map tasks}) use this event to send
\emph{statistics} such as the size of the output partitions produced to
the \textbf{ShuffleVertexManager} for the reduce vertex. The manager
receives these events and tries to model the final output statistics
that would be produced by the all the tasks. It can then advise the
\emph{vertex state machine} of the Reduce vertex to decrease the
parallelism of the vertex if needed. The idea being to first
\emph{over-partition} and then determine the correct number at runtime.
The \emph{vertex controller} can cancel extra tasks and proceed as
usual.

\paragraph{Reduce Slow-start/Pre-launch:}

Slow-start is a MapReduce feature where-in the reduce tasks are launched
before all the map tasks complete. The hypothesis being that reduce
tasks can start fetching the completed map outputs while the remaining
map tasks complete. Determining when to pre-launch the reduce tasks is
tricky because it depends on output data produced by the map tasks. It
would be inefficient to run reduce tasks so early that they finish
fetching the data and sit idle while the remaining maps are still
running. In Tez, the \emph{slow-start logic} is embedded in the
\textbf{ShuffleVertexManager}. The vertex state controller informs the
manager whenever a \emph{source task} (here the \textbf{Map task})
completes. The manager uses this information to determine when to
\emph{pre-launch} the reduce tasks and how many to pre-launch. It then
advises the vertex controller.

Its easy to see how the above can be extended to determine the correct
parallelism for \emph{range-partitioning} scenarios. The data samples
could be sent via the \textbf{VertexManager} events to the vertex
manager that can create the \emph{key-range histogram} and determine the
correct number of partitions. It can then assign the appropriate
key-ranges to each partition. Thus, in Tez, this operation could be
achieved without the overhead of a separate sampling job.

\section{Re-Using Containers in Apache Tez}
%\subsection{Motivation}

Tez follows the traditional Hadoop model of \emph{dividing a job into
individual tasks}, all of which are run as processes via \textbf{YARN},
on the users' behalf -- for \emph{isolation}, among other reasons. This
model comes with inherent costs -- some of which are listed below.

\begin{itemize}
\item
  Process \emph{startup} and \emph{initialization} cost, especially when
  running a Java process is fairly high. For short running tasks, this
  initialization cost ends up being a significant fraction of the actual
  task runtime. \emph{Re-using containers} can significantly reduce this
  cost.
\item
  Stragglers have typically been another problem for jobs -- where
  \emph{a job runtime is limited by the slowest running task}. With
  reduced static costs per tasks -- it becomes possible to run more
  tasks, each with a smaller work-unit. This reduces the runtime of
  stragglers (\emph{smaller work-unit}), while allowing faster tasks to
  process additional work-units which can \emph{overlap} with the
  stragglers.
\item
  \emph{Re-using containers} has the additional advantage of not needing
  to allocate each container via the \textbf{YARN ResourceManager (RM)} \cite{Saha:2013-06}.
\end{itemize}

Other than helping solve some of the existing concerns, re-using
containers provide additional opportunities for optimization where data
can be \emph{shared} between tasks.

\subsection{Consideration for Re-Using Containers}

\subsubsection*{Compatibility of Containers} 
Each vertex in Tez specifies parameters, which are used when launching
containers. These include the requested resources (memory, CPU etc),
\textbf{YARN LocalResources}, the environment, and the command line
options for tasks belonging to this \textbf{Vertex}. When a container is
first launched, it is launched for a specific task and uses the
parameters specified for the \emph{task} (or \emph{vertex}) -- this then
becomes the container's signature. An already running container is
considered to be compatible for another task when the running
container's signature is a superset of what the task requires.

\subsubsection*{Scheduling}
Initially, when no containers are available, the Tez AM will request
containers from the RM with location information specified, and rely on
YARN's scheduler for locality-aware assignments. However, for containers
which are being considered for re-use, the scheduling smarts offered by
YARN are no longer available.

The \textbf{Tez scheduler} works with several parameters to take
decisions on \emph{task assignments} -- \emph{task-locality
requirements}, \emph{compatibility of containers} as described above,
total available resources on the cluster, and the priority of pending
task requests.

When a task completes, and the container running the task becomes
available for re-use -- a task may not be assigned to it immediately --
as tasks may not exist, for which the data is local to the container's
node. The Tez scheduler first makes an attempt to find a task for which
the data would be \emph{local} for the container. If no such task
exists, the scheduler holds on to the container for a specific time,
before actually allocating any \emph{pending tasks} to this container.
The expectation here, is that more tasks will complete -- which gives
additional opportunities for scheduling tasks on nodes which are close
to the data. Going forward, \emph{non-local containers} may be used in a
speculative manner.

\emph{Priority} of pending tasks (across different vertices),
compatibility and cluster resources are considered to ensure that tasks
which are deemed to be of higher priority (either due to a
must-run-before relationship, failure, or due to specific scheduling
policies) have an available container.

In the future, \emph{affinity} will become part of the \emph{scheduling
decision}. This could be dictated by common resources shared between
tasks, which need only be loaded by the first task running in a
container, or by the data generated by the first task, which can then
directly be processed by subsequent tasks, without needing to
move/serialize the data -- especially in the case of \emph{One-to-One
edges}.

\subsection{Beyond simple JVM Re-Use}

\subsubsection*{Cluster Dependent Work Allocation}

At the moment, the number of tasks for a vertex, and their corresponding
`\emph{work-units}' are determined up front. Going forward, this is
likely to change to a model, where a certain number of tasks are setup
up front based on cluster resources, but \emph{work-units} for these
tasks are determined at runtime. This allows additional optimizations
where tasks which complete early are given additional work, and also
allows for better \emph{locality-based assignment} of work.

\subsubsection*{Object Registry}

Each Tez JVM (or \emph{container}) contains an \emph{object cache},
which can be used to \emph{share data} between different tasks running
within the same container. This is a simple \emph{Key-Object store},
with different levels of \emph{visibility/retention}. Objects can be
cached for use within tasks belonging to the same Vertex, for all tasks
within a \emph{DAG}, and for tasks running across a \emph{Tez
Session} (more on Sessions in a subsequent post). The resources being
cached may, in the future, be made available as a hint to the Tez
Scheduler for affinity based \emph{scheduling}.

\subsubsection*{Examples of Usage}

\be
\ii \emph{Hive} makes use of this \emph{object registry} to cache data
for \emph{Broadcast Joins}, which is fetched and computed once by the
first task, and used directly by remaining tasks which run in the same
JVM.
\ii The sort buffer used by \emph{OnFileSortedOutput} can be cached,
and re-used across tasks.
\ee

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.4\textwidth]{tez121}
        \caption{Container reuse in the Tez}
        \label{fig16}
\end{figure}

\section{Tez Sessions}

Most relational databases have had a notion of sessions for quite some
time. A database session can be considered to represent a connection
between a user/application and the database or in more general terms, an
instance of usage of a database. A session can encompass multiple
queries and/or transactions. It can leverage common services, for
example, caching, to provide some level of performance optimizations.

A Tez session, currently, maps to one instance of a Tez Application
Master (AM). For folks who are familiar with YARN and MapReduce, you
would know that for each MapReduce job, a corresponding MapReduce
Application Master is launched. In Tez, using a Session, a user can can
start a single Tez Session and then can submit DAGs to this Session AM
serially without incurring the overhead of launching new AMs for each
DAG.

\noindent
\\
\textbf{Motivation:} As mentioned earlier, the main proponents for Tez are Apache projects
such as Hive and Pig. Consider a Pig script, the amount of work
programmed into a script may not be doable within a single Tez DAG. Or
let us take a common data analytics use-case in Hive where a user uses a
Hive Shell for data drill-down (for example, multiple queries over a
common data-set). There are other more general use-cases such as users
of Hive connecting to the Hive Server and submitting queries over the
established connection or using the Hive shell to execute a script
containing one or more queries.

All of the above can leverage Tez Sessions \cite{Saha:2013-07}.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{sessions-tez1}
        \caption{Tez sessions}
        \label{fig17}
\end{figure}

\subsection{Using Tez Sessions}

Using a Tez Session is quite simple:

\begin{enumerate}
\item
  Firstly, instantiate a \emph{TezSession} object with the required
  configuration using \emph{TezSessionConfiguration}.
\item
  Invoke \emph{TezSession::start()}
\item
  Wait for the TezSession to reach a ready state to accept DAGs by using
  the \emph{TezSession::getSessionStatus()} api (this step is
  optional)
\item
  Submit a DAG to the Session using
  \emph{TezSession::submitDAG(DAG dag)}
\item
  Monitor the DAG's status using the \emph{DAGClient} instance
  obtained in step (4).
\item
  Once the DAG has completed, repeat step (4) and step (5) for
  subsequent DAGs.
\item
  Shutdown the Session once all work is done via
 \emph{TezSession::stop()}.
\end{enumerate}

There are some things to keep in mind when using a Tez Session:

\begin{itemize}
\item
  A \emph{Tez Session} maps to a single \emph{Application Master}
  and therefore, all resources required by any user-logic (in any
  subsequent DAG) running within the \emph{ApplicationMaster} should
  be available when the \emph{AM} is launched.
  \begin{itemize}
  \item
    This mostly pertains to code related to the
    \emph{VertexOutputCommitter} and any user-logic in the
    \emph{Vertex} \emph{scheduling} and \emph{management} layers.
  \item
    User-logic run in tasks is not governed by the above restriction.
  \end{itemize}
\item
  The resources (memory, CPU) of the \emph{AM} are fixed so please
  keep this in mind when configuring the AM for use in a session. For
  example, memory requirements may be higher for a very large DAG.
\end{itemize}

\subsection{Performance Benefits}

\textbf{Container Re-Use}. We know that \emph{re-use} of containers was
doable within a single DAG. In a Tez Session, containers are re-used
even across DAGs as long as the containers are compatible with the task
to be run on them. This vastly improves performance by not incurring the
overheads of launching containers for subsequent DAGs. Containers, when
not in use, are kept around for a \emph{configurable period} before
being released back to YARN's \emph{ResourceManager}.

\noindent
\\
\textbf{Caching with the Session}. When running drill-down queries on
common datasets, smarting caching of meta-data and potentially even
caching of intermediate data or previous results can help improve
performance. \emph{Caching} could be done either within the \emph{AM}
or within \emph{launched containers}. Such caching allows for more
\emph{fine-grained controls} with respect to caching policies. A
\emph{session-based cache} as compared to a global cache potentially
provides more predictable performance improvements.

\subsection{Example Usage}

The Tez source code has a
\href{https://git-wip-us.apache.org/repos/asf?p=incubator-tez.git;a=blob_plain;f=tez-mapreduce-examples/src/main/java/org/apache/tez/mapreduce/examples/OrderedWordCount.java;hb=master}{simple
\emph{OrderedWordCount} example}. This DAG is similar to the
\emph{WordCount} example in \emph{MapReduce} except that it also
orders the words based on their frequency of occurrence in the dataset.
The DAG is an \emph{MRR chain} i.e. a \emph{3-vertex linear chain} of
\emph{Map-Reduce-Reduce}.

To run the \emph{OrderedWordCount} example to process different
data-sets via a single Tez Session, use:

{\scriptsize
\begin{verbatim}
$ hadoop jar tez-mapreduce-examples.jar orderedwordcount 
-DUSE_TEZ_SESSION=true -DINTER_JOB_SLEEP_INTERVAL=0
/input1/ /output1/ /input2/ /output2/ 
/input3/ /output3/ /input4/ /output4/
\end{verbatim}
}

Figure \ref{fig18} is a graph depicting the times seen when running \emph{multiple}
MRR DAGs on the same dataset (the dataset had 6 files to ensure multiple
containers are needed in the map stage ) in the same session. This test
was run on my old MacBook running a single node Hadoop cluster having
only one \emph{DataNode} and one \emph{NodeManager}.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{sessions-tez2}
        \caption{Performance results of multiple MRR DAGs on the same dataset}
        \label{fig18}
\end{figure}

As you can see, even though this is just a simulation test running on a
\emph{very small data} set, leveraging containers across DAGs has a huge
performance benefit.

\section{Discussion and Future Work}
There are many different methods and tools for interacting and querying data within Hadoop. The most widely used tools allow for SQL based querying of the data. The following article summarizes a great comparison by MapR of the most common SQL on Hadoop technologies available today \cite{Intelli:2014}.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{dst01}
        \caption{A Common Tool Comparison: SQL Mode}
        \label{dst01}
\end{figure}

Initially developed by Facebook, Apache Hive is a data warehouse infrastructure that is built on top of Hadoop. It allows querying data stored on HDFS for analysis via HQL, an SQL-like language that is translated to MapReduce jobs. Although it seems to provide SQL functionality, Hive still runs jobs on Hadoop as batch processing and does not provide interactive querying. It stores metadata in a relational database and requires maintaining a schema for the data. Only four file formats are supported by Hive: text, SequenceFile, ORC and RCFile. Hive supports processing compressed on Hadoop and also user defined functions.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{dst02}
        \caption{SQL ANSI Completeness}
        \label{dst02}
\end{figure}

Cloudera's Impala is a query engine that runs on top of Hadoop and executes interactive SQL queries on HDFS and HBase. While Hive runs in batch processing, Impala runs the queries in real-time, thus integrating SQL based business intelligence tools with Hadoop. Although Cloudera is the main developer behind this tool, it is fully open source and supports the following file formats:  text, LZO, SequenceFile, Avro and RCFile. Impala can also run on the cloud via Amazon’s Elastic MapReduce.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{dst03}
        \caption{Client Access Methods}
        \label{dst03}
\end{figure}

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{dst04}
        \caption{Common File Format Support}
        \label{dst04}
\end{figure}

Presto is also an interactive SQL query engine. It runs on top of Hive, HBase, and even relational databases and proprietary data stores, thus combining data from multiple sources across the organization. Facebook is the main developer behind Presto and the company uses it to query internal data stores, including a 300PB data warehouse. Airbnb and Dropbox also use Presto, so it seems tried and tested for the enterprise.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{dst05}
        \includegraphics[width=0.5\textwidth]{dst06}
        \includegraphics[width=0.5\textwidth]{dst07}
        \caption{Data Sources, Data Types and Metadata}
        \label{dst06}
\end{figure}
%\noindent
%\\
%{\scriptsize
%\begin{tabular}{llllll}
%\textbf{SQL Mode} & \textbf{Hive} & \textbf{Drill} & \textbf{Impala} & \textbf{Presto}	& \textbf{Spark/Shark} \\
%\hline
%\hline
% &Batch &	Interactive & Interactive & Interactive	& In-memory\\
%& & & &  &/streaming
%\end{tabular}
%
%\begin{tabular}{llllll}
%\textbf{SQL ANSI Completeness} & \textbf{Hive} & \textbf{Drill} & \textbf{Impala} & \textbf{Presto}	& \textbf{Spark/Shark} \\
%\hline
%\hline
% &Batch &	Interactive & Interactive & Interactive	& In-memory\\
%& & & &  &/streaming
%\end{tabular}
%
%}

\subsection*{SQL-on-Hadoop Challenges}

\noindent
\\
\textbf{File Formats: }
While Hadoop supports storing all file formats, SQL-on-Hadoop technologies require data to be in rigid formats in order to process it. Therefore, they might not support storing and querying all of an organization’s data, which can arrive in various formats or in no format at all. Jethro and other tools require using their own file structure. Impala is fully compatible with text files and Parquet, a columnar storage format for Hadoop, while providing partial support for other formats. Presto is supposed to work with Hadoop file formats such as text, RCFile, and SquenceFile. Hive supports implementing a custom serializer/deserializer function that can read/write any file format, but this requires extra programming \cite{Mor}.

\noindent
\\
\textbf{Server Maintenance:}
Some SQL-on-Hadoop technologies such as CitusDB, Hadapt, and BigSQL require PostgreSQL to be installed on each node in the cluster. This could be cumbersome to deploy and maintain, especially when dealing with large clusters.

\noindent
\\
\textbf{Schema Maintenance:}
One of Hadoop’s advantages is the lack of schema. However, making SQL available on Hadoop requires defining and managing a schema, something which may present a problem when new data comes in that does not fit the schema. Hadapt, one of the SQL-on-Hadoop solutions, claims it does not require schema definition for self descriptive JSON or XML formats, but this ability is already available with standard SQL databases.

\noindent
\\
\textbf{ACID:}
SQL databases support ACID (Atomicity, Consistency, Isolation, Durability) to guarantee reliable database transactions. Hadoop does not support it, so it is up to the relevant technology to provide it, if it does provide it at all. Hive plans to support ACID in the future.

\noindent
\\
\textbf{OLTP:}
Since Hadoop is based on sequential reads and does not support updates, it is a lot more useful for On-line Analytical Processing (OLAP) by definition. Therefore, Hive, which is based on MapReduce, does not support On-line Transaction Processing (OLTP) since MapReduce does not do single row operations (future support is planned as part of ACID). Although other tools are not based on MapReduce, they still target analytical queries. HBase does provide transactional functionality, although it isn’t ACID compliant yet.

\noindent
\\
\textbf{SQL Functionality:}
SQL supports more features than just queries such as views, stored procedures, and user defined functions. Most SQL-on-Hadoop tools do not support them and require writing extra code instead - e.g. Java for Hive and C++ for Impala.

\noindent
\\
\textbf{Update Statements:}
Unlike SQL, HDFS does not support update statements. SQL-on-Hadoop tools may implement it, but it isn’t clear exactly how since it requires random read/write access to all the data on Hadoop, a feature that Hadoop does not provide. Maybe they implement it like HBase which uses in-memory indexes and compacts files once in a while to remove older versions.

\noindent
\\
\textbf{Joins and Dimensions:}
HDFS automatically manages how to spread blocks of data over the cluster, and this process cannot be controlled manually. In certain cases this could be counter-productive. Saving several pieces of data together on the same node or maybe on all of the cluster nodes could be necessary to help execute joins and dimensions more efficiently, data such as product names, categories, or clients. Otherwise it could take much more time to bring all the relevant data together from across the network.

Summary Hadoop works quite differently from SQL and requires learning new concepts and technologies. SQL-on-Hadoop tools can help bridge this knowledge gap. The best strategy though is to use each technology for its strength rather than to bend it into something else - SQL for transactional queries and Hadoop for batch processing.

\section{Conclusion}

A key and emerging example is the need for \emph{interactive query}, which today is challenged by the \emph{batch-oriented nature} of MapReduce. 
A key step to enabling this new world was \emph{Apache YARN} and today the community proposes the next step.

Tez is a framework that builds upon Apache Hadoop 2.0 (YARN).  YARN provides cluster management and resource allocation services and Tez provides a new AppMaster that processes a new job definition.  The Tez AppMaster calls on the YARN ResourceManager to allocate worker containers and calls the YARN NodeManagers to actually execute worker processes within the allocated containers.

\noindent
Tez provides \textit{runtime components}.
\bi\ii An execution environment that can handle traditional map-reduce jobs. \ii An execution environment that handles DAG-based jobs comprising various built-in and extendable primitives.\ii Cluster-side determination of input pieces.\ii Runtime planning such as \textit{task cardinality} determination and \textit{dynamic modification} to the DAG structure.\ei

\noindentTez provides APIs to access these services.\bi\ii Traditional map-reduce functionality is accessed via java classes written to the Job interface. \ii DAG-based execution is accessed via the new Tez DAG API.\ei

\noindentTez provides pre-made primitives for use with the \textit{DAG API}.\bi\ii Vertex Input,  Vertex Output\ii Sorting,  Shuffling, Merging\ii Data transfer 
\ei


We expect that the Tez dataflow definition API will be able to express a
broad spectrum of \emph{data processing topologies} and enable higher
level languages to elegantly transform their queries into Tez jobs.

As the Hive and Pig projects adapt to use Tez, we expect that the repository of various implementations of Inputs/Outputs/Processors will grow to house a common set of building blocks for use across the different projects.


\bibliographystyle{IEEEtran}
\bibliography{References}


\cleardoublepage

\section*{Appendix}
\lstinputlisting[label=sorted-output-code,caption=Tez LogicalInput,language=Java]{OnFileSortedOutput.java}
\lstinputlisting[label=merged-input-code,caption=Tez LogicalOutput,language=Java]{ShuffledMergedInput.java}
\lstinputlisting[label=kv-input-code,caption=Tez LogicalOutput,language=Java]{SuffledUnorderedKVInput.java}


\end{document}
