
\documentclass[twocolumn]{article}
\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{times}
\usepackage{titlesec} % 1
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
%\usepackage{sectsty} % "제 1 절" ...

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %                              My Commands
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\ii}{\item}
\newtheorem{Def}{Definition}
\newtheorem{Lem}{Lemma}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{graphicx}
\graphicspath{%
        {converted_graphics/}
        {./images/}
}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\usepackage{verbatimbox}

\usepackage[hangul,nonfrench,finemath]{kotex}
    
\setlength\textwidth{7in} 
\setlength\textheight{9.5in} 
\setlength\oddsidemargin{-0.25in} 
\setlength\topmargin{-0.25in} 
\setlength\headheight{0in} 
\setlength\headsep{0in} 
%\setlength\columnsep{5pt}
\sloppy 
 
\begin{document}

\title{
\vspace{-0.5in}\rule{\textwidth}{2pt}
\begin{tabular}{ll}\begin{minipage}{4.75in}\vspace{6px}
\noindent\large {\it KIWI Project}@Data Management Research Section\\
\vspace{-12px}\\
\noindent\LARGE ETRI\qquad  \large Technical Report 15ZS1410-TR-71
\end{minipage}&\begin{minipage}{2in}\vspace{6px}\small
218 Gajeong-ro, Yuseong-gu\\
Daejeon, 305-700, South Korea\\
http:/$\!$/www.etri.re.kr/\\
http:/$\!$/sungsoo.github.com/\quad 
\end{minipage}\end{tabular}
\rule{\textwidth}{2pt}\vspace{0.25in}
\LARGE \bf MapD 개요 \\
\large A Overview of MapD (Massively Parallel Database)
}

\date{}

\author{
{\bf Sung-Soo Kim}\\
\it{sungsoo@etri.re.kr}
}

\maketitle

\begin{abstract}
GPU 상의 범용계산 접근법 (GPGPU)은 일반적으로 컴퓨터 그래픽스를 위한 계산만 맡았던 그래픽 처리 장치(GPU)를, 전통적으로 CPU가 맡았던 응용 프로그램들의 계산에 사용하는 기술이다. 
기존 CPU 클러스터 기반의 하둡을 GPGPU기반 플랫폼으로 대체할 수 있을까? 
그래서 하둡과 같은 대규모 데이터 처리에 단순히 GPGPU의 기본적인 처리방식을 활용하면 성능개선을 기대하기 어렵다.
현재까지 GPGPU기반 데이터관리 사례로는 하버드와 MIT에서 개발한 대규모 스트림데이터 처리/분석/가시화를 수행할 수 있는 시스템인 MapD \cite{mapd:2015}가 있지만, 대부분 GPGPU기반 하둡 연구들은 실험결과 제시수준이다.
GPGPU를 통해 하둡기반 빅 데이터 시스템의 성능을 높이기 위해서는 GPU상에서 다루어야 할 \textit{데이터 저장/압축모델}, \textit{처리 방식} 뿐만 아니라, \textit{질의 처리} 방식도 GPU에 최적화되어야 한다. 

KIWI 플랫폼의 질의 처리에 필요한 내부 성능 개선을 위해 GPGPU 접근법을 고려하고 있다. 
이와 관련하여 본 기술문서에서는 MapD의 설계 철학, 구현에 적용된 기술들을 분석한다.
\end{abstract}

\section{Introduction}
In recent years, there has been a focus on using distributed clusters of computers to compute aggregates and other statistics for massive datasets. 
While these approaches, such as the popular MapReduce framework, are invaluable for extracting actionable information from petabyte-sized datasets, 
such methods typically require large amounts of hardware (even on a per-byte basis) and suffer from relatively high computational overheads due to the large amount of inter-node communication and synchronization involved. 
Moreover, for medium-to-large size datasets of less than a few hundred gigabytes, such methods introduce latencies that prevent real-time analysis of data, something immensely valuable in diverse fields such as business intelligence, disaster response, and financial market analysis, among others.The system described within, \textit{Massively Parallel Database} (MapD), takes advantage of the immense computational power available in commodity-level, of-the-shelf GPUs, originally designed to accelerate the drawing of 3D graphics to a computer screen, to form the backbone of a data processing and visualization engine that marries the data processing and querying features of a traditional RDBMS with advanced analytic and visualization features. 
As will be shown, MapD is a unique \textit{general-purpose modular system} that allows for real-time querying, analysis and visualization of “big data” in real-time on hardware ranging from sub-USD1,000 commodity laptops and desktops all the way to High Performance Computing (HPC) clusters with hundreds or thousands of nodes. Moreover, since MapD provides a vertically-integrated end-to-end solution for data querying, visualization and analysis – it avoids latencies that would otherwise be incurred formatting data and needlessly moving it in and out of ultra-fast GPU memory across the relatively slow PCI bus, which would be required if several different (GPU- based) programs were used to perform the same functions, each with their own protocols.

\section{Technical Background}
MapD is a software system that is designed to run on a hybrid architecture of GPUs and CPUs. Every modern computer has at least one CPU, and increasingly computer CPUs are divided into more than one sub-processing unit, called cores. It is not uncommon, as of early 2013, for a high-end laptop or desktop computer to have four cores, and many servers have up to 16 cores. The trend towards more cores has been propelled by the inability of computer chip designers to continue to increase chip clock speeds (i.e. the number of computational cycles a processor can complete per second), as the amount of energy required and heat produced increases super-linearly above a certain point, rendering further increases in clock speeds impractical. To counter this “clock-speed wall”, processor designers have resorted to other means to continue the inexorable exponential increase in processor speeds that both home and business consumers have come to expect. One strategy has been to increase the complexity of chips, improving the chips’ capacity for branch prediction (which allows data to be pre-fetched
from memory before execution) as well as deepening the processor’s pipeline, effectively increasing its throughput.The other strategy, as hinted at above, has been to increase the number of processing units in a physical chip. Theoretically, twice the number of processors allows for twice the number of instructions to be processed at once and thus provides twice the computational throughput. However, increasing the number of processors introduces a number of complications. First, some algorithms, such as compiling computer code within the same compilation unit, are very difficult to parallelize. Second, even the use of multi-core architectures to accelerate “embarrassingly parallel” architectures typically requires rewriting major parts of an existing codebase to take advantage of the extra cores. In between these two extremes lies the majority of algorithms, those that cannot be trivially parallelized, but with the right (likely non-obvious) algorithm, can still enjoy significant speedups when run in parallel. Such cases typically require even more extensive rewriting of existing code, as the algorithms used to process data may now be completely different than those used in the serial case.GPUs can be seen as the extreme embodiment of the second approach to overcoming the clock-speed dilemma. The modern GPU evolved in a race to provide the most cost-effective way to perform the massive amounts of computation necessary to render 3D graphics for video and console games – which typically involve hundreds of millions or billions of identical matrix operations per frame to compute the correct projection of vertices in 3D space onto a 2D screen and then fill the resulting polygons using a lighting-dependent shading algorithm.. Hence, emphasis was placed on endowing graphics cards with a large number of architecturally simple cores running at relatively slow clockspeeds, at least relative to the high-frequency, deep- pipelined, heavily-cached world of CPU cores. Moreover, since every core was designed to execute and perform the exact same operation in lockstep with every other core, GPUs ended up being built with a relatively low amount of control architecture, the part of the chip that enables the divergent execution of different branches of code. As such, the first programmable GPU was not offered until 2011, and even modern GPUs do not allow each core to execute independently (Nvidia’s architecture only allows for control at a resolution of 16 threads (processors), called a half-warp).GPUs compensate for these limitations by using a massive amount of cores. To date, state of the art consumer/gamer GPUs such as the Nvidia Titan might have 2,668 cores and be capable of 5 Teraflops of single precision performance and 1.48 teraflops of double precision performance. Two such cards installed together into a standard desktop computer would be faster than the fastest supercomputer in 2000, the ASCI RED, that took up 1600 square feet of space, used 850 KW of electricity, and cost USD55 million to build. In contrast, the two Titan GPUs described above cost approximately USD1,000 each, use a combined 500 watts of electricity at peak load, and can be easily be housed in a desktop computer system. The disparity between computational speeds over the last decade is due to the disruptive parallel architecture of the GPU. For instance, the performance of modern multicore CPU chips, offered by leading manufacturers such as Intel and AMD, is measured in tens of gigaflops, approximately a factor of 100 times slower compared to modern GPUs at a similar price point.The scientific community has in particular taken note of the computational potential of GPUs, and currently many workloads such as protein folding and physics simulations are
accelerated using GPUs. However, most of these programs are written as single-use scripts or programs and are not applicable to problems outside of the specific problem domain for which they were designed. Cross-purpose data management software platforms that use CPUs and - GPUs (as well as other massively parallel cards) in tandem to both perform data warehousing and analytic tasks have been slow to develop because:1 The parallelization process for most algorithms is difficult, particularly in the case of a GPU, which has less tolerance for divergent execution than a multi-core CPU;2 Contemporary programming languages and paradigms for GPUs are relatively under utilised and do not integrate well with existing codebases. For instance, writing code to run on GPUs requires a working knowledge of un-orthodox coding languages (in the sense that such languages are not typically taught in computer science programs), such as CUDA for Nvidia’s cards or The Kronos Group’s OpenCL;3 The database architecture most commonly used in conjunction with discrete GPUs requires the GPU to communicate with a CPU and transmit memory over the Peripheral Component Interface (PCI) bus, which under the current standard (PCI 3.0) can only transfer data at approximately 12 GB per second. Resultantly, by using MapD in conjunction with one or multiple GPUs or CPUs, data can be processed faster than in the case described above where computational speed is limited by the speed that data can be transferred to and from the CPU.4 Memory on GPUs is often limited compared to that found on the CPU. For example, state of the art GPU cards may have 6GB of GDDR5 RAM, whereas a high-performance server might have 128GB or even 256GB DDR3 RAM. The relatively limited amount of memory available on current GPUs is currently a serious constraint for GPU programmers.5 The majority of contemporary research focuses on the prevailing paradigm employed to process “big data”, Map-Reduce and cluster computing. These take advantage of the scalability and high thoroughput available when using massive amounts of hardware. Alternatively, MapD and a GPU-centric database architecture focuses on the ultra-low latency that can be achieved when processing big data using one or more GPUs.As described below, MapD uses many innovative techniques to overcome these limitations.DESCRIPTIONMapD uses a hybrid multi-CPU/multi-GPU architecture running across multiple nodes. This architecture allows for the massive parallelization of the querying, analysis, and visualization of big datasets, and results in an increased speed of processing, in the order of multiple orders of magnitude, across many of the various big data workloads common today. The MapD architecture works in conjunction with deep support for GPU-generated visualizations and allows for the practically real-time exploration of many large datasets by
multiple clients in ways that were formerly impossible. MapD utilizes a set of architectural, procedural and programming innovations to contemporary paradigms of big data analysis and computation to achieve ultra-low latency in the processing of big data workloads. Processing times are generally on the order of milliseconds as opposed to seconds, minutes or hours using other contemporary big database paradigms.The innovative features that are incorporated into MapD are, but not limited to:1. MapD is a GPU data analytical system that systematically exploits both multi-GPU architectures and multi-node architectures. Its design mitigates the costs associated with partitioning data both over multiple GPUs and multiple nodes.2. MapD uses a three-layer memory hierarchy where GPU memory acts as ultra-fast buffer pool atop of a CPU buffer pool, which in turn sits atop data stored on hard disk.3. MapD returns indexes or compressed bitmaps of matching rows from the GPU to the CPU after a filter operation rather than the post-filtered projected rows themselves. MapD thus overcomes the large GPU-to-CPU bandwidth costs associated with transferring these rows over the relatively slow PCI bus.4. GPU query functions are fused as much as possible to minimize memory access costs during GPU-assisted evaluation of queries. Previously GPU database designers have resorted to fusing kernels by hand (i.e. combining the ‘>’, ‘AND’, and ‘<’ operators into one function if this is a common workload for the database). In contrast, the first time MapD sees a certain type of query, it actually generates GPU assembly code for a given query plan and then compiles it to allow for the generation of arbitrarily complex fused kernels for queries that could not be anticipated at compile time. After this initial compilation, these compiled kernels are then stored by the system in a dictionary for future re-use so that the up-front cost of query plan compilation is not incurred again. Furthermore, compiled kernels are stored in file, only being deleted if they are not used again within a user-defined number of queries (the user can choose to never delete compiled kernels if disk and memory space is not an issue).5. Query optimization for complex queries often involves testing a huge number of possible query plans. Query optimizers found in modern databases typically “give up” after a predetermined amount of time, or constrain the search space to a limited subset of all possible query plans. Both techniques often result in sub-optimal query execution times. To address this issue, MapD experimentally uses the computational power of one or multiple GPUs to determine the optimal query plan for a given query, significantly increasing the number of query plans that can be run-time estimated in a given amount of time, thus increasing the likelihood that the most effective query is found, in turn improving overall system performance. Furthermore, MapD uses its built-in high-performance histogramming capabilities (see 6 below) to improve the statistics that the query optimizer uses (i.e. it improves selectivity optimization)6. The creation of an SQL extension along with supporting GPU query architecture to systematically handle the binning (histogramming) of data along an arbitrary number of continuously-valued dimensions, afterwards applying any SQL-standard or user-provided aggregate function, and then allowing for the post-processing of this binned aggregated data
using arbitrary (including user-defined) convolution filters. This allows the database client to request a heatmap of the percentage of data points in a certain area matching a given query in the same way as it allows for the generation of a time graph of minimum, maximum and average temperature for a given map extent. Convolution filters can then be used to perform functions such as spatial aggregation (via gaussian blur or box blur) or finding local extrema in the data. Finally, MapD allows for the systematic export of this post- binned/aggregated/convoluted data into a variety of formats, including but not limited to ordinary tabular format, CSV, JSON (useful when serving as a web server), PNGs (returned for Web Mapping Service requests), and finally as the basis for OpenGL rendering which can either be sent straight to the user’s screen if the MapD server is actually on the same node as the client, or out over a TCP connection via compressed H.264 video (where compression is performed on the GPU).7. An algorithm to conduct hyper-fast spatial joins (for example, finding the number of tweets in Texas when the tweets only contain coordinate information). This is done by using the GPU to rasterize in parallel a polygon vector file (such as a shapefile), and then pyramiding it into a lookup tree. Finding the polygon that contains a given point then becomes as simple as looking up the correct pixel in the raster tree and then returning the polygon ID for that pixel. Edge pixels are recorded as such and require an actual intersection test using the underlying vector geometries, but these cases are rare enough so as to have a negligible impact on the overall speed of the algorithm. Preliminary tests have shown speedups of over 1,000,000 times compared to in-memory implementations in PostGIS (a PostgresSQL extension), partly a function of the increased speeds of the GPUs (which are exceptionally fast at texture lookups), but mostly due to the design of the algorithm in question.8. A novel algorithm to store the content of text data using the products of prime numbers, leading to both computational speedups and significant data compression. This is called “prime encoding”.9. MapD allows for SQL query results computed using its database architecture to be seamlessly passed to various processing modules (such as an included graph analytic package). For example, a user can filter a dataset to filter only those records pertaining to Facebook users from California, and then a compute a real-time 3D force diagram on the results and associated link records, finally passing the data into the provided OpenGL rendering module. One big advantage of MapD’s vertically-integrated architecture as an end-to-end data querying, processing, analytic and visualization engine is that when possible, data never needs to leave GPU memory as it is manipulated by successive (and perhaps fused) kernels. This leads to huge speedups over competing solutions that are more piecemeal, even those that run part of their workloads on GPUs.10. A novel algorithm for performing deep geo-location on documents using machine learning algorithms on the GPU. “Deep geolocation” is here defined to be a means at getting a best estimate of the location of a document’s author - where location here is a time-weighted function of their past locations as well as their present position (a user that says “ain’t” might be from the Southern United States - but the fact that they repeatedly say “Eiffel Tower” and “Louvre” may suggest that they are currently in Paris). Previous geo-location systems have only relied on pre-generated dictionaries or gazateers containing place names, ignoring various language features that might provide a clue to the user’s location (for example, the
use of “wicked” near Boston, the use of “bra” instead of “bro” near Atlanta, etc.) The GPU processes a training dataset (such as a dataset of tweets and their geolocations), and then for any document, uses a spatial (2D) Bayesian classifier or other suitable machine learning algorithm to determine probabilities of a user belonging to a given set of N equally sized cell defined by an X and Y extent (a good choice might be a 4X4 grid covering the area in question) The algorithm recursively descends into a given cell if the probability of the document being produced in that cell was higher than a certain amount, stopping when the probability of all sub-bins of a given cell are approximately the same, suggesting that the algorithm could not obtain any more resolution on the document’s location (for example, some users may only be able to be located to the southwest). For other users however, the algorithm may not only be able to determine with high probability that not only are they from New York but also that they frequent a certain bar. The output of the algorithm can be either a probability kernel density heatmap (or pyramid of maps) or explicit probabilities that a user is from a given area represented as a polygon on the map (say a state or census district).11. A novel algorithm to determine “trends” in a dataset over any arbitrary extent in time or space. This is currently referred to as “deep trend detection”. Given a weighting function that weights uniqueness in time vs. space, the algorithm essentially uses the computational power available though GPUs to create a 4-D histogram (x, y, time, keyword) of percentage matches for the N most common words in teh dataset as well as the M most common bigrams for a given time and geographic extent. While the top-N unigrams are easy to determine over a given sample, the huge Cartesian space required to represent the space of all possible bigrams (NxN) requires the use of an on-GPU hashing algorithm to keep count of the top bigrams in the limited GPU memory space. It then uses t-tests to test for statistically significant increases in data points for a given term or phrase for a given cell relative to previous time bins for the same cell. The result is then put through a convolution filter to detect geographic blobs - i.e. sets of neighboring cells that all experienced an increase in relative frequency relative to past time bins. If all cells in a given spatial extent experience a similar increase - this is tagged as a trend that is uniform across geographic space. Trends that are confined to a certain group of cells, however, can be plotted on a map. While there are existing technologies to detect geographically-based trends on sources, such as the social network Twitter, due to processing and algorithmic limitations these are limited to only detecting trends in the current time frame and not on-the-fly and in with near-zero latency across time and space.12. The integration of a graph analytic module that takes filtered results, upon which various graph statistics can be computed (including but not limited to finding the shortest path length between nodes, the average path length between all nodes, and the centrality of a given node). The module also supports the modeling and rendering of iterative force diagrams of graph data. The results can be rendered in real-time using OpenGL or sent over a network connection as a list of positions or a pre-rendered JPG or PNG.13. Fast GPU-accelerated clustering and k-means functionality that can be applied to any result set.TECHNICAL DETAILS
There are a number of features used in MapD that have been implemented, to varying degrees, in existing RDBMS and data analytics packages. Occasionally, these have also been implemented as custom programs as part of larger big data paradigms such as the popular Map Reduce Framework. However, MapD, through a number of innovative architectural, procedural and programming features working in unison with existing paradigms of big data and GPU based database computation, creates a novel hybrid multi-CPU/multi-GPU database architecture for the computation of big data.The base of the MapD system is a hybrid CPU/GPU column-store relational database. A column-store database is a relatively new variant of a traditional RDBMS that stores its data in columns rather than in rows as in a traditional database. This architecture allows for both the more efficient processing of data on the GPU or CPU, or multiple units of both, as well as for optimal use of relatively limited GPU memory. GPU memory is optimized in that the columns, which are frequently used to filter on or aggregate on, can be cached in the memory of the GPU, while the data columns that are not frequently used can be stored in the more-abundant memory located on the CPU.Contemporary database paradigms are built around the concept of a buffer pool – an area reserved in CPU memory to cache certain blocks of records read from disk. The reason for this is that it is possible to read data from CPU memory orders of magnitude faster than to read it from a hard disk. However, since CPU memory is often smaller by an order of magnitude or more than the space available on a hard disk, it is important to design a database architecture that carefully and effectively selects which blocks of data or records are to be cached in CPU memory. Much research has been conducted on this subject, with various strategies developed to determine which data or records should be cached , or alternatively, which records should be the first to be replaced (via a replacement strategy) when new memory is read from disk to main memory.MapD is designed with this basic database architecture in mind. However, MapD does not use a dual-level memory model, where data moves between CPU memory and memory on a hard disk. Rather, the MapD architecture implements a three-level model of memory and data synchronisation. This three-level model is arranged in a pyramid, where each successive level of memory is slower computationally but larger in size than the last. The fastest and smallest level is the memory on the GPU, next is the memory on the CPU and, finally, the base of the pyramid, being the slowest but most plentiful, is the memory on the hard disk.Every level of MapD’s memory hierarchy is a mirrored subset of the level below it. For example, if a given dataset is one terabyte in size, 80 gigabytes of the most frequently used data or records might be mirrored and stored on the CPU memory. Then perhaps the most frequently used 24 gigabytes of those 80 gigabytes will also be mirrored and stored in GPU memory (given perhaps 4 GPUs with 6GB of memory each). See Figure 1 for an overview of MapD’s architecture.

%%
% Figure 1.

The atomic unit of memory management in MapD is the “chunk”, a user-definable section of a column, by default set to be 1,048,576 million ($2^20$) records, equaling roughly 4 megabytes of integers or 8 megabytes of double-precision floating point values. However, the chunk size of variable-length data such as strings (for instance, data strings might hold a sequence of user generated natural language text, among other things) is by default set to be smaller, at 65,536 ($2^16$) rows. This is because storing variable length data requires both storing the data itself (for example, the component letters in a word, i.e. “h-e-l-l-o”) as well as an indexed offset in the data array to know where a given data entry starts and ends. 65,536 rows is the maximum number that can be indexed with two bytes (a short integer) rather than four (a regular integer), so breaking up variable length data and storing indexed offsets in chunks of this size requires only half as much memory space to store the data’s offsets.The size of the chunks involves a compromise. As chunks get smaller, the MapD database is able to distribute data among the three memory levels described above with a greater level of granularity. However, the smaller the chunk, the slower the database can both read the chunk from a hard disk into the CPU memory and the slower it can transfer the
memory from the CPU memory to the GPU memory and back (since much of the costs of reading and transferring memory can be amortized across the entire length of data being accessed. For example, to begin a new read from a random memory location, disk-based hard disks must physically seek the disk head to the correct position. Since the head is already in the correct position for subsequent reads, these reads can occur much faster).In the MapD architecture, each chunk, if taken from a column that allows null values, also has a corresponding bitmap to represent null values. This typically takes only a small amount of space relative to the column itself (1 bit per row so 1/32th the size in the case of 4- byte integers). Further, all chunks for a given set of rows of the same table are combined into a table fragment. In addition to the chunks of its constituent columns, a table fragment also contains a bitmap representing rows that have been deleted from the database (again, requiring 1-bit per row). Periodically, the MapD system repacks a table fragment, removing the rows that are deleted and recompressing each of the table fragment’s constituent chunks. Note that a table in itself can be composed of any number of constituent chunks. See Figure 2.

%%
% Figure 2.
% Figure 2: The structure of a sample database table, split into M fragments, each which is split into N chunks, each with its own bitmap to record null values (if the column permits nulls). Finally, each fragment keeps a bitmap to record row deletions. The columns of fragments are periodically compressed by removing rows that are flagged in the row deletion bitmap.


There are two ways in which MapD can partition data. The first, the round-robin method, essentially takes an incoming row of data (inserted either via SQL “insert” syntax or by binary batch insert), splits the row into its constituent columns/fields, and writes each to the appropriate chunk of a table fragment for that column inside the CPU memory. These chunks cannot be moved to the GPU until full, until then – they are queried on the CPU. This prevents the costly memory overhead of transferring every record to the GPU and allows for the batching of transfers for greater net throughput. Note, however that updates and deletions to chunks on the GPU must be recorded on the GPU immediately – as queries on these “mature” chunks run on the GPU and thus could return inaccurate results if the null and deletion columns are not kept up-to-date on the Gpu memory. After this initial “incubation” stage in CPU memory, all chunks for a given column are treated as equals, with the database only keeping statistics on which columns are accessed more, keeping the chunks of these columns higher in the memory hierarchy. However, the user can issue a command to declare that the chunks of a given column should always be, or alternatively, never be held in GPU memory. This can be useful in cases in which the user requires queries for certain columns to run at a given speed, or when due to the data requirements of a certain application (such as prior knowledge that the latitude and longitude columns of a table will be always used when rendering heatmaps of datasets that include geo-location data), the user knows that certain columns will be involved in every query and does not want these columns to ever be moved off the GPU memory.
The second method is via explicit clustering of the data by the values of one or more variables (columns) of a table. MapD allows the user to choose the number of partitions as a settable parameter, but in default mode MapD dynamically chooses the number of partitions for a given table given both the table size and the query workload. All things being equal, the more rows a table has the more partitions it can viably support. Partitioning can be setup to run in simple mode, in which the data of the column to be partitioned on is sorted and then split into N sections. The average value of the maximum value of one partition and the minimum of the next is then chosen to be the partitioning point. So if table T is partitioned along column A, which as 100 values [1..100], and the number of partitions is chosen to be 4, partition points will be chosen at 25.5, 50.5, and 75.5. Alternatively, table partitioning can be done taking into account past query history, with the goal of clustering data that tends to be accessed together (by the same query) so as to minimize memory accesses and the number of GPU kernel functions that must be run.Clustered partitioning based on query history allows for the following:1) Since we know the range of values each clustered column in a table fragment can take, we do not need to scan table fragments with clustered column values outside of this range. This can lead to major speedups2) Table fragments corresponding to different clustered column ranges can be placed on different GPUs, maximizing the probability that one GPU in a multi-GPU system can handle a query by itself (the other GPUs in the system can be handling their own queries) - removing the cost of merging each GPUs result set (whether an aggregate value, a heatmap, or a table of data sorted on some value).HOW MAPD PROCESSES QUERIESAlthough the architectures for several GPU-accelerated databases have been laid out in various academic papers, these systems typically work on the assumption that the data to be queried always resides on the GPU, which may be an unrealistic assumption for real-world query workloads and database applications, or that data is just blindly streamed to and from main CPU memory query, leading to only modest speedups due to the disproportionate time that CPU-to-GPU and GPU-to-CPU transfers over the PCI bus require relative to GPU execution time. MapD allows for the streaming of data to-and-from the systems GPUs such that there is no “cap” on the maximum amount of data that can be processed for a given query. However, unlike previous systems, it attempts to minimize PCI transfer by only sending compressed bitmaps or index maps of post-filtered results across the bus. Furthermore, since GPU and CPU execution can occur asynchronously, MapD’s query planner runs query workloads that run relatively faster on the CPU, such as text query using an inverted index, on the CPU while performing queries that run relatively better on the GPU, such as brute force table scans, simultaneously on the system’s GPU(s).The following steps through what occurs when a typical SQL query arrives to the system, whether over a normal TCP connection or as part of a HTTP request via MapD’s REST or Web Mapping Service (WMS) APIs (see Figure 3 at the end of this section for more detail).
1) The query is parsed and an Abstract Syntax Tree (AST) is produced according to established practice, with one node per abstract operation on the data;2) The query optimizer attempts to determine the quickest way to execute the query, taking other query load on the system into account. To do this, it must determine which chunks are in GPU memory, CPU memory, and on a hard disk. As noted above, it attempts to separate the query plan into parts and process on the GPU and CPU those parts that are most relatively faster on those components. It also must determine which parts of the query are dependent on intermediate result sets being materialized and generate plans accordingly.a) Query optimization requires accurate statistics on the distribution of data within a given table to produce good results (i.e. to estimate how many result tuples will be produced by a given filter operation). To that end, MapD works to keep up-to-date statistics (histograms) of the data it stores (selectivity estimation) - using periods of low query activity to analyze ist data using the same query architecture it uses to handle actual queries produced by clients (i.e. it runs query simulations).b) During this optimization process, the query optimizer can reorder the AST to the extent allowed by the rules of relational commutativity to speed up the query. This often involves “pushing” operations like filters to the bottom of the tree so that any joins (typically costly) occur over the minimum amount of records possible.c) Given a complex enough query, the number of possible query plans can become extremely large, increasing exponentially with every new node in the AST. Given such queries, modern databases will typically estimate execution times for only a fraction of the total possible query plans, attempting to find a “decent” query plan if not a globally optimal plan. To address this shortcoming of traditional database architectures, MapD uses the system’s GPU(s) to speed up query plan evaluation, greatly increasing the number of possible query plans that can be considered by a database over a certain amount of time.3) Using a hash of the chosen query plan (including the data types of the columns to be queried/processed), the database then checks to see if it has stored compiled GPU and CPU code to execute the query, which it stores in a dictionary mapping function definition hashed to function pointers to compiled code. If it has the compiled code cached, it immediately executes the query plan. If not, MapD will then generate GPU and CPU assembly code based on the general query plan. (Sometimes only one part of a query plan will need to be compiled) To the extent possible, it attempts to create a single fused function/kernel for the entire query plan. Sometimes this is not possible and intermediate data must be materialized (i.e. written to memory - a relatively resource costly process), before starting a new kernel to continue processing the query (this frequently occurs in the case of joins, where post-filtered results from both join tables must be written to memory before evaluating the join. Once the assembly for the kernel is generated, it is linked using existing compiler functionality (for example, nvcc for Nvidia’s CUDA API) and a function pointer to the kernel is stored in the kernel dictionary described above. Thus, compiling code for a novel query entered into the system is only an up-front cost, which due to the pre- compilation of assembly code performed by the database itself is a relatively fast process.4) The compiled kernels are then executed - typically simultaneously on the node’s CPUs and GPUs, according to the query plan.
5) The last part of the query plan may involve computing aggregates on the filtered and joined data. If so, then this generally results in a result set that only occupies a tiny fraction of memory that the original dataset and the resource overhead of GPU-to-CPU memory transfer is negligible. However, there are cases in which the user requests one or more non- aggregated columns. Within other constraints, MapD endeavours to minimize GPU-to-CPU memory transfer due to its relatively resource costly nature. GPU-to-CPU memory transfer generally falls into the following subcategories:a) After the computation of aggregates, as described above, these aggregates (whether the product of a GROUP BY or a BIN BY query (described below) - normally have a small memory footprint relative to the dataset they are derived from. In such cases the resource overhead of PCI memory transfer is negligible.b) The query involves filtering by some criteria but the projected columns are returned unaltered (for example, the query “SELECT * from geo\_tweets where lat > 30.0 and lat < 50.0 and lon > 30.0 and lon < 50.0 and time > ‘2012-08-01 11:54’ and time < ‘2012-08- 02 11:54’, the * telling the database to return all columns that match the filter criteria unaltered). In such cases, the database only needs to return the indices of the rows of the table fragment that pass the filter test rather than all of the projected columns. If the percentage of results relative to the entire result set is high, it may make most sense to return a bitmap of the matching rows, one bit per row, where each bit is set to either True or False depending on whether the row passed the filter test. The bitmap has the advantage of being the raw form that the GPU uses to compute results, with each thread in the GPU writing its row’s result directly to that row’s indexed offset in the bitmap, requiring no thread coordination. However, if the result set is a small percentage of the total dataset, it may make sense to only return the indices of the matching row in integer form, which is generated from the bitmap described above via a parallel prefix scan. Furthermore, the database can run-length encode (compress) these indices (currently implemented with Rice Encoding) to save even more bandwidth. For instance, in a case where a filter operation on a billion rows only returns one thousand results. Instead of returning a billion-bit bitmap (1 billion/8 ~= 125 megabytes), the database would only have to return 1,000 4-byte integers (4 kilobytes). Aside from the drastically lower bandwidth required to transfer the indexed list from GPU to CPU, the CPU also now has a much smaller result list to scan. However, to transform the raw bitmap into a list of integer indices, a relatively costly parallel-prefix scan operation must be performed. The actual strategy chosen taken at query time is left to the query optimizer, which makes its decision based on estimated costs of each step (in turn based on computed statistics of data distribution as well as other benchmarks (such as the actual PCI transfer speed of the system in question as well as the relative difference between GPU and CPU processing power).c) The query involves the computation of derived results on the post-filtered projected columns. This means that all derived columns computed on the GPU, and not just the indices of the rows that pass the filter test, need to be returned to CPU memory. If a LIMIT clause is invoked (limiting the number of rows returned), only the specified number of derived rows need to be returned, lessening the bandwidth transfer. MapD’s
Figure 3query optimizer will push more computational load on the CPU to compensate for the increase bandwidth required as needed.

%%%%%%
% Figure 3.

BINNING/HISTOGRAMMING + CONVOLUTION FUNCTIONALITYWhile the current SQL standard allows for aggregation by discrete values (via the GROUP BY) operator, it is very difficult to easily specify the creation of arbitrary histograms across an arbitrary number of dimensions, each a continuous variable. Consider the following use case: the client wishes to generate a heatmap showing the average temperature over a given geographic extent for a certain time range. Although this is possible using ordinary SQL, generating such a query is incredibly unwieldy. The SQL query to perform compute the data necessary to populate such a heatmap is the following:
Expr 1

\noindent
SELECT $(x - x_{min})/(x_{max} - x_{min}) * num_{x_{bins}}$ \\
AS $x_{bin}, (y - y_{min})/(y_{max} - y_{min}) * num_{y_{bins}}$\\ 
AS $y_{bin}$, COUNT(*) AS $N$, AVG(temp) AS avg\_temp \\
FROM weather\\ 
WHERE date $>=$ ‘2012-07-01’ AND date $<=$ ‘2012-07-07’ \\
GROUP BY $x_{bin}, y_{bin}$\\ 
HAVING $x_{bin} >= 0$ \\
AND $x_{bin} < num_{x_{bins}}$\\ 
AND $y_{bin} >= 0$\\ 
AND $y_{bin} < num_{y_{bins}}$\\ 
AND $N > min_n$;\\This query bins the data into a 2D histogram (by x and y) for all weather observations for a given spatial and time extent. The number of bins, as well as their bounding points, is determined by the variables $x_min, $x_max, $num_x_bins, $y_min, $y_max, and $num_y_bins. Note that the HAVING clause, aside from limiting the result set to the correct bins, also performs a post aggregation filter on each bins count (aliased to N), specifying that the cell has at least a minimum number of observations ($min_n) for the bin to be included in the result set.Now consider MapD’s histogram functionality. In MapD, the query above could be expressed as:Expr 2SELECT x, y, COUNT(*) as N, AVG(temp) as avg_temp FROM weather WHERE date >= ‘2012-07-01’ AND date <= ‘2012-07-07’ BIN BY (x, $x_min, $x_max, $num_x_bins), (y, $y_min, $y_max, $num_y_bins) HAVING N > $min_n;Note how the BIN BY operator, given the appropriate minimum, maximum, and bin cardinality parameters for each dimension, automatically allows for the computation of the appropriate bin of the data (performed as a projection in the SQL query), in addition to handling the bounds checking performed by the HANDLING operator in the SQL query.While the example above computes a binned aggregate across two dimensions (the variables x and y), the same methods abstract to any number of dimensions. For example, the user can generate a time graph of the percentage of tweets matching a given query term via:Expr 3
SELECT time, AVG(tweet_text ilike ‘%$query%’) from tweets BIN BY (time, $time_min, $time_max, $num_time_bins);Or alternatively, a 3D histogram of batting average for batters against a certain pitcher, binned by the x and y coordinates that the ball crosses the plate and the speed of the pitch:Expr 4SELECT x, y, speed, AVG(was_hit) FROM at_bat_stats WHERE pitcher = ‘$pitcher_name’ BIN BY (x, $min_x, $max_x, $num_x_bins), (y, $min_y, $max_y, $max_y_bins), (speed, $min_speed, $max_speed, $num_speed_bins);Note that when using this functionality to render two-dimensional heatmaps for on- screen-display, the number of bins in each dimension often equals the size in pixels of the frame being requested from the MapD server.MapD also allows for the integrated specification of convolution filters that are run on the post-binned data. A convolution filter applies a given function on every bin in a histogram over any number of dimensions. Typically the input into the convolution filter for a given bin is the bin’s value along with the values of any number of neighboring bins. The convolution filter can optionally be parameterized such that it only considers the N closest bins along any given dimension.MapD allows for the user-defined convolution filters. Typically this is done by providing MapD with a D-dimensional matrix that expresses the weights that each bin within a [N0,N1... ND] bin neighborhood of a bin contributes to the new value for that bin. However, the user can also provide a coded User Defined Function (UDF) to perform custom operations not expressible by a convolution matrix.Convolution functions in MapD are defined using the CONVOLUTE operator, which is applied to the result of any bin operation using nested query syntax. Taking our previous example of the computation of average temperature across a geographical region:Expr 5SELECT CONVOLUTE(SELECT x, y, COUNT(*) as N, AVG(temp) as avg_temp FROM weather WHERE date >= ‘2012-07-01’ AND date <= ‘2012-07-07’ BIN BY (x, $x_min, $x_max, $num_x_bins), (y, $y_min, $y_max, $num_y_bins) HAVING N > $min_n, GAUSSIAN($sigma), 100, 100);The first thing to note is that the inner query of this example is the same as the query in Expr 2. However, this inner query is wrapped in a convolution query. The first parameter specifies the input(required to be an SQL function with the BIN BY operator), the second parameter is the convolution function to be applied (a Gaussian blur in this case) – optionally parametitized, here $sigma is the standard deviation of the blur function (the second parameter
can also be a matrix rather than a named function as described before). Finally, the parameters 100, 100 specify the maximum convolution window (in x and y) that should be considered in determining any bin’s post-convolution output value.The output format for a binning/histogram query can be specified by the user/client. This is useful because frequently, the goal of a binning query is to generate a visualization, whether rendered as a static image and returned to a web browser, or perhaps pushed into the GPU’s graphics pipline for real-time manipulation in 3D, or alternatively, rendered as a video (for example, animating using the third time dimension of a heatmap over some spatial extent defined over the variables x and y). By default, the results of the binning operation are pushed back to the CPU (for queries that run on the GPU). However, the exact output destination (as well as relevant parameters) can be specified via the OUTPUT TO operator that is placed at the end of the full query. Current options include but are not limited to OUTPUT TO TABLE (default), OUTPUT TO CSV, OUTPUT TO JSON (useful for web clients), OUTPUT TO PNG, OUTPUT TO JPEG, OUTPUT TO MP4, OUTPUT TO H264, OUTPUT TO OPENGL and OUTPUT TO DIRECTX. (The program allows for its users to write modules to output to any other format as well). For the image formats (PNG and JPEG), the data must have been binned over either three or less dimensions. For the image and video output options, the system by default (or explicitly set using PROJECT TO HEAT) depicts the “dependent” variable on a color gradient, specifiable either by naming a colorramp preset or by providing a list of 4-member tuples [\% maximum, [red, green, blue]] where \% maximum lies within the range [0-100], while [red, blue, green] are each specified between [0-255]. However, when provided the PROJECT TO RELIEF parameter, the system instead renders the output with the dependent variable projected into another dimension. Thus a graph of some aggregated attribute against time becomes a proper time graph, while an aggregate binned over two dimensions becomes a 3D relief map. With relief maps, two independent variables can be depicted simultaneously by using “height” to show the distribution of one and “color” for the other. In general, only histograms over two dimensions or less can be rendered as relief maps, and 3D maps over two independent dimensions requires the use of OpenGL or DirectX on the GPU for 3D rendering.In addition to linear, log, standard deviation, and other scalings supported by all output formats (specified by including the appropriate function in the SQL (or other supported query format) itself – i.e. LOG(AVG(Z) or STD_DEV(AVG(Z)), the heatmap output option supports Jenks clustering as well as other scalings that require a binned (i.e. non continuous) output/dependent variable. Also, in the case of the PROJECT to HEAT option, the system allows for the client to request a legend identifying how each color or range of colors matches with each range of values in the output data. This can be outputted either in raw data format (i.e. JSON), or as an actual png image for the client.OUTPUT TO OPENGL and OUTPUT TO DIRECTX allow for the output of the histogram to a 3D Graphics context, after which the context can be manipulated in real-time using mouse or other input methods (such as a joystick). This is possible since context switching between GPGPU compute APIs such as CUDA and OpenCL and GPU rendering APIs such as OpenGL and DirectX is very quick. This is particularly useful when attempting to view a three- dimensional histogram. In such a case, the computed histogram can be made semi-transparent
(the degree of transparency controllable by the user in real-time) and then manipulated (rotated and translated), as well as cut into cross-sections, in real time. Note that this mode requires a monitor to be connected to the GPU server so that the OpenGL or DirectX API can output directly to screen. Also note that OUTPUT TO PNG, OUTPUT TO JPEG, OUTPUT TO MP4, and OUTPUT TO H264 may also use either the OpenGL or DirectX APIs for rendering, except that instead of rendering the output directly to screen the rendered buffer in GPU memory is transformed into the appropriate output format and then sent out over a network connection.Until now only the system’s capabilities to generate and output binned histograms, as well as the interface it provides for clients to access such capabilities, have been described. While this interface allows for the generic specification of a histogram to be generated over an arbitrary number of dimensions (i.e. an arbitrary set of continuous variables), there is a fair bit of specialization in the back-end application (MapD) to handle such requests in the most efficient manner possible to maximize throughput and minimize latency.Much of this specialization revolves around attempting to take advantage of higher- speed shared memory on the GPUs, which is much faster than a GPU’s global memory but also typically much more limited in capacity. Shared memory is specific to each Streaming Multiprocessor (SM in Nvidia and ATI terminology), To do this, MapD first must programmatically determine the amount of shared memory and global memory on all GPUs installed on the host system. Then, when a client request to generate a binned histogram arrives to the system, MapD determines if there is enough shared memory capacity to generate the histogram in shared memory rather than global memory. If there is enough shared memory to construct the histogram of the user-requested size, MapD uses shared memory, merging each SM to global memory as a final step. Otherwise, each GPU thread writes to global memory.

Figure 4, Figure 5, and Figure 6 depict real examples of MapD-generated heatmaps, point maps, and time graphs for queries over hundreds of millions of tweets. Note that pointmaps are also generated using the binning and then convolution filter method outlined above (in this case, marking the bin as inside a “point” if any any bin within a user-defined raidius has a matching tweet count greater than zero. All of these maps were generated within milliseconds on a 4-GPU system, orders of magnitude than a CPU-only system could accomplish the same task.%%%%
% Figure 4- Pointmap, Heatmap, and Time Graph showing flu outbreak in December 2012 in the American South
% Figure 5 – Heatmap and Pointmap of tweets containing “Central Park” near Manhattan. The heatmap shows the percentage of tweets containing the place name, suggesting the utility of the “deep geolocation” method described above.
% Figure 6 – Heatmap and time graph of percentage of tweets containing the word “snow” in December 2012
% Figure 7 – Heatmap of points per shot over x and y court position for nearly one million NBA shots.

SYSTEM OF USING RASTERS TO TEST FOR SPATIAL INTERSECTIONSAt the core of MapD is a system to rasterize vectorized polygon input such that spatial lookups can be performed near-instantaneously. This system currently takes ESRI shapefiles, an industry-standard format to represent geospatial data (polygons, lines and points) and maps each of the polygons contained in the file to a texture file of user-specified dimensions, with each “pixel” containing an integer value specifying the ID of the polygon at that pixel. This ID can then be joined with a table packaged with the shapefile in dbf format to get the name and other attributes of the polygon. To determine the “parent” polygon of a point, the point is simply projected onto the map and the ID value for that pixel fetched. A further refinement of the system involves pyramiding the raster into a raster tree - where each pixel references a 16X16 child raster. If a pixel lies entirely inside of a polygon, the ID of the polygon is simply recorded at that branch of the “tree” terminates. Otherwise, an offset to the start of the pixel’s child raster is recorded. Such a method saves a significant amount of space at the extent of requiring the GPU to perform multiple texture lookups per query. Since real-world polygons are not neatly defined by square pixels, the tree is terminated at a certain depth for a given pixel if the pixel at
that point is not entirely inside a polygon (which might occur if the centroid of a pixel lies exactly on a polygon edge). If 100% accuracy is not required, the database can report these points as lying on a polygon edge and not determine the actual container polygon for these “edge” cases (with a sufficient sized raster tree, this typically only occurs for < 1% of points). However, if 100% accuracy is required, the system can then perform a standard spatial intersection test on the edge cases, restricting the candidate polygons to those that border the edge in question. Although the method of testing for point-to-polygon intersections is described here, lines or polygons can also be tested for intersection in the same matter by looking up their constituent points. As noted above, speedups of over 1,000,000 times result from using this algorithm on the GPU versus an in-memory implementation of spatial joins in the popular PostgreSQL GIS extension PostGIS.It is notable that MapD can rasterize polygons in parallel. This is done using the following algorithm:(a) Preparing table containing the coordinates of a polygon’s vertices and the id of the owner polygon for those vertices(b) Using one GPU thread per pair of vertices, determine and store the number of pixels that like between the two vertices given the desired raster size(c) Use a parallel prefix scan to compute each thread’s correct memory offset to write the edge pixels that lie between the pair of vertices to(d) Each GPU thread now determines the actual edge pixels that lie between its pair of vertices and writes them into its assigned memory slots given by the offsets determined in c)(e) Sort this output by polygon id and then by row(f) Assign each thread to an outputted edge pixel – the thread then writes tomemory True if the previous pixel is on a different row, signifying the start of anew row.(g) Again use a parallel prefix scan to build a dense index of the offsets into theedge list of the start of every row by polygon.(h) Assign a thread to each of these offsets, which will scan until the next offset(representing the next row for the polygon)(i) During the scan, the thread will increment an “edge” counter every time itencounters an edge on that row.(j) If the edge count is odd, the thread fills in the X by Y raster (representing therasterized map) with the polygons id until it encounters the next edge

%%%%
% Figure 8 - a (visualized) example of MapD’s rasterization of a shapefile containing world polygons

PRIME ENCODINGAt its most basic level, text search/Information Retrieval (IR) usually involves converting text to an inverted index in which each word is a key for a sorted list of all document ID’s containing that word, perhaps with the addition of information about where the word was found in the document. Although inverted-lists can be compressed to less than one byte per document ID per index term using some algorithms, there is a significant computational cost to decompressing such data which varies by the compression method used. Moreover, due to the serial nature of such run-length compressed listings, it is difficult to efficiently access inverted lists on the GPU unless done as batch queries.MapD instead uses products of primes to encode text. To do this, a sample corpus is first generated that is seen to represent the type of text to be encoded. The unique set of words from the corpus are then ordered according to their frequency of occurrence in the corpus. Each unique word key is then assigned a prime number such that the most frequent word is assigned the prime number “2”, the next most frequent (usually “the”), “3”, and so on. The word frequencies can be recomputed at any time and new prime encodings generated for each document to reflect the new ordering, although this is found to be of limited effectiveness after a fairly small corpus is ingested due to the relatively static nature of the relative frequencies of unigram use in language. This set of associations between word key and prime number value is then kept in memory as a hash map. Then, to encode a document, MapD looks up the prime value for each word in the document text, multiplying that value by the product of the primes of all words that came before it. These products are stored as sets of unsigned 8-byte integers; when none of the integers in a document’s existing set can be multiplied by the next prime without overflowing a new integer is added to the set. Then, when MapD needs to test whether a certain word is in a document, it simply looks up the word’s prime in the hash map and then divides each prime product (the 8-byte integers) by the word’s prime, testing for a zero
remainder (signifying that the word was in the document). In practice, a bit-shift trick is employed that is significantly faster to compute than the relatively costly modulo operation.In practice, prime-encoding text results in significant compression of the text, reducing the approximately 80 bytes of UTF-8 encoded text to an average of 15.07 bytes or less than two unsigned long integers, besting the bytes per word ratio of the best inverted index compression techniques by a factor of three. This is the case even though only one prime hash map was used for all languages in the Twitter dataset, which contains significant non-English content. Further compression could be likely achieved by using different hash maps for different country groups (easily achieved using MapD raster lookup system) such that a prime mapping for the Arabic countries would associate common Arabic words with low primes, thus achieving greater compression. Also, common IR techniques, such as removing common words such as “and” (stopwords) and stemming tokens (i.e. transforming cars à car) would significantly reduce the total memory footprint required. Finally, an algorithm is constructed that would dynamically rearrange prime products to minimize the number of 8-byte integers needed to encode a given document. See Figure 10 for a depiction of how prime encoding works to encode and query a sample document “To you, I’m Me.”

%%%%
% Figure 9 – Prime Encoding

%In this section, we provide a brief overview of both a baseline GPU architecture, based primarily on NVIDIA’s G80/Fermi architecture, and GPGPU programming using the CUDA programming model.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{system-overview.pdf}
%        \caption{A system overview with CPU and a discrete GPU.}
%        \label{fig:system_overview}
%\end{figure}
%
%Figure \ref{fig:system_overview} shows how a GPU is typically connected with a modern processor \cite{Kim:2012}. A GPU is an accelerator (or a co-processor) that is connected to a host processor (typically a conventional general-purpose CPU processor). The host processor and GPU communicate to each other via PCI Express (PCIe) that provides 4 Gbit/s (Gen 2) or 8 Gbit/s (Gen 3) interconnection bandwidth. This communication bandwidth often becomes one of the biggest bottlenecks; thus, it is critical to offload the work to GPUs only if the benefits of using GPUs outweigh the offload cost. The communication bandwidth is expected to grow as the CPU bus bandwidth of the system memory increases in the future.
%
%\subsection{An Overview of GPU Architecture}
%Figure \ref{fig:nvidia-g80} illustrates the major components of a general-purpose graphics processor, based on a simplified diagram of G80. At a high level, the GPU architecture consists of several streaming multiprocessors (SMs), which are connected to the GPU’s DRAM. (NVIDIA calls an SM and AMD calls a Compute Unit (CU).) Each SM has a number of single-instruction multiple data (SIMD) units, also called stream processors (SPs), and supports a multithreading execution mechanism. GPU architectures employ two important execution paradigms, which we explain below.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{nvidia-g80.pdf}
%        \caption{Block diagram of NVIDIA’s G80 graphics processor (the components required for graphics processing are not shown).}
%        \label{fig:nvidia-g80}
%\end{figure}
%
%\noindent
%\textbf{SIMD/SIMT} 
%GPU processors supply high floating-point (FP) execution bandwidth, which is the driving force for designing graphics applications. To make efficient use of the high number of FP units, GPU architectures employ a \textit{SIMD} or \textit{SIMT} execution paradigm. In SIMD, \textit{one instruction} operates on \textit{multiple data} (i.e., only one instruction is fetched, decoded, and scheduled but on multiple data operands). Depending on the word width, anywhere from 32, 64, or 128 FP operations may be performed by a single instruction on current systems. This technique significantly increases system throughput and also improves its energy-efficiency. This SIMD-style execution is essentially the same technique used in early vector processors. To support SIMD/vector operations, the register file should provide high bandwidth of read and write operations.
%
%At a high level, the GPU programming model is based on the \textit{Single Program Multiple Data} (SPMD) model; a kernel function defines the program that is executed by each of the thousands of fine-grained threads that compose a GPU application. 
%%Since the programming model is SPMD, most of the threads perform the same work. Hence, a group of threads are executed in a lock-step fashion, executing the same instruction (on different data). This microarchitectural grouping of threads, which can affect both control flow and memory access efficiency, introduces the concept of \textit{warp}—a group of threads that are executed together in a lock step.
%
%The execution model of G80 is called \textit{SIMT} (single instruction multiple threads). SIMT is very similar to SIMD with slight differences. When programmers write code, they can treat each thread separately. The program model allows each individual thread to perform different work. In contrast, in SIMD, the vector width is determined by the ISA level and one single instruction must perform the fixed vector width data at the same time. 
%
%\noindent%\textbf{Multithreading} The other important execution paradigm that GPU architectures employ is \textit{hardware multithreading}. As in more conventional highly multithreaded architectures, such as HEP, M-Machine, Tera MTA, GPU processors use fast hardware-based context switching to tolerate long memory and operation latencies.
%%The effectiveness of multithreading depends on whether an application can provide a high number of concurrent threads. Most graphics applications have this characteristics since they typically need to process many objects (e.g., pixels, vertices, polygons) simultaneously. Multithreading is the key to understanding the performance behavior of GPGPU applications. In conventional CPU systems, thread context switching is relatively much more expensive: all program states, such as PC (program counter), architectural registers, and stack information, need to be stored by the operating system in memory. However, in GPUs, the cost of thread switching is much lower due to native hardware support of such process. Although modern CPUs also implement hardware multithreading (e.g., Intel’s \textit{hyperthreading}), thus far the \textit{degree} of multithreading (the number of simultaneous hardware thread contexts) is much lower in CPUs than in GPUs (e.g., two hyperthreads vs. hundreds of GPU threads).
%%To support multithreading in hardware, a processor must maintain a large number of registers, PC registers, and memory operation buffers. Having a large register file is especially critical. For example, the G80 architecture has a 16 KB first-level software managed cache (shared memory) while having a 32 KB register file. This large register file reduces the cost of context switch between threads. This fact is a key distinction from conventional CPU architectures.
%
%\subsection{Design of GPU Architecture}
%This section sketches the design of current GPU architectures. Much of this discussion pertains to NVIDIA’s Tesla/Fermi architectures. However, our descriptions are not intended to correspond directly to any existing industrial product.
%
%\subsubsection{GPU Pipeline}
%Figure \ref{fig:streaming-processor} shows an overview of a GPU architecture pipeline. It shows one streaming multiprocessor, which is based on an in-order scheduler. Similar to traditional architectures, it has fetch, decode, scheduler, register read, execution, and write-back stages.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{gpu-streaming-pipeline.pdf}
%        \caption{An overview of GPU streaming multiprocessor pipeline.}
%        \label{fig:streaming-processor}
%\end{figure}
%
%\noindent
%\textbf{Fetch and Decode Stage}%The front-end is very similar to traditional multithread architectures. Multiple PC registers exist to support multiple warps. The scheduler selects a warp to fetch based on scheduling algorithms, such as round-robin or greedy-fetch. The round-robin policy selects a warp from the list of ready warps, which gives an equal priority to each warp. In the greedy-fetch policy, the streaming multiprocessor fetches instructions from one warp until a certain event occurs such as I-cache miss or fetching a branch instruction or an instruction buffer full. Since the front-end has multiple warps to fetch, when it encounters such events, it simply switches to fetch another warp. For the same reason, branch predictors play a diminished role and are not typically implemented. Newer GPUs execute multiple warps at one cycle, so the front-end could fetch instructions from different warps at the same cycle instead of one warp at one cycle.%%After an instruction is fetched, it is decoded in the decode stage. The streaming multiprocessor can have an instruction buffer for each warp or share a buffer for all warps.
%
%\noindent%\textbf{Scheduler and Score Boarding} %The GPU processor has an in-order scheduler. In the G80 architecture, it executes only one warp at a time; later architectures like Fermi schedule multiple warps. The scheduler uses a scoreboard to find a ready warp. So far, no GPU architectures have employed out-of-order schedulers. However, the scheduler can select any warps that are ready. Hence, from a programmer’s view point, a program might look like an out-of-order execution. For example, the scenario in Figure \ref{fig:intruction-traces} is possible.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{instruction-traces.pdf}
%        \caption{An example of static and dynamic instruction traces.}
%        \label{fig:intruction-traces}
%\end{figure}
%
%\noindent
%\textbf{Scoreboarding} \textit{Scoreboarding} is one method to implement dynamic scheduling. It was first introduced in CDC6600. It checks read-after-write and write-after-write data dependencies. Scoreboarding does not provide the register renaming mechanism, but instructions can execute out of order (i.e., instructions should be scheduled in-order but can be finished (complete functional units/memory accesses) out of order) when there are no conflicts and the hardware is available. In GPUs, scoreboarding is used to check any RAW or WAW dependency, so instructions from the same warp can be executed even if earlier instructions have not finished yet. This approach increases instruction/memory-level parallelism. 
%
%\noindent
%\textbf{Register read/write}%To accommodate a relatively large number of active threads, the GPU processor maintains a large number of register files. For example, if a processor supports 128 threads and each thread uses 64 registers, in total 128×64 registers are needed. As a result, the G80 has a 64 KB register file and the Fermi supports 128 KB of register file storage per streaming multiprocessors (in total, a 2 MB register file). The register file should have a high capacity and also high bandwidth. If a GPU has 1 Tflop/s peak performance and each FP operation needs at least two register reads and one register write, 2 T*32 B/s=64 TB/s register read bandwidth is required. Providing such high bandwidth is particularly challenging, so several techniques have been used, including multiple banks and operand buffer/collectors.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{register-file.pdf}
%        \caption{GPU register file accesses. Left: using multiple banks; right: using operand buffering}
%        \label{fig:register-file}
%\end{figure}
%
%\noindent
%\textbf{Multiple banks} 
%The streaming multiprocessor provides high bandwidth by subdividing the register file into multiple banks. 
%Figure \ref{fig:operand} shows the register file structure. All threads in the warp read the register value in parallel from the register file indexed by both warp ID and register ID. Then, these register values are directly fed into the SIMD backend of the pipeline. (Please remember that each SIMD unit/lane is used by only one thread.)
%
%\noindent%\textbf{Operand Buffer} 
%Gebhart et al.  show another example of the register file structure in Figure \ref{fig:operand} . In that design, a buffer exists between the register file and the execution units. Instead of
%reading all the necessary register values right before the values are needed, which gives a very high pressure to the register file read/write ports, the processor can buffer the register values.%The buffer can store register values that are read through multiple cycles, thereby reducing the register read bandwidth requirement. In their design, four SIMT lanes form a cluster. Each entry in the streaming multiprocessor’s main register file is 128 bits wide, with 32 bits allocated to the same-named register for threads in each of the four SIMT lanes in the cluster. Several documents indicate that they employ operand collector buffers. These operand collectors are also used to aggregate result values. The operand collector works as a result queue, which buffers the output from functional units before being written back into the register file. Although the main benefit of the result queues is to increase the effective write throughputs, it also provides further optimization opportunities. When the outcomes of instructions are used only in the instructions dynamically scheduled close enough, the output values can be forwarded to the input of the next operation. This behaves just like a CPU’s forwarding network.
%%When an instruction requires multiple accesses to the same bank, the register values are read over multiple cycles. Since the register addresses are determined statically, the compiler can in principle reduce or remove bank conflicts.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{operand.jpg}
%        \caption{Detailed diagram of Operand Collector}
%        \label{fig:operand}
%\end{figure}
%
%\noindent
%\textbf{Register File Cache} 
%To reduce the pressure on the register file, Gebhart et al. propose a \textit{register file cache}. Although a register file cache was originally proposed to reduce the register file access time, in GPUs the register file cache is proposed to reduce register reads and writes. Current GPU architectures do not support precise exceptions, so register files do not have to maintain architectural states. Therefore, many write operations can be easily merged inside the register file cache, which may reduce register writes significantly.
%
%\noindent
%\textbf{Execution stage}%In the execution stage, an instruction accesses either the memory unit or functional units. %The execution stage consists of \textit{vector processing units} (AMD GPUs have a scalar unit as well). One vector lane executes one thread, which is called a stream processor in NVIDIA's terminology. The simplest design would be to have the number of lanes and the warp size be the same. However, this approach could also require a relatively large amount of static power consumption and a large area. Instead, GPUs execute the same instruction over multiple cycles. For example, the G80 has only 8 vector lanes, and therefore the streaming multiprocessor takes four cycles to execute 32 threads (i.e., one warp). The computed results are temporarily stored and then written back to the register file together.
%
%\noindent%\textbf{Back-to-back operation} 
%The execution width of GPU is wide (32 threads), which makes it much harder to have a data forwarding path. For example, 32 × 32 B (total 1 KB) or 32 × 64 B (total 2 KB) widths are quite large. Hence, the results are either written directly to the register file or temporarily stored in a small buffer and then written back to the register file over multiple cycles. In either case, the results are not available to dependent instructions immediately after execution. This scenario is quite different from many modern CPUs, where data forwarding is used widely. When there is a forwarding path, register read/write cycles are not in the critical path and any back-to-back operations are executed immediately following the execution latency. However, GPUs can avoid this penalty by utilizing \textit{thread-level parallelism} (TLP). The pipeline simply schedules instructions from other warps so it can hide the latency. When there is enough TLP, the execution and register write latency can be hidden.%%Volkov discussed this issue in his GTC'10 talk. When there are four threads, the streaming multiprocessor can simply switch to other threads so the execution latency can be hidden. However, when there is only one thread, these back-to-back operations take much longer. 
%
%\noindent%\textbf{Special function units} 
%To date, NVIDIA architectures have adopted SIMD execution units and AMD architectures have used VLIW architectures. In addition, special function units provide VLIW-like effects in NVIDIA GPU architectures. Graphics applications often require transcen- dental functions for algorithms such as geometric rotations and scaling. It is often possible to use implementations of the corresponding transcendental functions that do not have ultra-high accuracy. Many GPU architectures therefore provide fast, approximate implementations in hardware-based special function units (SFUs). Since many programs do not need these SFUs all the time, the pro- cessor has only two-four SFUs in addition to its regular FP units. The scheduler can issue SFU instructions and regular FP instructions together if they are independent, e.g., when the instructions come from different warps. In such cases, these SFU units provide additional execution bandwidth. 
%
%\subsection{Programming a GPU}%Programs that run on a graphics card are written in the so-called \textit{kernel programming model}. 
%Programs in this model consist of \textit{host code} and \textit{kernels}. The host code manages the graphics card, initializing data transfer and scheduling program execution on the device. A kernel is a simplistic program that forms the basic unit of parallelism in the kernel programming model. Kernels are scheduled concurrently on several scalar processors in a SIMD fashion: Each kernel invocation - henceforth called \textit{thread} - executes the same code on its own share of the input. All threads that run on the same multiprocessor are logically grouped into a \textit{workgroup}.
%%One of the most important performance factors in GPU programming is to avoid data transfers between host and device: All data has to pass across the PCIexpress bus, which is the bottleneck of the architecture. Data transfer to the device might therefore consume all time savings from running a problem on the GPU. This becomes especially evident for I/O-bound algorithms: Since accessing the main memory is roughly two to three times faster than sending data across the PCIexpress bus, the CPU will usually have finished execution before the data has even arrived on the device.
%%Graphics cards achieve high performance through massive parallelism. This means, that a problem should be easy to parallelize to gain most from running on the GPU. Another performance pitfall in GPU programming is caused by divergent code paths. Since each multiprocessor only has a single instruction decoder, all scalar processors execute the same instruction at a time. If some threads in a workgroup diverge, for example due to data-dependent conditionals, the multiprocessor has to serialize the code paths, leading to performance losses. While this problem has been somewhat alleviated in the latest generation of graphics cards, it is still recommended to avoid complex control structures in kernels where possible.
%%Currently, two major frameworks are used for programming GPUs to accelerate database systems, namely the \textit{Compute Unified Device Architecture} (CUDA) and the \textit{Open Compute Language} (OpenCL). Both frameworks implement the kernel programming model and provide API’s that allow the host CPU to man- age computations on the GPU and data transfers between CPU and GPU. In contrast to CUDA, which supports NVIDIA GPUs only, OpenCL can run on a wide variety of devices from multiple vendors.
%However, CUDA offers advanced features such as allocation of device memory inside a running kernel or \textit{Uniform Virtual Addressing} (UVA), a technique where CPUs and GPUs share the same virtual address space and the CUDA driver transfers data between CPU and GPU transparently to the application.
%
%\section{Performance Principles}
%Developing algorithms for GPGPUs is fundamentally about applying the same long-studied principles of parallelization and I/O-efficient design relevant to other shared memory parallel platforms. 
%In this section, we reviews these principles, focusing on recent results in both the theory and practice of parallel algorithms, and suggests a connection to GPGPU platforms. Ideally, applying these principles with the right cost models leads not only to provably efficient algorithms, but also offers hints to architects about the features and configurations likely to have the most impact on the performance of a given computation. Thus, we believe this discussion will be useful to practitioners in various aspects of parallel computing, not just those interested specifically in GPGPUs.
%
%\subsection{Algorithm Design Models Overview}
%The two most important characteristics of any algorithm likely to determine its performance are: (a) how much parallelism is available and (b) how much data must move through the memory hierarchy. Thus, when designing an algorithm, we would like an abstract machine model that allows us to assess our algorithm along these dimensions. In such a model, we might want to do the same kind of “big-O” analysis to which we are accustomed in the sequential case. Doing so would allow us to get the high-level algorithm design right before moving on to lower-level performance optimization and tuning. Importantly, the abstractions should not be so cumbersome that we cannot in a reasonable amount of time design and analyze candidate algorithms.
%%Although the state of algorithm design models is in flux, we have reasonable options. For GPGPUs and other manycore-style processors, two suitable models are the so-called \textit{work-depth} (or \textit{work-span}) model for analyzing parallelism, and the external memory model for analyzing I/O behavior in the presence of a memory hierarchy. These two models evolved separately, but recent work has shown ways in which to connect them. 
%
%\subsection{Characterizing Parallelism}%In the \textit{work-depth model}, we represent a computation by a \textit{directed acyclic graph} (DAG) of operations, where edges indicate dependencies, as illustrated in Figure \ref{fig:worker-depth}. 
%Given the DAG, we measure its \textit{work}, $W(n)$, which is the total number of unit-cost operations for an input of size $n$, and its \textit{depth} or \textit{span}, $D(n)$, which is its critical path length measured again in unit-cost operations. 
%Note that $D(n)$ ought to be a lower bound on the minimum execution time, and the ratio $W(n)/D(n)$ effectively measures the average amount of available parallelism as each critical path node executes. 
%In fact, the ratio $D(n)/W(n)$ is similar to the concept of a sequential fraction, as one might use in evaluating Amdahl’s Law. 
%Thus, our implicit goal is to maximize $W (n)/D(n)$, or, alter- natively, minimize $D(n)/W(n)$. 
%Importantly, this model makes no explicit reference to the number of processors. 
%In this sense, we may regard the model as being machine-independent. 
%Nevertheless, if we want to know how many processors to throw at an algorithm, $W (n)/D(n)$ is a suitable guide.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.38\textwidth]{work-depth-model.pdf}
%        \caption{A parallel computation in the work-depth (or work-span) model. The computation for an input of size $n$ is a directed acyclic graph with $W(n)$ nodes, each representing a unit-cost operation; edges representing strict dependencies among these operations; and a critical path of length $D(n)$ nodes. Our goal is to design algorithms that achieve \textit{work-optimality} while maximizing the average available parallelism, $W (n)/D(n)$.}
%        \label{fig:worker-depth}
%\end{figure}
%
%However, it is easy to maximize $W(n)/D(n)$ and still get a bad algorithm. 
%For instance, we can artificially inflate the total operations $W (n)$. 
%Thus, we should also try to ensure our algorithm is \textit{work-optimal}, a property which says that $W (n)$ is not asymptotically worse than the best sequential algorithm. Indeed, work-optimality is a critical requirement.%%Let’s see why work-optimality matters. 
%First, note that it is possible to estimate (crudely) the algorithm’s running time given $p$ identical processors, using a theorem by Brent. This theorem says that if the nodes of the DAG have unit cost and the machine has p processors, then it is possible to schedule the DAG so that the time to execute (compute) the DAG, $T_{comp}(n; p)$, is
%\begin{equation}
%T_{comp}(n; p) = D(n) + \frac{W(n) - D(n)}{p}
%\label{eqn:cost}
%\end{equation}
%Suppose we have designed a highly parallel algorithm, with $W (n) \gg p \cdot D(n)$, and furthermore that our algorithm only exceeds the work $W_0(n)$ of the best sequential algorithm by a factor of $\varepsilon(n)$, i.e., $W(n) = W0(n) \cdot \varepsilon(n)$. 
%Then, the speedup of our algorithm on p processors is roughly $W0(n)/(W(n)/p)=p/\varepsilon(n)$.
%That is, the best possible speedup of p will be reduced by $\varepsilon(n)$.
%Consider the relatively small factor of $\varepsilon(n) = \log n$. Even for $n = 1024$, $\log n = 10$, meaning the best possible speedup is an order of magnitude less than we might hope for. 
%Put another way, to even match the sequential algorithm, we need $p >\varepsilon(n)$. 
%Thus, if we are deciding between an $O(n)$ sequential algorithm and an $O(n^2/p)$ parallel algorithm, we will need $p = n$ just to match the sequential case. These examples underscore the importance of work-optimal algorithms.
%
%\noindent
%\textit{\textbf{Example: Reduction.}} 
%Suppose we wish to compute the sum of n values. A sequential algorithm would lead to a DAG like the one shown in Figure \ref{fig:worker-depth-example}. 
%In this case, we perform $\Theta(n)$ operations but the critical path is also of length $\Theta(n)$. 
%Thus, the available parallelism—-or ratio of $W (n)/D(n)$—-is a constant: no matter how large the input, there is never more than a fixed amount of concurrency.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.35\textwidth]{work-depth-example.pdf}
%        \caption{Work-depth example: summing a list of $n$ elements with a sequential algorithm. Both the work and the depth are $\Theta(n)$, meaning the available parallelism $W (n)/D(n)$ is a constant.}
%        \label{fig:worker-depth-example}
%\end{figure}
%
%An algorithm with more parallelism might instead have the DAG shown in Figure \ref{fig:worker-depth-16}. 
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.49\textwidth]{work-depth-16.pdf}
%        \caption{Work-depth example: summing a list of $n = 16$ elements with a tree algorithm.}
%        \label{fig:worker-depth-16}
%\end{figure}
%This algorithm organizes the additions into a tree, where independent subtrees can be performed in parallel. 
%This tree still performs $W (n) =\Theta(n)$ total operations, and is thus work-optimal. 
%However, the depth of this DAG is just $D(n) =\Theta(\log n)$, the height of the tree. 
%Thus, as $n$ grows, so does the average available parallelism, in the amount of $W (n)/D(n) =\Theta(n/ \log n)$. 
%By this measure, this algorithm is a better one than the sequential algorithm, just as we would expect.%In the specific case of Figure \ref{fig:worker-depth-16}, where $n = 16$, the work $W (16) = 31$ nodes and the depth $D(16) = 5$. (Imagine that the “input” nodes at the top of Figure \ref{fig:worker-depth-16} represent load operations to retrieve the values.) Then, $W (16)/D(16) = 31/5 = 6.2$. Thus, there are an average of  $\approx 6$ nodes available to be executed in parallel during the computation. This implies that we could not gainfully use more than about six “processors.”
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.49\textwidth]{matrix-work-depth.pdf}
%        \caption{Work-depth example: multiplying two n × n matrices.}
%        \label{fig:matrix}
%\end{figure}
%
%\noindent
%\textbf{\textit{Example: Matrix multiply.}} 
%Consider the multiplication of two $n × n$ matrices, $C \leftarrow A × B$, as illustrated in Figure \ref{fig:matrix}. 
%Each output element $c_{ij}$ is the dot product between $A_{i,:}$, which is row $i$ of $A$, and $B_{:,j}$ , which is column $j$ of $B$. $A$ dot product involves an elementwise multiplication of the vectors, followed by a sum-reduction of those results. This dot product requires $\Theta (n)$ operations.
%Since there are $n^2$ elements of $C$, the work $W(n) = \Theta(n^3)$\footnote{We assume a conventional matrix multiply, rather than an asymptotically faster algorithm, such as Strassen’s algorithm}.
%To compute $D(n)$, observe that the $n^2$ output elements have no dependencies among them; the only dependencies occur during the reduction to compute each output element. Thus, $D(n) = \Theta(\log n)$. 
%The ratio of $W(n)/D(n) = \Theta(n^3/\log n)$ is asymptotically very high, and so our analysis confirms what we would expect, namely, that a matrix multiply has plenty of parallelism for even modestly sized values of $n$.
%\subsection{Charterizing I/O Behavior}
%Besides parallelism, data movement is the other critical characteristic of an algorithm. To analyze data movement, we consider the classical \textit{external memory model}. 
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.3\textwidth]{abstract-machine.pdf}
%        \caption{An abstract machine with a \textit{two-level memory hierarchy}. The fast memory (e.g., cache or local-store) can hold $Z$ words. Our goal is to design algorithms that, given an input of size n, are work-optimal but also minimize the number of transfers $Q(n; Z)$ between the slow and fast memories (alternatively, that are work-optimal and maximize the computational intensity, $W (n)/Q(n; Z)$).}
%        \label{fig:abstract-machine}
%\end{figure}
%
%Consider first a \textit{sequential} processor with a two-level memory hierarchy consisting of a large but slow memory and a small fast memory of size $Z$ words; work operations may only be performed on data that lives in fast memory. This fast memory may be an automatic cache or a software- controlled scratchpad; our analysis here is agnostic to this choice. We may further consider that transfers between slow and fast memory must occur in discrete transactions (blocks) of size $L$ words. When we design an algorithm in this model, we again measure the work, $W (n)$; however, we also measure Q(n; Z, L), the number of L-sized transfers between slow and fast memory for an input of size n. There are several ways to design either cache-aware or cache-oblivious algorithms and then analyze $Q(n; Z, L)$. 
%In either case, when we design an algorithm in this model, we again aim for work-optimality while also trying to maximize the algorithm’s \textit{computational intensity}, which is $W (n)/ (Q(n; Z, L) \cdot L)$. Intensity is the algorithmic ratio of operations performed to words transferred and, in practice, is often converted and cited as the algorithm’s “flop-to-byte” ratio.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.49\textwidth]{block-matrix.pdf}
%        \caption{A blocked matrix multiply performs $O(n^3/(L\sqrt Z))$ I/Os in the external memory model, assuming that: 
%(i) each of the three blocks of $A, B,$ and $C$ needed to compute an output block of $C$ are stored contiguously; and (ii) that these three blocks just fit into the fast memory.}
%        \label{fig:block-matrix}
%\end{figure}
%
%\noindent
%\textbf{\textit{Example: Matrix multiply.}} 
%Algorithms for conventional matrix multiply perform $W(n) = \Theta(n^3)$ operations on $ \Theta(n^2)$ words of data;
%thus, a naive lower bound on $Q(n; Z, L)$ would be $\Sigma(n^2/L)$ transfers. The intensity could therefore be a high as $O(n)$.
%
%Consider two candidate algorithms for matrix multiply. The first is the one shown in Figure \ref{fig:matrix}, which is based on performing $n^2$ reductions of length $n$ each. This algorithm results in $Q(n; Z, L) = O(n^3/L)$ transfers, assuming the preceding analysis for reductions. The intensity is therefore just $O(1)$. 
%Compared to our naive estimate of $O(n)$, it is likely there is a better algorithm.%%The well-known alternative appears in Figure \ref{fig:block-matrix}. 
%This algorithm updates blocks of $C$ at a time, rather than a single element of $C$, 
%and streams through blocks of $A$ and $B$ as shown. 
%That is, the algorithm performs block updates instead of row/column dot products. In this case, if
%\bi
%\ii the block size is $b \times b$;%\ii three blocks can be fit in fast memory at once, i.e., $3b^2 \leq Z$; and%\ii blocks are stored contiguously, so that just $b^2/L$ transfers are required to load a block,
%\ei
%then it can be shown that $Q(n; Z, L) = O(n^3/(L \sqrt{Z}))$.
%Therefore, the intensity is $O(\sqrt{Z})$.
%If $Z=\Omega(L^2)$, the intensity of this algorithm is much higher than that of the naive algorithm. In practice, a local-store or cache size $Z$ is typically much larger than corresponding minimum transfer or line size $L$, so this assumption is likely to hold\footnote{An exception to this rule is a TLB, which for typical configurations is like a cache with very large lines and having a capacity of a small number of lines.}.
%
%One might reasonably ask whether this value of $Q(n; Z, L)$ can be improved upon further, given our naive lower-bound estimate of $O(n^2)$.
%For general matrices and the conventional matrix multiply algorithm, the answer is that no algorithm can move fewer words than the blocked algorithm -- the blocked algorithm is asymptotically optimal.
%So-called \textit{cache-oblivious} approaches also only match this bound.
%
%\noindent
%\textit{\textbf{A few summary observations about I/O.}} 
%The case of I/O analysis is evidently more complex than the analysis of parallelism in the following ways.
%\bi%\ii The I/O analysis is not independent of the machine parameters, the way the work-depth analysis was independent of the number of processors, $p$.%\ii The two example I/O analyses both included assumptions of \textit{contiguous layouts}. 
%That is, inclusion of the parameter, $L$, forces the algorithm designer to take data layout into account.
%\ii Recall that for matrix multiply, we were able to state a \textit{lower bound} on the number of I/O transfers for \textit{any} algorithm. This is one area in which theoretical analysis, when further refined to account for additional architectural details, can provide insights into performance that are difficult to extract from code or benchmarks.
%\ei
%
%In addition, we might ask what the relative asymptotic pay-off from increasing hardware memory resources
% (e.g., $Z$ and $L$) is relative to increasing hardware parallelism (e.g., $p$). 
%For example, for compute-rich matrix multiply, we see that doubling $Z$ yields a $\sqrt{2}$ reduction in I/Os, whereas doubling $L$ cuts I/Os in half. Then, if the cost of doubling $Z$ and doubling $L$ are equal, it would be more cost effective to double $L$. This analysis confirms an intuition behind GPU design of favoring significant increases in memory-level parallelism (larger $L$) over bigger caches (larger $Z$).
%
%\subsection{Abstract and Concrete Measures}
%Work, depth, and the number of I/Os or cache misses are all abstract quantities, whereas the ultimate goal—-if it is possible—-would be to make stronger statements about execution time and time- scalability. Such a time-based analysis minimally requires an architectural cost model. If we are able to do so, what would the theory tell us?
%
%Consider an example of the generic manycore processor system shown in Figure \ref{fig:manycore}. 
%This system has $p$ cores, each of which can deliver a maximum of $C_0$ operations per unit time; a fast memory of total size $Z$ words, partitioned among the cores; and a memory system whose cost to transfer $m \cdot L$ words is $\alpha + m \cdot L/\beta$, where $\alpha$ is the latency and $\beta$ is the bandwidth in units of words per unit time\footnote{This abstract machine is intended to very coarsely approximate an NVIDIA Fermi-class architecture, where $Z$ might represent the aggregate register and multiprocessor-private first-level local-store and cache capacity.}.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.25\textwidth]{manycore-processor.pdf}
%        \caption{An abstract manycore processor with $p$ cores, each with a peak execution rate of $C_0$ operations per unit time; an aggregate fast memory capacity of $Z$ words partitioned among the cores; and a channel between slow and fast memory such with latency α time units, bandwidth $\beta$ words per unit time, and minimum transaction size $L$ words.}
%        \label{fig:manycore}
%\end{figure}
%
%Equation \ref{eqn:cost} gives us a way to estimate the time, $T_{comp}(n; p)$, just to complete the computational work for this system. To estimate the I/O time, $T_{mem}(n)$, suppose we know $Q_p(n; Z, L)$ and charge the full latency $\alpha$ for each enode on the critical path. 
%Let us further assume that all $Q_p(n; Z, L)$ memory transfers will in the best case be aggregated and pipelined by the memory system and thereby de delivered at the peak bandwidth, $\beta$. Then,
%\begin{equation}
%T_{mem}(n; p, Z, L, \alpha, \beta) = \alpha \cdot D(n) + \frac{Q_p(n; Z, L)(n) \cdot L}{\beta}
%\end{equation}
%This cost model is not definitive, but sufficient to illustrate a few points.
% \section{Exploring the Design Space}
%\subsection{Design Space of a GPU-Aware DBMS Architecture}
%In this section, we explore the design space of a GPU-accelerated database management system (GDBMS). 
%GDBMSs should be \textit{in-memory column stores}, should use the \textit{block- at-a-time processing model }and exploit all available processing devices for query processing by using a \textit{GPU-aware query optimizer}.
% Thus, main memory DBMSs are similar to GPU-accelerated DBMSs, and most in-memory, column-oriented DBMSs can be extended to efficiently support co-processing on GPUs.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{design-space.pdf}
%        \caption{Design space of GPU-aware DBMSs}
%        \label{fig:design-space}
%\end{figure}
%Figure \ref{fig:design-space} shows the design space of a GPU-aware DBMSs.
%A GPU-aware database system should reside \textit{in-memory} and use \textit{columnar storage}. 
%As processing model, it should implement \textit{operator-at-a-time} bulk processing model \cite{Manegold:2009}, potentially enhanced by dynamic code compilation. 
%The system should make use of all available (co-)processors in the system (including the CPU!) by having a locality-aware query optimizer, which distributes the workload across all available processing resources. In case the GPU-aware DBMS needs transaction sup- port, it should use an optimistic transaction protocol, such as the timestamp protocol. Finally, in order to reduce implementation overhead, the ideal GDBMS would be hardware-oblivious, meaning all hardware-specific adaption is handled transparently by the system itself.
%
%\section{A Survey of GDBMSs}
%In this section, we refine our theoretical discussion of the GDBMS design space.
%We identified the following \textit{eight} academic systems that are relevant for our survey; CoGaDB, GPUDB, GPUQP, GPUTx, MapD, Ocelot, OmniDB, and Virginian \cite{gpgpu:2014}. We present for each GDBMS the storage system, the \textit{storage} and \textit{processing} model, \textit{query placement} and \textit{query optimization}, and support for \textit{transaction} processing.
%
%\subsection{CoGaDB}%CoGaDB\footnote{ Source code available at: \href{http://wwwiti.cs.uni-magdeburg.de/iti db/research/gpu/cogadb/}{http://wwwiti.cs.uni-magdeburg.de/iti db/research/gpu/cogadb/}.} focuses on GPU-aware query optimization to achieve efficient co-processor utilization during query processing \cite{BreB:2013, seb:2013}. Figure \ref{fig:cogadb} shows the architecture of CoGaDB.
%
%\noindent
%\textit{Storage System:} 
%CoGaDB persists data on disk, but loads the complete database into main memory on startup. If the database is larger than the main memory, CoGaDB relies on the operating system’s virtual memory management to swap the least recently used memory pages on disk.%%\noindent
%\textit{Storage Model:}
% CoGaDB stores data in data structures optimized for in-memory databases. Hence, it stores the data column-wise and compresses VARCHAR columns using dictionary encoding. Furthermore, the data has the same for- mat when stored in the CPU’s or the GPU’s memory.%%\noindent
%\textit{Processing Model:} CoGaDB uses the operator-at-a-time bulk processing model to make efficient use of the memory hierarchy. This is the basis for efficient query processing using all processing resources.%%\noindent
%\textit{Query Placement \& Optimization:} CoGaDB uses the \textit{Hybrid Query Processing Engine} (HyPE) as physical optimizer \cite{BreB:2013, BreB:2014}. HyPE optimizes physical query plans to increase inter-device parallelism by keeping track of the load condition on all (co-)processors (e.g., the CPU or the GPU).
%%\noindent
%\textit{Transactions:} Not supported.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{cogadb.pdf}
%        \caption{The architecture of CoGaDB}
%        \label{fig:cogadb}
%\end{figure}
%
%\subsection{GPUDB}%In order to study the performance behaviour of OLAP queries on GPUs, Yuan and others developed GPUDB\footnote{Source code available at: \href{https://code.google.com/p/gpudb/}{https://code.google.com/p/gpudb/}.} \cite{Yuan:2013} (Fig. \ref{fig:gpudb}).%%\noindent
%\textit{Storage System:} 
%GPUDB keeps the database in the CPU’s main memory to avoid the hard-disk bottleneck. Yuan and others identified a crucial optimization for main-memory DBMS with respect to GPU accelerated execution: In case data is stored in pinned host memory, query execution times can significantly improve (i.e., Yuan and others observed speedups up to 6.5x for certain queries of the \textit{Star Schema Benchmark} (SSB) \cite{Rabl:2013}).
%%\noindent
%\textit{Storage Model:}
% GPUDB stores the data column-wise because GPUDB is optimized for warehousing workloads. Additionally, GPUDB supports common compression techniques (run length encoding, bit encoding, and dictionary encoding) to decrease the impact of the PCIe bottleneck and to accelerate data processing.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{gpudb.pdf}
%        \caption{GPUDB: Query engine architecture}
%        \label{fig:gpudb}
%\end{figure}
%%\noindent
%\textit{Processing Model:}
%GPUDB uses a \textit{block-oriented processing} model: Blocks are kept in GPU RAM until they are completely processed. This processing model is also known as \textit{vectorized processing}. Thus, the PCIe bottleneck can be further reduced by overlapping data transfers with computation. For certain queries, Yuan and others observed speedups up to 2.5x compared to no overlapping of processing and data transfers.
%%GPUDB compiles queries to \textit{driver programs}. A driver program executes a query by calling pre-implemented GPU operators. Hence, GPUDB executes all queries on the GPU and the CPU performs only dispatcher and post processing tasks (i.e., the CPU is used less than 10\% of the time during processing SSB queries \cite{Yuan:2013}).
%
%\noindent
%\textit{Query Placement \& Optimization:} GPUDB has no support for executing queries on the CPU and GPU in parallel.%%\noindent
%\textit{Transactions:} Not supported.%%%\subsection{GPUQP}%He and others developed GPUQP\footnote{Source code available at: \href{http://www.cse.ust.hk/gpuqp/}{http://www.cse.ust.hk/gpuqp/}.}, a relational query processing system, which stores data in-memory and uses the GPU to accelerate query processing \cite{Fang:2007}. In GPUQP, each relational operator can be executed on the CPU or the GPU (Fig. 6).
%%\noindent
%\textit{Storage System:} 
%GPUQP supports in-memory and disk-based processing. 
%Apparently, GPUQP also attempts to keep data cached in GPU memory. 
%Unfortunately, the authors do not provide any details about the used data placement strategy.%%\noindent
%\textit{Storage Model:} 
%Furthermore, GPUQP makes use of columnar storage and query processing, which fits the hardware capabilities of modern CPUs and GPUs.%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.2\textwidth]{gpuqp.pdf}
%        \caption{Execution engine of GPUQP}
%        \label{fig:gpudb}
%\end{figure}%%\noindent
%\textit{Processing Model:} 
%GPUQP's basic processing strategy is operator-at-a-time bulk processing. 
%However, GPUQP is also capable of partitioning data for one operator and execute the operator on the CPU and the GPU concurrently. 
%Nevertheless, the impact on the overall performance is small \cite{He:2009}.%%\noindent
%\textit{Query Placement \& Optimization:} 
%GPUQP combines a Selinger-style optimizer \cite{Selinger:1988} with an analytical cost model to select the cheapest query plan. For each operator, GPUQP allocates either the CPU, the GPU, or both processors (partitioned execution). The query optimizer splits a query plan to multiple sub-plans containing at most ten operators. For each sub-query, all possible plans are created and the cheapest sub-plan is selected. Finally, GPUQP combines the sub-plans to a final physical query plan.
%
%He and others focus on optimizing single queries and do not discuss multi- query optimization. Furthermore, load-aware query scheduling is not considered and there is no discussion of scenarios with multiple GPUs.%%\noindent
%\textit{Transactions:} Not supported.
%%\subsection{GPUTx}%In order to investigate relational transaction processing on GPUs, He and others developed GPUTx, a transaction processing engine that runs on the GPU \cite{He:2011}.
%%\noindent
%\textit{Storage System \& Model:}
%GPUTx keeps all OLTP data inside the GPU’s memory to minimize the impact of the PCIe bottleneck. 
%It also applies a columnar data layout to fit the characteristics of modern GPUs.%%\noindent
%\textit{Processing Model:} The processing model is not built on relational operators as in GPUQP. Instead, GPUTx executes pre-compiled stored procedures, which are grouped into one GPU kernel. Incoming transactions are grouped in bulks, which are sets of transactions that are executed in parallel on the GPU.%%\noindent
%\textit{Query Placement \& Optimization:}
%Since GPUTx performs the complete data processing on the GPU, query placement approaches are not needed.%%\noindent
%\textit{Transactions:}
%GPUTx is the only system in our survey – and that we are aware of – that supports running transactions on a GPU. It implements three basic transaction protocols: \textit{Two-phase locking}, \textit{partition-based execution} and $k$\textit{-set- based execution}. The major finding of GPUTx is that \textit{locking-based} protocols do not work well on GPUs. 
%Instead, \textit{lock-free} protocols such as partition-based execution or $k$-set should be used.
%%\subsection{MapD}%Mostak develops MapD (\textit{Massively Parallel Database}), which is a data processing and visualization engine, combining traditional query processing capabilities of DBMSs with advanced analytic and visualization functionality \cite{mapd:2015}. 
%One application scenario is the visualization of twitter messages on a road map\footnote{\href{http://mapd.csail.mit.edu/tweetmap/}{http://mapd.csail.mit.edu/tweetmap/}.}, in which the geographical position of tweets is shown and visualized as heat map.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.49\textwidth]{mapd-visualization.pdf}
%        \caption{A MapD-rendered visualization of 500 million tweets in Europe colored by language used.}
%        \label{fig:mapd-visualization}
%\end{figure}
%%\noindent
%\textit{Storage System:} The data processing component of MapD is a relational DBMS, which can handle data volumes that do not fit the main memory. MapD also tries to keep as much data in-memory as possible to avoid disk accesses.
%
%\noindent
%\textit{Storage Model:} 
%MapD stores data in a columnar layout, and further partitions columns into \textit{chunks}. 
%A chunk is the basic unit of MapD's memory manager. 
%The basic processing model of MapD is processing one \textit{operator-at-a-time}. 
%Due to the partitioning of data into chunks, it is also possible to process on a per- chunk basis. Hence, MapD is capable of applying \textit{block-oriented} processing.%%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.38\textwidth]{mapd-diagram.pdf}
%        \caption{MapD vision}
%        \label{fig:mapd-diagram}
%\end{figure}
%%\noindent
%\textit{Processing Model:} 
%MapD processes queries by compiling a query to executable code for the CPU and GPU.%%\noindent
%\textit{Query Placement \& Optimization:} 
%The optimizer tries to split a query plan in parts, and processes each part on the most suitable processing device (e.g., text search using an index on the CPU and table scans on the GPU). MapD does not assume that an input data set fits in GPU RAM, and it applies a streaming mechanism for data processing.%%\noindent
%\textit{Transactions:} Not supported.
%
%\noindent
%\textbf{Performance:} While GPU performance and memory capacity are still rapidly increasing, in the current generation, up to eight GPU cards with 192GB total GPU memory can be installed in a single server, allowing data to be queried at rates approaching three terabytes per second by almost 40,000 cores. Furthermore, using the sophisticated graphics pipeline on each GPU, the data can be visualized in situ and sent to the client without the need for costly transfers to other platforms.
%%Although MapD was designed to deliver weightless data exploration straight to the browser, it is equally capable of processing programmatic SQL queries with blistering speed (at a rate over a trillion rows per second per server for certain workloads).
%
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{mapd-graph.pdf}
%        \caption{Map performance compared to leading in-memory database on 2-socket, 8-GPU system.}
%        \label{fig:mapd-graph}
%\end{figure}
%
%\noindent
%\textbf{The MapD platform uniquely:}%Caches the hot data in GPU memory such that no time is lost moving the data across the PCIe bus onto the GPU when a query is executed.
%
%\bi%\ii \textit{Vectorizes execution} of queries whenever possible. Vectorized code allows the compute resources of a processor to process multiple data items simultaneously. This is a must to achieve good performance on GPUs, which can comprise thousands of execution units. Additionally, optimizing vectorized execution also translates well to CPUs, which increasingly have “\textit{wide}” execution units capable of processing multiple data items at once.%\ii Employs \textit{highly-optimized kernels} for common database operations. Although GPUs are extremely fast, GPU code often requires more optimization work than comparable CPU code to achieve the best performance. MapD is%the product of thousands of hours of benchmarking and testing to extract the maximum vectorized performance from both GPU and CPU.%\ii Compiles queries \textit{on the fly} for maximum execution speed.%\ii Executes queries \textit{simultaneously} on both CPU and GPU, or entirely on CPU if the working dataset cannot fit in GPU memory.%\ii Visualizes and performs complicated analytics on data in situ. Since the relevant data is already cached on the GPUs, MapD does not need to copy the query result set before rendering it (using the GPUs) or using it as input to a follow-on machine learning algorithm.
%\ei
%%\subsection{Ocelot}%Heimel and others develop Ocelot\footnote{Source code available at: \href{http://goo.gl/GHeUv}{http://goo.gl/GHeUv}.}, which is an OpenCL extension of MonetDB, enabling operator execution on any OpenCL capable device, including CPUs and GPUs \cite{Heimel:2013}(Fig. \ref{fig:ocelot}).%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{ocelot.pdf}
%        \caption{The architecture of Ocelot}
%        \label{fig:ocelot}
%\end{figure}
%%\noindent
%\textit{Storage System:} 
%Ocelot’s storage system is built on top of the \textit{in-memory} model of MonetDB. Input data is automatically transferred from MonetDB to the GPU when needed by an operator. In order to avoid expensive transfers, operator results are typically kept on the GPU. They are only returned at the end of a query, or if the device memory is too filled to fulfill requests. Additionally, Ocelot implements a \textit{device cache} to keep relevant input data available on the GPU.
%
%\noindent
%\textit{Storage Model:} 
%Ocelot/MonetDB stores data column-wise in \textit{Binary Association Tables} (BATs). Each BAT consists of two columns: One (optional) head storing object identifiers, and one (mandatory) tail storing the actual values.%%\noindent
%\textit{Processing Model:} Ocelot inherits the \textit{operator-at-a-time} bulk processing model of MonetDB, but extends it by introducing \textit{lazy evaluation} and making heavy use of the OpenCL event model to forward operator dependency information to the GPU. This allows the OpenCL driver to automatically interleave and reorder operations, e.g., to hide transfer latencies by overlapping the transfer with the execution of a previous operator.%%\noindent
%\textit{Query Placement \& Optimization:} 
%In MonetDB, each query plan is represented in the \textit{MonetDB Assembly Language} (MAL). Ocelot reuses this infrastructures and adds a new query optimizer, which rewrites MAL plans by replacing data processing MAL instructions of vanilla MonetDB with the highly parallel OpenCL MAL instructions of Ocelot.%Ocelot does not support cross-device process- ing, meaning it executes the complete workload either on the CPU or on the GPU.%%\noindent
%\textit{Transactions:} Not supported.
%%\subsection{OmniDB}%Zhang and others developed OmniDB\footnote{Source code available at: \href{https://code.google.com/p/omnidb-paralleldbonapu/}{https://code.google.com/p/omnidb-paralleldbonapu/}.}, a GDBMS aiming for good code maintainability while exploiting all hardware resources for query processing \cite{Zhang:2013}. 
%The basic idea is to create a hardware oblivious database kernel (qkernel), which accesses the hardware via \textit{adaptors}. Each adapter implements a common set of operators decoupling the hardware from the database kernel (Fig. 8).
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{omnidb.pdf}
%        \caption{OmniDB: Kernel adapter design}
%        \label{fig:omnidb}
%\end{figure}
%
%\noindent
%\textit{Storage System \& Model:} 
%OmniDB is based on GPUQP, and hence, has similar architectural properties to GPUQP. 
%OmniDB keeps data in-memory in a column- oriented data layout.%%\noindent
%\textit{Processing Model:} 
%OmniDB schedules and processes work units, which can vary in granularity (e.g., a work unit can be a query, an operator, or a chunk of tuples). Although it is not explicitly mentioned in the paper, the fact that OmniDB can process also chunks of tuples is a strong indicator that it supports block-oriented processing.%%\noindent
%\textit{Query Placement \& Optimization:} 
%Regarding query placement and optimization, OmniDB chooses the processing device with highest throughput for a work unit. To avoid overloading a single device, OmniDB’s scheduler ensures that the workload on one processing device may not exceed a certain percentage of the average workload on all processing devices. The cost model relies on the adapters to provide cost functions for the underlying processing devices.%%\noindent
%\textit{Transactions:} Not supported.
%%\subsection{Virginian}%Bakkum and others develop Virginian\footnote{Source code available at: \href{https://github.com/bakks/virginian}{https://github.com/bakks/virginian}.}, which is a GPU-accelerated DBMS keeping data in main memory and supporting filter and aggregation operations on all processing devices \cite{Bakkum:2012}.%%\noindent
%\textit{Storage System:} 
%Virginian uses no traditional caching of operators, but \textit{uniform virtual addressing} (UVA). This technique allows a GPU kernel to directly access data stored in pinned host memory. The accessed data is transferred over the bus transparently to the device and efficiently overlaps computation and data transfers.%%\noindent
%\textit{Storage Model:} 
%Virgnian implements a data structure called \textit{tablet}, which stores fixed size values column oriented. Additionally, tables can handle variable sized data types such as strings, which are stored in a dedicated section inside the tablet. Thus, Virginian supports strings on the GPU. This is a major difference to other GDBMSs, which apply dictionary compression on strings first and work only on compressed values in the GPU RAM.%%\noindent
%\textit{Processing Model:} Virginian uses operator-at-a-time processing as basic query- processing model. It implements an alternative processing scheme. While most systems call a sequence of highly parallel primitives requiring one new kernel invocation per primitive, Virginian uses the opcode model, which combines all primitives in a single kernel. This avoids writing data back to global memory and reading it again in the next kernel ultimately resulting in block-wise processing on the GPU.
%
%\noindent
%\textit{Query Placement \& Optimization:} Virginian can either process queries on the CPU or on the GPU. Thus, there is no mechanism splitting up the workload between CPU and GPU processing devices and hence, no hybrid query optimizer is available.%%\noindent
%\textit{Transactions:} Not supported.
%
%\section{Potential Optimizations}%We will now discuss and summarize potential optimizations, which a GDBMS may implement to make full use of the underlying hardware in a hybrid CPU/GPU system. 
%Additionally, we briefly discuss existing approaches for each optimization. 
%As already discussed, data transfers have the highest impact on GDBMS performance. 
%Hence, every optimization avoiding or minimizing the impact of data transfers are mandatory. 
%We refer to these optimizations as cross-device optimizations. 
%Based on our surveyed systems, we could identify the following \textit{cross-device optimizations} :
%%\noindent
%\textbf{Efficient Data Placement Strategy:} 
%There are two possibilities to manage the GPU RAM. The first possibility is an explicit management of data on GPUs using a buffer-management algorithm. The second possibility is using mechanisms such as Unified Virtual Addressing (UVA), which enables a GPU kernel to directly access the main memory. 
%Kaldewey and others observed a significant performance gain (3-8x) using UVA for Hash Joins on the GPU compared to the CPU \cite{Kaldewey:2012}. Furthermore, data has not to be kept consistent between CPU and GPU, because there is no “real” copy in the GPU RAM. However, this advantage can also be a disadvantage, because caching data in the GPU RAM can avoid the data transfer from the CPU to the GPU.
%%\noindent
%\textbf{GPU-aware Query Optimizer:} 
%A GDBMS should make use of all processing devices to maximize performance. Therefore, it should offload operations to the GPU. However, offloading single operations of a query plan does not necessarily accelerate performance. 
%Hence, a GPU-aware optimizer has to identify sub plans of a query plan, which it can process on the CPU or the GPU \cite{He:2009}. Furthermore, the resulting plan should minimize the number of copy operations. 
%Since optimizers are typically cost based, a GDBMS needs for each GPU operator a cost model. The most common approach is to use analytical models.
%However, with the increasing hardware complexity, machine-learning-based models become increasingly popular.
%%\noindent
%\textbf{Data Compression:}
%The data placement and query optimization techniques attempt to avoid data transfers as much as possible. To reduce overhead in case a GDBMS has to perform data transfers, the data volume can be reduced by compression techniques. 
%Thus, compression can significantly decrease processing costs \cite{Yuan:2013}. 
%%\noindent
%\textbf{Overlap of Data Transfer and Processing:} 
%The second way to accelerate processing, in case a data transfer needs to performed, is overlapping the execution of a GPU operator with a data transfer operation \cite{Yuan:2013}. This optimization keeps all hardware components busy, and basically narrows down the performance of the system to the PCIe bus bandwidth.%%\noindent
%\textbf{Pinned Host Memory:} 
%The third way to accelerate query processing in case we have to perform a copy operation is keeping data in pinned host memory. This optimization saves one indirection, because the DMA controller can transmit data directly to the device \cite{Yuan:2013}. Otherwise, data has to be copied in pinned memory first, introducing additional latency in data transmission. However, using pinned host memory has the drawback that the amount of available pinned host memory is much smaller than the amount of unpinned memory (i.e., memory that can be paged to disk by the virtual memory manager). Therefore, a GDBMS has to decide which data it should keep in pinned host memory. It is still an open issue how much memory should be spent on a pinned host memory buffer for faster data transfers to the GPU.%Figure \ref{fig:cross-device-tree} illustrates the identified cross-device optimizations and the relationships between them.
%
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.5\textwidth]{cross-device-tree.pdf}
%        \caption{Cross-device Optimizations}
%        \label{fig:cross-device-tree}
%\end{figure}
%
%The second class of optimizations we identified, targets the efficiency of opera- tor execution on a single processing device. We refer to this class of optimizations as \textit{device-dependent optimizations}. 
%Since we focus on GPU-aware systems, we only discuss optimizations for GPUs. 
%Based on the surveyed systems, we summarize the following GPU-dependent optimizations:
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.45\textwidth]{device-dependent-tree.pdf}
%        \caption{Device-dependent optimizations: Efficient processing models}
%        \label{fig:device-dependent-tree}
%\end{figure}
%Figure \ref{fig:device-dependent-tree} illustrates the identified device-dependent optimizations and the relationships between them.
%
%\noindent
%\textbf{Block-oriented Query Processing:} 
%A GDBMS can avoid the overhead of writing results of an operator back to a processing device’s main memory by processing data on a \textit{per block basis} rather than on a per operator basis. 
%The idea is to process data already stored in the cache (CPU) or shared memory (GPU), which saves memory bandwidth and significantly increases performance of query processing \cite{Yuan:2013}. 
%Additionally, \textit{block-oriented processing} is a necessary prerequisite for overlapping processing and data transfer for single operations and allows for a more fine grained workload distribution on available processing devices \cite{Zhang:2013}. 
%Note that traditional pipelining of blocks between GPU operators is not possible, because inter-kernel communication is undefined. 
%While launching a new kernel for each block is likely to be expensive, \textit{query compilation} and \textit{kernel fusion} are promising ways to allow block-oriented processing on the GPU as well.
%%\noindent
%\textbf{Compilation-based Query Processing:} 
%Compiling queries to executable code is a common optimization in main-memory DBMSs. 
%As already discussed, query compilation allows for block-oriented processing on GPUs as well and achieves a significant speedup compared to primitive-based query processing (e.g., operator-at-a-time processing \cite{He:2009}). 
%However, query compilation introduces additional overhead, because compiling a query to executable code typically is more expensive than building a physical query execution plan. Yuan and others overcome this shortcoming by pre-compiling operators. Thus, they only need to compile the query plan itself to a driver program \cite{Yuan:2013}. 
%A similar approach called \textit{kernel weaver} is used by Wu and others \cite{Wu:2012}. 
%They combine CUDA kernels for relational primitives into one kernel. This has the advantage that the optimization scope is larger and the compiler can perform more optimizations. However, the disadvantage is the increased compilation time. Rauhe and others introduce in their approach two processing phases: \textit{compute} and \textit{accumulate}. In the compute phase, a number of threads are assigned to a partition of the input data and each thread performs all operations of a query on one tuple and then, continues with the next tuple, until the thread processed its partition. 
%In the accumulate phase, the intermediate results are combined to the final result \cite{Rauhe:2013}.%%\noindent
%\textbf{All-in-one Kernel:} 
%A promising alternative to compilation-based approaches is to combine all relational primitives in one kernel \cite{Bakkum:2012}. Thus, a relational query has to be translated to a sequence of op codes. An op code identifies the next primitive to be executed. Therefore, it is basically an on-GPU virtual machine, which saves the initial overhead of query compilation. However, the drawback is a limited optimization scope compared to \textit{kernel weaver}  \cite{Wu:2012}.%%\noindent
%\textbf{Portability:} 
%Until now, we mainly discussed performance optimizations. How- ever, each of the discussed optimizations are mainly implemented device dependent. This increases the overall complexity of a GDBMS. The problem gets even more complex with new processing device types such as accelerated processing units or the Intel Xeon Phi. Heimel and others implemented a hardware oblivious DBMS kernel in OpenCL and still achieved a significant acceleration of query processing \cite{Heimel:2013}. 
%Zhang and others implemented \textit{q-kernel}, a hardware-oblivious database kernel using device adapters to the underlying processing devices \cite{Zhang:2013}. It is still not clear which part of a kernel should be hardware oblivious and which part should be hardware aware. 
%For the parts that have to be hardware aware, modern software engineering methods such as software product lines can be used to manage the GDBMS’s complexity \cite{Broneske:2014}.
%
%\section{Reference Architecture for GDBMSs}
%Based on our in-depth survey of existing GDBMSs, we now derive a reference architecture for GDBMSs. After careful consideration of all surveyed systems, we decided to use the GPUQP/OmniDB \cite{Zhang:2013} architecture as basis for our reference architecture, because they already include a major part of the common properties of the surveyed systems. We illustrate the reference architecture in Fig. \ref{fig:gdbms}.
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{gdbms.pdf}
%        \caption{Layered architecture of GDBMSs}
%        \label{fig:gdbms}
%\end{figure}
%
%We will describe the query-evaluation process in a top-down view. On the upper levels of the query stack, a GPU-accelerated DMBS is virtually identical to a “traditional” DBMS. It includes functionality for integrity control, parsing SQL queries, and performing logical optimizations on queries. Major differences between main-memory DBMSs and GDBMSs emerge in the physical optimizer. While classical systems choose the most suitable access structure and algorithm to operate on the access structure, a GPU-accelerated DBMS has to additionally decide for each operator on a processing device. For this task, a GDBMS needs refined13 cost models that also predict the cost for GPU and CPU operations. Based on these estimates, a scheduler can allocate the cheapest processing device. Furthermore, a query should make use of multiple processing devices to speed up execution. Hence, the physical optimizer has to optimize hybrid CPU/GPU query plans, which significantly increases the optimization space.
%%Relational operations are implemented in the next layer. These operators typically use access structures to process data. In GDBMSs, access structures have to be reimplemented on GPUs to achieve a high efficiency. However, depending
%on the processing device chosen by the CPU/GPU scheduler, different access structures are available. This is an additional dependency the query optimizer needs to take into account.
%%Then, a set of parallel primitives can be applied to an access structure to process a query. In this component, the massive parallelism of CPUs and GPUs is fully used to speed up query processing. However, a GPU operator can only work on data stored in GPU memory. Hence, all access structures are built on top of a data-placement component, that caches data on a certain processing device, depending on the access patterns of the workload (e.g., certain columns for column scans or certain nodes of tree indexes). Note that the data- placement strategy is the most performance critical component in a GDBMS due to the major performance impact of data transfers.%%The backbone of a GDBMS is a typical in-memory storage, which frequently stores data in a column-oriented format. Compression techniques are not only beneficial in keeping the major part of a database in-memory, compression also reduces the impact of the PCIe bottleneck.
%
%\subsection{Extension Points for MMDBMSs}%In summary, we can extend most \textit{main-memory DBMSs} (MMDBMSs) supporting column- oriented data layout and bulk processing to be GPU-accelerated DBMSs. 
%We identify the following extension points: Cost models, CPU/GPU scheduler, hybrid query optimizer, access structures and algorithms for the GPU, and a data placement strategy.
%
%\noindent%\textbf{Cost Models:} For each processor, we need to estimate the execution time of an operator. This can be either done by \textit{analytical} cost models or \textit{learning-based} approaches.%%\noindent%\textbf{CPU/GPU Scheduler:} Based on the cost models, a scheduler needs to allocate processing devices for a set of operators.%%\noindent%\textbf{Hybrid Query Optimizer:} The query optimizer needs to consider the data transfer bottleneck and memory requirements of operators to create a suit- able physical execution plan. Thus, the optimizer should make use of cost models, a CPU/GPU scheduler, and heuristics minimizing the time penalty of data transfers.%%\noindent%\textbf{Access structures and algorithms for the GPU:} In order to support GPU- acceleration, a DBMS needs to implement access structures on the GPU (e.g., columns or B+-trees) and operators that work on them. 
%%\noindent%\textbf{Data Placement Strategy:} A DBMS needs to keep track of which data is stored on the GPU, and which access structure needs to be transferred to GPU memory. Aside from a manual memory management, it is also possible to use techniques such as UVA and let the GPU driver handle the data transfers transparently to the DBMS. However, this may result in less efficiency because a manual memory management can exploit knowledge about the DBMS and the workload.
%%Implementing these extensions is a necessary precondition for a DBMS to sup- port GPU co-processing efficiently.
%
%
%
%\section{Conclusion}
%The pioneer of modern co-processors is the GPU, and many prototypes of GPU-accelerated DBMSs have emerged over the past seven years implementing new co-processing approaches and proposing new system architectures. We argue that we need to take into account tomorrows hardware in today's design decisions. Therefore, in this paper, we theoretically explored the design space of GPU-aware database systems. 
%
%In summary, we argue that a GDBMS should be an \textit{in-memory}, \textit{column-oriented} DBMS using the \textit{block-at-a-time} processing model, possibly extended by a \textit{just-in-time-compilation} component. The system should have a query optimizer that is aware of \textit{co-processors} and \textit{data-locality}, and is able to distribute a workload across all available (co-)processors.


\bibliographystyle{abbrv}
\bibliography{sqlonhadoop}

\end{document}
