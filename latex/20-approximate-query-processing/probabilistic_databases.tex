% Probabilistic Databases

\section{Probabilistic Databases}
Probabilistic databases are databases where the value of some attributes or the presence of some records are uncertain and known only with some probability. 
Applications in many areas such as information extraction, RFID and scientific data management, data cleaning, data integration, and financial risk assessment produce large volumes of uncertain data, which are best modeled and processed by a probabilistic database.

Traditional relational databases are \textit{deterministic}. 
Every record stored in the database is meant to be present with \textit{certainty}, and every field in that record has a precise, unambiguous value. 
The theoretical foundations and the intellectual roots of relational databases are in First Order Logic, which is essentially the relational calculus, the foundation of query languages such as SQL. 
In First Order Logic, the fundamental question is whether a logical sentence is true or false. 
Logical formulas under first-order semantics can be used to assert that a record is, or is not, in a relation, or in a query result, but they cannot make any less precise statement. The original applications that motivated the creation of relational databases required certain query results: accounting, inventory, airline reservations, and payroll. 
Database systems use a variety of tools and techniques to enforce this, such as integrity constraints and transactions.

Today, however, data management needs to include new data sources, where data are uncertain, and which are difficult or impossible to model with traditional semantics or to manage with a traditional Relational Database Management System (RDBMS). 
For an illustration, consider \textit{Business Intelligence} (BI), whose goal is to extract and analyze business data by mining a large collection of databases. 
BI systems can be made more useful by including \textit{external data}, such as twitter feeds or blogs, or email messages in order to extract even more valuable business information. 
For example, by analyzing blogs or twitter feeds and merging them with offline databases of products, companies can obtain early feedback about the quality of a new product or its degree of adoption, such as for a new car model, a new electronic gadget, or a new movie; such knowledge is very valuable, both for manufacturers and for investors. However, a traditional RDBMS requires the data to be precise: for each tweet, the system needs to know precisely what product it mentions and whether the comment is favorable or unfavorable. The data must be cleaned before it can be used in a traditional RDBMS.

The goal of \textit{Probabilistic Databases} is to extend todayâ€™s database technology to handle \textit{uncertain} data. 
The uncertainty is expressed in terms of probabilities: a tuple is present only with some probability, or the value of an attribute is given by a probability distribution. Probabilistic databases are expected to scale as well as traditional database systems, and they should support queries as complex as those supported by advanced query processors today; however, but they will do this while allowing the data to be uncertain, or probabilistic. Both the data and their probabilities are stored in standard relations. The semantics, however, is \textit{probabilistic}: the exact state of the entire database is not known with precision; instead, it is given by a probability distribution. 
When an SQL query is executed, the system returns a set of answers and it annotates each answer with a probability, representing the degree of confidence in that output. Typically, the answers are ranked in decreasing order of their output probability, so that users can inspect the top, most credible answers first. 
Thus, the main use of probabilities is to record the degree of uncertainty in the data and to rank the outputs to a query; in some applications, the exact output probabilities matter less to the user than the ranking of the outputs. Probabilistic databases have a major advantage in processing uncertain data over their traditional counterparts. The data can be simply stored in the database without having to be cleaned first. Queries can be run immediately on the data. Cleaning can proceed gradually if and when more information becomes available by simply adjusting the probability value until it becomes 1.0, in which case the data becomes certain, or 0.0, in which case the data item can be removed. Even data that cannot be cleaned at all and will remain forever uncertain can still be stored and queried in a probabilistic database system.

Probabilistic databases take an evolutionary approach: the idea is to extend relational technology with a probabilistic semantics, rather than to develop a new artifact from scratch. All popular database techniques should carry over automatically to a probabilistic database: indexes, query optimization, advanced join algorithms, parallel query processing, etc. The goal is to extend the existing semantics of relational data to represent uncertainties but keep all the tools and techniques that have been proven so effective on deterministic data. 
%As we will see in this book, this is not an easy task at all. The foundations of probabilistic databases are in First Order Logic extended with probabilities where the computational complexity of inference and model checking problems has only recently started to be understood.

