% This is a simple LaTex sample document that gives a submission format
%   for IEEE PAMI-TC conference submissions.  Use at your own risk.
% Make two column format for LaTex 2e.\
\documentclass[11pt,twocolumn]{article} %,twocolumn
\usepackage{times,amsmath,amsfonts}

% Use following instead for LaTex 2.09 (may need some other mods as well).
%\documentstyle[times,twocolumn]{article}
\usepackage[dvips]{graphicx,graphics}
% Set dimensions of columns, gap between columns, and paragraph indent
\setlength{\textheight}{10.5in} \setlength{\textwidth}{7.6in}
%\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{-1in}
\setlength{\parindent}{1pc}
\setlength{\oddsidemargin}{-0.6in}  % Centers text.
\setlength{\evensidemargin}{-0.5in}

% Add the period after section numbers.  Adjust spacing.
\newcommand{\Section}[1]{\vspace{-5pt}\section{\hskip -1em.~~#1}\vspace{-3pt}}
\newcommand{\SubSection}[1]{\vspace{-2pt}\subsection{\hskip -1em.~~#1}
        \vspace{-3pt}}
\newcommand{\bqn}{\begin{eqnarray}}
\newcommand{\eqn}{\end{eqnarray}}

\begin{document}

% Make title bold and 14 pt font (Latex default is non-bold, 16pt)
\title{Stat 471: Lecture 25\\
Bayesian Inference II.}
% For single author (just remove % characters)
\author{Moo K. Chung\\
mchung@stat.wisc.edu}
% For two authors (default example)
\maketitle \thispagestyle{empty}
\begin{enumerate}

\item Consider a simple linear model $$Y_i = \beta_0 + \epsilon_i,
\; i=1,\cdots,n$$ where $\epsilon_i \sim N(0,1)$. Suppose 3, 2, 4,
2, 1 are observations. We know somehow that the prior to be
$\pi(\beta_0)
\sim N(3,1)$. Estimate $\beta_0$ using MCMC.\\

{\em Solution.} This is a dumb example but serves the purpose of
illustrating the use of MCMC for parameter estimation. Note that
$Y_i|\beta_0 \sim N(\beta_0, 1)$. The posterior is
\bqn\pi(\beta_0|y) &=& cf(y_1,\cdots,y_n|\beta_0)\pi(\beta_0)\\
&=& cf(y_1|\beta_0)\cdots f(y_n|\beta_0)\pi(\beta_0) \eqn

where $c$ is a normalizing constant, i.e.
$$c^{-1}=\int_{-\infty}^{\infty}
f(y_1,\cdots,y_n|\beta_0)\pi(\beta_0) \; d\beta_0.$$

Then the posterior is
$$\pi(\beta_0|y) \propto \exp \Big(-\frac{(\beta_0-3)^2}{2}\Big)
\exp \Big(-\frac{\sum_{i=1}^n(\beta_0-y_i)^2}{2}\Big).$$

The Bayes estimate $\hat \beta_0 = \mathbb{E}(\beta_0|y)$ can be
computed analytically in this particular example but requires some
algebraic manipulation and if the prior is something like
$\pi(\beta_0) \propto \frac{1}{1+\beta_0^2}$, it may not even
possible to compute the Bayes estimate analytically.

This gives a motivation for computing $\hat \beta_0$ via MCMC. We
use the random-walk Metropolis algorithm with symmetric
propositional density $q(y|x) \sim N(x,1)$. For this particular
distribution
$$q(y|x)=\frac{1}{\sqrt{2\pi}} e^{-\frac{(y-x)^2}{2}} =
q(x|y).$$
 So function $\alpha$ in the MCMC algorithm is trivially
$$\alpha(a,b) = \min \Big[1, \frac{\pi(a|y)}{\pi(b|y)}\Big].$$
Then we generate Markov chain $X_0 \to X_1 \to X_2 \cdots$ with
invariant distribution $\pi(\beta_0|y)$. For large $i$, $X_i \sim
\pi(\beta_0|y)$.

\begin{verbatim}
n=200;
y=[3 2 4 2 1];
X(1)=10;
for i=2:n
  candidate=normrnd(X(i-1),1);
  pi_n=normpdf(candidate,3,1)*
       prod(normpdf(candidate,y,1));
  pi_d=normpdf(X(i-1),3,1)*
       prod(normpdf(X(i-1),y,1));
  alpha=min([1,pi_n/pi_d]);
  u=rand;
  if u <= alpha
    X(i)=candidate;
  else
    X(i)=X(i-1);
  end;
end
>> mean(X(50:200))
ans =
    2.4498
>> mean(y)
ans =
    2.4000
\end{verbatim}
\begin{figure}
\centering
\renewcommand{\baselinestretch}{1}
\includegraphics[scale=0.4]{lecture25-1.eps}
\end{figure}
\item {\em Hierarchical model.} Assume $Y_i \sim N(\beta_0,1)$ in
the previous model. Give prior $\beta_0 \sim \pi(\beta_0|\gamma)$,
$\gamma \sim \pi(\gamma)$. In this case, we construct the Gibbs
sampler
$$\beta_0 \sim \pi(\beta_0|y,\gamma),$$
$$\gamma \sim \pi(\gamma|y,\beta_0).$$
\end{enumerate}



\end{document}
