% This is a simple LaTex sample document that gives a submission format
%   for IEEE PAMI-TC conference submissions.  Use at your own risk.
% Make two column format for LaTex 2e.\
\documentclass[11pt,twocolumn]{article} %,twocolumn
\usepackage{times,amsmath,amsfonts}

% Use following instead for LaTex 2.09 (may need some other mods as well).
%\documentstyle[times,twocolumn]{article}
\usepackage[dvips]{graphicx,graphics}
% Set dimensions of columns, gap between columns, and paragraph indent
\setlength{\textheight}{10.5in} \setlength{\textwidth}{7.6in}
%\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{-1in}
\setlength{\parindent}{1pc}
\setlength{\oddsidemargin}{-0.6in}  % Centers text.
\setlength{\evensidemargin}{-0.5in}

% Add the period after section numbers.  Adjust spacing.
\newcommand{\Section}[1]{\vspace{-5pt}\section{\hskip -1em.~~#1}\vspace{-3pt}}
\newcommand{\SubSection}[1]{\vspace{-2pt}\subsection{\hskip -1em.~~#1}
        \vspace{-3pt}}
\newcommand{\bqn}{\begin{eqnarray*}}
\newcommand{\eqn}{\end{eqnarray*}}
\newcommand{\bq}{\begin{eqnarray}}
\newcommand{\eq}{\end{eqnarray}}

\begin{document}

% Make title bold and 14 pt font (Latex default is non-bold, 16pt)
\title{Stat 471: Lecture 29\\
Monte-Carlo smoothing.}
% For single author (just remove % characters)
\author{Moo K. Chung\\
mchung@stat.wisc.edu}
% For two authors (default example)
\maketitle \thispagestyle{empty}
\begin{enumerate}
\item In the previous lecture, we introduced the {\em
deterministic} bivariate smoothing. There is a Monte-Carlo version
of bivariate smoothing.
Suppose we have a following model

$$Y = g(p) + \epsilon$$
where $p_i \in \mathbb{R}^k$. For example, we might have the
following simple linear model $$\tt{house value} =
\beta_0+\beta_1\tt{longitude} + \beta_2 \tt{latitude} +
\tt{error}.$$

Assuming $\mathbb{E} \epsilon =0$, we have $g(p) = \mathbb{E} Y$.
If we have $m$ observations at position $p_0$, we might have $m$
different observations $y_0^1,\cdots, y_0^m$. So we can estimate
regression function $g$ at position $p_0$ as

\bq\widehat g(p_0) = \frac{1}{m}\sum_{j=1}^m
y_0^j.\label{eq:ghat1}\eq

\begin{figure}
\centering
\renewcommand{\baselinestretch}{1}
\includegraphics[scale=0.25]{lecture29-1.eps}
\includegraphics[scale=0.25]{lecture29-2.eps}
\end{figure}
\item However, we usually do not have repeated measurements at
position $p_0$. Rather we will have observations at $p_1,\cdots,
p_m$ around $p_0$. Let $y_1,\cdots, y_m$ be the observations taken
at $p_1, \cdots, p_m$ respectively. If $\|p_i -p_0\|$ is small, we
can take $y_i$ to be another measurement at $p_0$. On the other
hand if $\|p_i - p_0\|$ is large, we may assume that observation
$y_i$ is highly unlikely to be another measurement at $p_0$. Then
we can assign probability to each observation $y_i$ at $p_i$.
$$w_i = P(y_i \mbox{ will occure at } p_0) \propto \exp \big(
-\frac{\|p_i -p_0\|^2}{2\sigma^2}\big).$$ We normalize it such
that
$$w_i = \frac{\exp \big(-\frac{\|p_i -p_0\|^2}{2\sigma^2}\big)}{ \sum_{i=0}^m \exp\big(
-\frac{\|p_i -p_0\|^2}{2\sigma^2}\big)}.$$ This is one example of
assigning probabilities to each $y_i$. We estimate $g$ as \bq
\widehat g(p_0) = \mathbb{E}_w Y_0 =\sum_{i=0}^m
 w_iy_i.\label{eq:ghat2}\eq Note that when $p_1=p_2=\cdots=p_m$,
(\ref{eq:ghat1}) becomes a special case of (\ref{eq:ghat2}). The
estimation (\ref{eq:ghat2}) is the {\em multivariate kernel
smoothing} we studied in lecture 28.

\item Instead of computing weights directly, there is an alternate
Monte-Carlo method. Suppose $X \sim f$ be the density of $X$. Then
for function $Y(p) =g(p) + \epsilon(p)$, \bqn \mathbb{E}  \;Y(X)
&=&
\frac{\int y(x)f(x) \;dx}{\int f(x) \; dx} \\
&\approx& \sum_{i=0}^m w_iy(p_i) = \sum_{i=0}^m w_iy_i.\eqn We
will choose $f$ such that it will give weights $w_i$ similar to
the ones described in the previous section. Then we estimate
$$\widehat g(p_0) = \frac{1}{n}\sum_{i=1}^{n} y(X_i),$$
where $p(X_i =p_i) = w_i$.
\begin{verbatim}
x=-30:30;
g=0.2*x.*exp(-x.^2/100) + cos(x/30).^2;
y=g+0.2*randn(1,61);

m=10; sigma=2;
temp_y=y;
mc_y=zeros(1,61); mc_run=50;
for i=1:mc_run
    for i=1:61
        X=fix(normrnd(i,sigma,100,1));
        p=X(find((X>=1) & (X <=61)));
        p=p(1:m);
        smooth_y(i) = mean(temp_y(p));
    end;
    %hold on;plot(x,smooth_y,'k:')
    mc_y=mc_y+smooth_y;
end;
mc_y=mc_y/mc_run;
\end{verbatim}
%Note that there are many different versions of MC smoothing.
\end{enumerate}
\end{document}
