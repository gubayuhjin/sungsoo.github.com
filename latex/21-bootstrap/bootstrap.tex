\documentclass[12pt]{article}
   \textheight=25.cm
   \textwidth=16cm
   \oddsidemargin=-.2cm
   \topmargin=-1.7cm
\begin{document}
\centerline{\Large\bf Bootstrapping for estimating MSE of some
estimator}
\begin{enumerate}
\item When estimating $\theta(F)$, we need  to assess the {\bf\em quality} of the estimator $g({\vec X})\equiv
g(X_1,\ldots,X_n)$; that is, to estimate the {\em mean square error}
(MSE) of $g({\vec X})$ , where $X_i\stackrel{i.i.d.}{\sim} F,
i=1,\ldots,n$. \bigg(\mbox{\scriptsize We like to have
$\mbox{E}(g({\vec X}))=\theta$ and small $\mbox{Var}(g({\vec X}))$
or}\\
\mbox{\scriptsize small MSE$=\left[\mbox{E}(g({\vec X}))-\theta
\right]^2+\mbox{Var}(g({\vec X}))$.} \bigg)

\item For estimator
$g({\vec X})$ to be useful, we need to estimate   the MSE of
$g({\vec X})$, denoted by $\mbox{MSE$(F)$}\equiv E_F\!\!\left[
(g({\vec X})-\theta(F))^2\right]$. \bigg(\mbox{\scriptsize If
$g({\vec X})$ is unbiased for $\theta$ (i.e., $\mbox{E}(g({\vec
X}))=\theta$), then MSE$=\mbox{Var}(g({\vec X}))$.}\\
\mbox{\scriptsize Furthermore, if $g({\vec X})\sim \mbox{normal}$,
we have $\mbox{P}\!\!\left(\left|g({\vec X})-\theta\right|\le
2\sqrt{\mbox{Var}(g({\vec X}))}\right)=95\%$.}\bigg)

\item {\bf When $F$ is known, we can theoretically compute
$\mbox{MSE$(F)$}$.}  However, $F$ is often unknown and can be
estimated by the empirical distribution function
\begin{eqnarray*}
   && F_e(x)  =\frac{\mbox{number of $i: X_i\le x$}}{n}, \qquad x\in R\\\\
   &\Longleftrightarrow & \mbox{P}(Y=x_i)=\frac{1}{n},\qquad
   i=1,\ldots,n.\qquad(\mbox{\scriptsize This can be easily seen in the plot of $F_e(x)$})
\end{eqnarray*}

\item {\bf Another way of thinking about $F_e$} is that it is the
distribution function of a random variable $Y$ which is equally
likely to take on any of the $n$ values \linebreak $x_i, \; i=1,
\ldots, n$. That is, $Y\sim \mbox{uniform}(x_1,\ldots,x_n)$.

\item By the strong law of large numbers, $F_e\approx F$ as $n
\longrightarrow \infty$, implying that
\begin{eqnarray*}
\mbox{MSE$(F_e)$}\approx
 \mbox{MSE$(F)$}
\end{eqnarray*}
with $\mbox{MSE$(F_e)$}\equiv E_{F_e}\!\!\left[ (g({\vec
Y})-\theta(F_e))^2\right]=
\sum_{i_1}\cdots\sum_{i_n}\frac{[g(x_{i_1},\ldots,x_{i_n})-\theta(F_e)]^2}{n^n}$,
an average of $n^n$ terms.


\item  Since $F_e$ is known, we can either {\bf analytically derive}
\mbox{MSE$(F_e)$} or  {\bf exactly compute} \mbox{MSE$(F_e)$} when
$n^n$ is not too large.
\\
\item If $n^n$ is large, estimate \mbox{MSE$(F_e)$} by simulation. \\\\
{\bf First compute $\theta(F_e)$}, then {\bf sample}
$(Y_1^1,\ldots,Y_n^1), \ldots, (Y_1^b,\ldots,Y_n^b)$, and finally
evaluate $g(Y_1^1,\ldots,Y_n^1),\ldots, g(Y_1^n,\ldots,Y_n^n)$ to
form the estimator
\begin{eqnarray*}
{\widehat {\mbox{MSE}}} (F_e) =
\sum_{j=1}^b\frac{[g(Y_1^j,\ldots,Y_n^j)-\theta(F_e)]^2}{b}.
\end{eqnarray*}
It has been
reported that choosing $b=100$ is usually sufficient.
\\\\
{\bf Key:} $\theta=?$,  $g({\vec X})=?$, and $\theta(F_e)=?$
\newpage
{\bf Example:} When estimating $\theta(F)={\mbox{E}}(X)$  by ${\bar
X}$, we can estimate its MSE by $S^2/n$. Now estimate its MSE by
bootstrapping as follows.
\\\\
Suppose the realized sample is $\{x_1,\ldots,x_n\}$. \\
Since $Y\sim
\mbox{uniform}(x_1,\ldots,x_n)$, we have
$\theta(F_e)=\sum_{i=1}^n\frac{1}{n}\cdot x_i={\bar x}$.
\\\\
{\bf Analytically derive} the MSE$(F_e)$ as follows.
\begin{eqnarray*}
\mbox{MSE}(F_e) &=&\mbox{E}_{F_e}\!\!\left[\left({\bar
Y}-\underbrace{{\bar x}}_{\mbox{
                     \tiny $=\mbox{E}(Y)=\mbox{E}({\bar Y})$}}
\right)^2
\right]\\
&=&\mbox{Var}_{F_e}\!\!\left({\bar Y}\right)%
\\
&=&\mbox{Var}_{F_e}\!\!\left( Y \right)/n.
 \end{eqnarray*}
 Since
\begin{eqnarray*}
\mbox{Var}_{F_e}\!\!\left( Y \right)=\mbox{E}_{F_e}\!\!\left[\left(
Y-{\bar x} \right)^2 \right]=\sum_{i=1}^n(x_i-{\bar x})^2/n,
\end{eqnarray*}
we have
\begin{eqnarray*}
\mbox{MSE}(F_e) =\sum_{i=1}^n(x_i-{\bar x})^2/n^2\;=\;(S^2/n)\cdot
\left(\frac{n-1}{n}\right)\approx  S^2/n.
\end{eqnarray*}
\end{enumerate}
\noindent\\
{\bf Key:} $\theta=\mu$,  $g({\vec X})={\bar X}$, and
$\theta(F_e)={\bar x}$ \noindent\\\\ {\bf Example:} If $n=2$ and
$X_1=1$ and $X_2=3$, what is the bootstrap estimate of
$\mbox{Var}(S^2)$?
\begin{enumerate}
\item {\bf Bootstrapping technique is used to estimate MSE of some
estimator ${\widehat \theta}$.} For unbiased estimator,
MSE$({\widehat \theta})=\mbox{Var}({\widehat \theta})$.

\item
Since $S^2$ is an unbiased estimator of $\sigma^2$, estimating
$\mbox{Var}(S^2)$ is equivalent to estimating MSE$(S^2)$. \\
\\
Thus, consider $\theta=\sigma^2$ and $g({\vec X})=S^2$. Then
\begin{eqnarray*}
\mbox{MSE}(F_e)=\mbox{E}_Y\!\!\left[(g({\vec
Y})-\theta(F_e))^2\right]
\end{eqnarray*}
where $\theta(F_e)=\left[(1-2)^2+(3-2)^2\right]/2=1$ and
\begin{eqnarray*}
g({\vec Y})=\left\{\begin{array}{ll} 0,
  & \mbox{if $(Y_1,Y_2)=(1,1)$}\\
   0,
  & \mbox{if $(Y_1,Y_2)=(3,3)$}\\
   \left[(1-2)^2+(3-2)^2\right]/(2-1)=2,
  & \mbox{if $(Y_1,Y_2)=(1,3)$}\\
   2,
  & \mbox{if $(Y_1,Y_2)=(3,1)$},
\end{array}\right.
\end{eqnarray*}
yielding
$\mbox{MSE}(F_e)=\left[(0-1)^2+(0-1)^2+(2-1)^2+(2-1)^2\right]/2^2=1$.
\\
So the bootstrap estimate of $\mbox{Var}(S^2)$  is equal to $1$.
\\\\
{\bf Key:} $\theta=\sigma^2$,  $g({\vec X})=S^2$, and
$\theta(F_e)=\sigma^2(F_e)=1$.
\end{enumerate}
\end{document}
