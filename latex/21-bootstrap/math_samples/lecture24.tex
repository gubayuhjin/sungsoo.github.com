% This is a simple LaTex sample document that gives a submission format
%   for IEEE PAMI-TC conference submissions.  Use at your own risk.
% Make two column format for LaTex 2e.\
\documentclass[11pt,twocolumn]{article} %,twocolumn
\usepackage{times,amsmath,amsfonts}

% Use following instead for LaTex 2.09 (may need some other mods as well).
%\documentstyle[times,twocolumn]{article}
\usepackage[dvips]{graphicx,graphics}
% Set dimensions of columns, gap between columns, and paragraph indent
\setlength{\textheight}{10.5in} \setlength{\textwidth}{7.6in}
%\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{-1in}
\setlength{\parindent}{1pc}
\setlength{\oddsidemargin}{-0.6in}  % Centers text.
\setlength{\evensidemargin}{-0.5in}

% Add the period after section numbers.  Adjust spacing.
\newcommand{\Section}[1]{\vspace{-5pt}\section{\hskip -1em.~~#1}\vspace{-3pt}}
\newcommand{\SubSection}[1]{\vspace{-2pt}\subsection{\hskip -1em.~~#1}
        \vspace{-3pt}}
\newcommand{\bqn}{\begin{eqnarray}}
\newcommand{\eqn}{\end{eqnarray}}

\begin{document}

% Make title bold and 14 pt font (Latex default is non-bold, 16pt)
\title{Stat 471: Lecture 24\\
Bayesian Inference.}
% For single author (just remove % characters)
\author{Moo K. Chung\\
mchung@stat.wisc.edu}
% For two authors (default example)
\maketitle \thispagestyle{empty}
\begin{enumerate} 

\item For model $Y_i = g(x_i; \beta) + \epsilon_i$, we studied two methods for estimating parameters $\beta$ and doing statistical inference. In the parametric regression, the distribution of $\epsilon_i$ is specified, i.e. $\epsilon_i \sim (0,\sigma^2)$ with unknown variance $\sigma^2$. In the bootstrap approach, we did not assume any distribution form for $\epsilon_i$. In both parametric and nonparametric bootstrap approach, we assumed $\beta$ to be fixed unknown scalar values to be estimated. 

\\
In the {\em Bayesian} framework, parameters are assumed to be random and described by a probability distribution called the {\em prior distribution} $\pi(\beta)$. This is formulated before the data are observed based on a certain prior belief. Let $f(Y|\beta)$ be the sampling distribution of a sample $Y_i$. The {\em posterior distribution} of $\beta$ is given by {\em Bayes formula}
$$\pi(\beta|Y) = \frac{f(Y|\beta)\pi(\beta)}{\int f(Y|\beta)\pi(\beta)\; d\beta}.$$Note that 
$$\pi(\beta|Y) \propto f(Y|\beta)\pi(\beta)= \pi(\beta)\Pi_{i=1}^n f(y_i|\beta).$$
The product term is the {\em likelihood}. The Bayes estimate $\hat \beta$ would be the mean of the posterior distribution,
i.e. 
$$\hat \beta = \mathbb{E} (\beta|Y)= \frac{\int \beta f(Y|\beta)\pi(\beta) \; d\beta}{\int f(Y|\beta)\pi(\beta) \; d\beta}.$$
In general one need to compute $\mathbb{E}(h(\beta)|Y)$ for Bayesian inference but this may be hard and Monte-Carlo integration based on MCMC might be one answer.

\item Consider a simple linear model $$Y_i = \beta_0 + \epsilon_i, \; i=1,\cdots,n$$ where $\epsilon_i \sim N(0,\sigma^2)$ with known $\sigma^2$. This can be written as $Y_i|\beta_0 \sim N(\beta_0, \sigma^2)$. Let the prior be $N(\mu,\tau^2)$. Then the posterior is
$$\pi(\beta_0|y) \propto \exp(-\frac{(\beta_0-\mu)^2}{2\tau})\exp(-\frac{\sum_{i=1}^n(\beta_0-y_i)^2}{2\sigma^2}).$$
We can find, for instance, the Bayes estimate $\hat \beta$ analytically but let us estimate via MCMC. Let $y_i$ be $\tt{grip}$ from $\tt{strength.data}$. Based on the random-walk Metropolis algorithm, we simulate the posterior $\pi(\beta_0|Y)$ based on the symmetric propositional density $q(y|x) \sim N(x,1)$.
\begin{verbatim}
sigma=23.6; mu=110.2; tau=5
n=1500; b(1)=90;
for i=2:n
  y=b(i-1) + normrnd(0,1);
  pi_n=normpdf(y,mu,tau)*prod(
         normpdf(grip,y,sigma));
  pi_d=normpdf(b(i-1),mu,tau)*prod(
        normpdf(grip,b(i-1),sigma));
  alpha=min([1,pi_n/pi_d]);
  u=rand;
  if u <= alpha
    b(i)=y;
  else
    b(i)=b(i-1)
  end
end
>> mean(b(500:1500)) 
ans =
  110.7409
\end{verbatim}
\begin{figure}
\centering
\renewcommand{\baselinestretch}{1}
\includegraphics[scale=0.4]{lecture24-1.eps}
\end{figure}
\item Homework 5. Fit model $Y_i =\beta_0 + \beta_1x_i + \epsilon_i$ using the 2D Gibbs sampler from lecture 23. You need to simulate 2D Markov chain $(\beta_0^i,\beta_1^i)$ whose invariant distribution is the posterior $\pi(\beta_0,\beta_1|Y)$. All these examples are somewhat boring use of MCMC. Use it for complicated nonlinear model fitting when it is hard to compute $\mathbb{E}(h(\beta)|Y)$. 

\item Lecture 25-27 will deal with Monte-Carlo optimization and the EM algorithm. Please read the textbook.
\end{enumerate}



\end{document}


