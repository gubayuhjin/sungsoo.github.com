% This is a simple LaTex sample document that gives a submission format
%   for IEEE PAMI-TC conference submissions.  Use at your own risk.

% Make two column format for LaTex 2e.\
%\documentclass[10pt,twocolumn]{article} %,twocolumn
\documentclass[11pt,twocolumn]{article} %,twocolumn
\usepackage{times,amsmath,amsfonts}
\usepackage[dvips]{graphicx,graphics}
% Use following instead for LaTex 2.09 (may need some other mods as well).
%\documentstyle[times,twocolumn]{article}

% Set dimensions of columns, gap between columns, and paragraph indent
\setlength{\textheight}{10in} \setlength{\textwidth}{6in}
%\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}

% Add the period after section numbers.  Adjust spacing.

\setlength{\textheight}{10.5in} \setlength{\textwidth}{7.6in}
%\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{-1in}
\setlength{\parindent}{1pc}
\setlength{\oddsidemargin}{-0.6in}  % Centers text.
\setlength{\evensidemargin}{-0.5in}

% Add the period after section numbers.  Adjust spacing.
\newcommand{\Section}[1]{\vspace{-5pt}\section{\hskip -1em.~~#1}\vspace{-3pt}}
\newcommand{\SubSection}[1]{\vspace{-2pt}\subsection{\hskip -1em.~~#1}
        \vspace{-3pt}}
\newcommand{\bqn}{\begin{eqnarray*}}
\newcommand{\eqn}{\end{eqnarray*}}
\newcommand{\bq}{\begin{eqnarray}}
\newcommand{\eq}{\end{eqnarray}}


\begin{document}

% Make title bold and 14 pt font (Latex default is non-bold, 16pt)
\title{Stat 471: Lecture 33\\
Gaussian Processes}
% For single author (just remove % characters)
\author{Moo K. Chung \tt{mchung@stat.wisc.edu}}
% For two authors (default example)
\maketitle \thispagestyle{empty}


%\begin{figure}
%\centering
%\renewcommand{\baselinestretch}{1}
%\includegraphics[scale=0.8]{homework05-1.eps}
%\end{figure}

\begin{enumerate}
\item In lecture 28 we simulated signal of the form \bq  Y_i =
g(p_i) + \epsilon_i\label{Cov}\eq for the bivariate smoothing
estimation of $g$. We assumed $\epsilon_i$ to be i.i.d.
$N(0,\sigma^2)$.
\begin{verbatim}
[px,py] = meshgrid([-3:0.1:3]);
g = px.*exp(-px.^2-py.^2);
e=normrnd(0,1,61,61);
y1=g+0.2*e;
\end{verbatim}
The above error assumption is unrealistic since most observations
$y_i$ are correlated. The reasonable assumption would be that the
covariance between $Y_i$ and $Y_j$ would be a function of distance
between $p_i$ and $p_j$, i.e. $${Cov} (Y_i,Y_j) \propto
\rho(\|p_i-p_j\|)$$ for some function $\rho$. One way to simulate
correlated noise $\epsilon_i$ is based on Gaussian random
processes.

\item A random process $\epsilon(t)$ is a Gaussian process if
$\epsilon(t_1),\cdots, \epsilon(t_m)$ are multivariate normal for
any $t_i \in \mathbb{R}^N$. A mean zero Gaussian random variable
is completely characterized by variance. Similarly mean zero
Gaussian process is characterized by the {\em covariance function}
$R$ which is defined as $R(t,s) = \mathbb{E}\big[
\epsilon(t)\epsilon(s)\big]$. Sometimes it is called the {\em
autocovariance function}. Note that $R(t,t)$ gives the variance of
$\epsilon(t)$. If we specify $R(t,s)$, we know the corresponding
Gaussian process $\epsilon$. Based on Gaussian process, we rewrite
equation (\ref{Cov}),
$$Y(t) = g(t) + \epsilon(t).$$


 \item {\em Linear filtering} of a random process $w$ is
defined as

$$ \epsilon(t) = \int K(t,\tau)w(\tau)  d\tau$$
where $K$ is the kernel of the linear filter. In many
applications, we are interested in the kernel of form $K(t,\tau) =
K(t-\tau)$ and in such case the above linear filtering is called
the {\em convolution}. When $K(t-\tau) \sim N(0,\sigma^2I_N)$, we
have Gaussian kernel smoothing of process $w$, i.e.
$$\epsilon(t) = K * w (t).$$
Sometime this is called the {\em moving average} (MA) technique
for obvious reason.
\begin{figure}
\centering
\renewcommand{\baselinestretch}{1}
\includegraphics[scale=0.25]{lecture33-1.eps}
\includegraphics[scale=0.25]{lecture33-2.eps}
\includegraphics[scale=0.25]{lecture33-3.eps}
\includegraphics[scale=0.25]{lecture33-4.eps}
\end{figure}
\item {\em White noise} is defined as a random process whose
covariance function is proportional to the Dirac-delta function
$\delta$, i.e. $R(t,s) \propto \delta(t-s)$. We can define it as
the limiting density function of $N(0,\sigma^2I_N)$ as $\sigma \to
0$. %One property of the Dirac-delta is
%$$\int f(s-t) \delta (s) ds = f(t)$$
%for any function $f$.
Sometimes this is taken as the definition of
Dirac-delta. {\em White Gaussian noise} is a white noise whose
linear filtering is a Gaussian process. This is the usual
definition of white Gaussian noise that can be used in simulating
Gaussian processes. In practice, we use the discrete white
Gaussian noise which is simply $N(0,\sigma^2I_N)$ random
variables. The discrete version of kernel smoothing is
$$\epsilon(t) = \sum_{i=1}^m K(t-\tau_i)w(\tau_i)$$
where $\tau_i$ are regular grid points in $\mathbb{R}^N$. Assume
further that $w(\tau_i) \sim N(0,\sigma^2I_N)$ and $w(\tau)=0$ if
$\tau \neq \tau_i$. Since any linear combination of Gaussian is
again Gaussian, $\epsilon(t_1), \cdots, \epsilon(t_m)$ must be
multivariate normal. Therefore, process $\epsilon(t)$ constructed
in this way must be a Gaussian process.
\begin{verbatim}
kernel=inline('exp(-(x.^2+y.^2)/
                      (2*sigma^2))')
dx=kron(ones(5,1),[-2:2]);
dy=kron(ones(1,5),[-2:2]');
weight=kernel(0.8,dx,dy)/
         sum(sum(kernel(0.8,dx,dy)))
smooth_e=conv2(e, weight,'same');
 y2=g+0.2*smooth_e;
\end{verbatim}
\end{enumerate}
\end{document}
