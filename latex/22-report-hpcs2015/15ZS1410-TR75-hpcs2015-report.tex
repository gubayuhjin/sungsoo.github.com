
\documentclass[twocolumn]{article}
\usepackage{mathpazo}
\usepackage{microtype}
\usepackage{times}
\usepackage{titlesec} % 1
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
%\usepackage{sectsty} % "제 1 절" ...

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %                              My Commands
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\ii}{\item}
\newtheorem{Def}{Definition}
\newtheorem{Lem}{Lemma}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{graphicx}
\graphicspath{%
        {converted_graphics/}
        {./images/}
}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\usepackage{verbatimbox}

\usepackage[hangul,nonfrench,finemath]{kotex}
    
\setlength\textwidth{7in} 
\setlength\textheight{9.5in} 
\setlength\oddsidemargin{-0.25in} 
\setlength\topmargin{-0.25in} 
\setlength\headheight{0in} 
\setlength\headsep{0in} 
%\setlength\columnsep{5pt}
\sloppy 
 
\begin{document}

\title{
\vspace{-0.5in}\rule{\textwidth}{2pt}
\begin{tabular}{ll}\begin{minipage}{4.75in}\vspace{6px}
\noindent\large {\it KIWI Project}@Data Management Research Section\\
\vspace{-12px}\\
\noindent\LARGE ETRI\qquad  \large Technical Report 15ZS1410-TR-75
\end{minipage}&\begin{minipage}{2in}\vspace{6px}\small
218 Gajeong-ro, Yuseong-gu\\
Daejeon, 305-700, South Korea\\
http:/$\!$/www.etri.re.kr/\\
http:/$\!$/sungsoo.github.com/\quad 
\end{minipage}\end{tabular}
\rule{\textwidth}{2pt}\vspace{0.25in}
\LARGE \bf 국외 출장 보고서 \\
\large The 2015 International Conference on High Performance Computing \& Simulation (HPCS 2015)
}

\date{}

\author{
{\bf 김성수 (Sung-Soo Kim)}\\
\it{sungsoo@etri.re.kr}
}

\maketitle

\begin{abstract}
고성능 컴퓨팅(High-Performance Computing, HPC)은 고급 연산 문제를 풀기 위하여 슈퍼컴퓨터 및 컴퓨터 클러스터를 사용하는 것을 말한다. 
고성능 컴퓨팅 기술은 빅 데이터 분석 플랫폼을 개발하기 위해서 필요한 핵심 요소기술 중 하나다. 
금번 13번째 HPCS 2015 학회는 네델란드 암스테르담 힐튼 호텔에서 열렸다. 
출장자는 논문 발표 및 빅데이터 분산 병렬처리 관련 최신 연구동향 파악을 목적으로 학회에 참석하였다.
금번 학회 논문 채택률은 37\%로 45편의 full paper가  채택되었고, 6개 관련워크샵 참석자들을 합쳐 180여명이 참석하였다.
다음번 학회 HPCS 2016은 오스트리아 빈에서 개최될 예정이다.

본 출장보고서는 HPCS 2015에서 참석했던 튜토리얼 및 세션을 중심으로 연구 동향을 살펴보고, 또한 해당 주제와 관련된 가트너 (Gartner) 자료를 조사하여 함께 정리함으로써, 연구동향 파악에 도움이 될 수 있는 기술문서 형태로 작성하였다. 본 출장보고서가 현재 연구분야 수행 및 신규 프로젝트 기획 시 도움이 되길 진심으로 바란다.
\end{abstract}

\section{서론}
과학 연구와 연관된 고성능 컴퓨터의 이용을 일반적으로 널리 쓰이는 고성능 컴퓨팅의 정의로 본다. 
고성능 컴퓨팅(High Performance Computing, HPC)이라는 분야는 지난 수십년 동안의 IT 기술 발전의 선도 역할을 수행해 왔음에 틀림이 없다. 

HPC 기술 발전의 결과물로 고성능 멀티 코어 프로세스, 저전력 서버 기술, 인피니밴드와 같은 고성능 네트워크 기술, 대용량 저장을 위한 스토리지 시스템 기술 등이 현실화되었으며, 이는 HPC 분야 뿐만 아니라 클라우드로 통칭되는 현대의 모든 IT 기술의 기반이 되었다고 해도 과장이 아니다.

이번 HPCS 2015 학회 \cite{hpcs:2015} 참석을 통해, 이러한 HPC, 클라우드, 빅데이터를 모두 포괄하는\textit{ e-Science}, 즉, 데이터 집약적 과학적 발견 (data-intensive scientific discovery)과 관련된 주요 연구 결과를 살펴 볼 수 있었으며, 필자에게는 진행하고 있는 과제와 관련된 최신 연구동향을 배울 수 있었던 아주 유익한 출장이었다. 
또한, 연구결과에 대한 논문발표 뿐만 아니라, 국제학회에서 세션 좌장 (session chair) 역할을 수행함으로써, 학회에 기여하면서 개인적으로는 좋은 경험을 쌓을 수 있었다.
필자가 금번 출장을 통해 배우게 된 핵심 내용을 키워드로 요약 정리하면 아래와 같다.

\bi
\ii In-memory computing (IMC) capabilities for the Hadoop
\ii IMC-Enabled Hybrid Transaction/Analytical Processing
\ii Science Gateways, Large Scale Distributed Computing
\ii Accelerated Cloud
\ii Software-Defined [\textit{Everything}]
\ei
\subsection{출장 개요}

\bi
\ii 출장자: 김성수 선임연구원 (데이터관리연구실)
\ii 출장목적: HPCS 2015 논문발표 및 빅데이터 분산 병렬처리 관련 최신 연구동향 파악
\ii 출장기간: 2015.07.19 - 2015.07.26 (6박 8일)
\ii 장소: 암스테르담 힐튼호텔 (네델란드)
\ii 수행과제: 배치/온라인 듀얼 모드 빅데이터 분석 플랫폼 기술 개발 [15ZS1410]
\ei

\subsection{출장 세부 일정}

\begin{tabular}{ c | c } 
\hline	
\textbf{일자} & \textbf{주요 활동 내용} \\ 
\hline 
\hline
2015.07.19 & 대전 출발, 암스테르담 도착 \\ 
2015.07.20 - 2015.07.24 & 학회 등록, 논문 발표 (7/22)  \\ 
 					 & 튜토리얼 세션, 논문 세션 참석 \\ 
2015.07.25 - 2015.07.26 & 암스테르담 출발, 대전 도착 \\
\hline 
\end{tabular}

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{hilton.png}
        \caption{학회장소: Hilton Amsterdam Hotel}
        \label{fig:hilton}
\end{figure}

이번 13번째 HPCS 2015 학회는 네델란드 암스테르담 힐튼 호텔에서 열렸다. 호텔은 암스테르담 도시 중심부에서 비교적 가까운 거리 (트램으로 10분)에 위치해 있다.

\section{Tutorials and Keynotes}
본 출장보고서에서 참석한 세션별 각 발표 내용에 대해 아래와 같은 형식으로 정리하고자 한다.
\bi
\ii \textbf{Contributions:} 발표의 주요한 연구 기여에 대해 기술
\ii \textbf{Why:} 왜? 발표 주제가 중요한가? 흥미로운가? 에 대해 기술
\ii \textbf{How:} 연구목표를 달성하기 위해 어떠한 접근법(방법)을 적용했는가 기술
\ii \textbf{What:} 연구 수행결과로 도출된 주요 결과물 기술
\ii \textbf{Technology Trends:} 연구 주제와 관련된 가트너 (Garnet) 자료와 연관된 분석내용을 기술
\ii \textbf{Definition:} 연구 주제로부터 분류된 키워드에 대한 정의를 기술
\ii \textbf{Note:} 연구 주제와 관련된 부연 설명을 기술
\ei

\subsection{Tutorials}
튜토리얼은 학회 첫날(7/20) 총 5개의 주제에 대해 병렬 세션으로 진행되었으며, 3개의 튜토리얼에 참석하였다.
첫번째 튜토리얼은 대규모 분산환경에서 다양한 워크로드를 실행할 수 있는 오픈소스 워크로드 엔진인 OpenMOLE에 대해 4시간 발표가 있었다.
이후 Science Gateways 주제로 2시간 튜토리얼이 진행되었다. 세번째 참석 세션은 인텔 멀티코어 CPU에 있는 벡터처리용 보조 프로세스를 활용한 벡터화(vectorization) 프로그래밍에 대한 주제로 세시간 반의 튜토리얼에 참석했다.

\subsubsection{Model Exploration using OpenMOLE: A Workflow Engine for Large Scale Distributed Design of Experiments and Parameter Tuning}
\textbf{Speakers:} Romain Reuillon$^1$, Mathieu Leclaire, Jonathan Passerat- Palmbach$^2$, $^1$Complex Systems Institute, Paris, France; $^2$BioMedIA Group, Imperial College London, London, U.K.

OpenMOLE (Open MOdeL Experiment)은 분산 컴퓨팅 환경에서 프로그램 실행을 간단하게 수행할 수 있게 해 준다 \cite{openmole:2015}. 3명의 발표자가 튜토리얼을 7월 20일 오전 4시간에 걸쳐 마치 토론하듯이 진행했다.
OpenMOLE은 다양한 입력 (매개 변수 또는 데이터 세트)에 대해 동일한 프로그램을 실행하려면 필요한 도구다. 
OpenMOLE의 일반적인 유즈케이스로는 고성능 모델 정합 (high performance model calibration), 모델 탐사 (model exploration), 기계 학습 (machine learning), 최적화, 데이터 처리가 있다. OpenMOLE은 다음과 같은 특징을 제공한다.
\bi
\ii \textbf{Works with your programs} - Java, Binary exe, NetLogo, R, SciLab, Python, C++...
\ii \textbf{Distributed computing} - Works on your multi-core machines, clusters, grids, desktop grid.
\ii \textbf{Expressive} - Graphical and scripted workflow system to describe your naturally parallel processes.
\ii \textbf{Scalable} - Handles millions of tasks, years of computation, and GBs of data.
\ii \textbf{Mature} - Developed since 2008 and widely used.
\ii \textbf{Open} - AGPLv3 free software license.
\ei

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.49\textwidth]{openmole-code.png}
        \caption{An OpenMOLE Hello World!}
        \label{fig:openmole-code}
\end{figure}

\noindent
\textbf{Contributions:} OpenMOLE은 분산 컴퓨팅 환경에서 다양한 워크로드 실행시 파라미터를 조절해가며 실행할 수 있는 워크로드 실행 엔진이다. 
사용자는 간단한 DSL (Domain-Specific Language)를 이용하여 실행 로직을 코드로 기술함으로 간단히 분산 컴퓨팅 환경 실행을 수행할 수 있다.

\noindent
\textbf{Why:} 다양한 분산 컴퓨팅 환경과 세부 응용 소프트웨어에 대한 파라미터 조절을 해 가며 워크로드를 실행하기 위해서는 워크로드 실행 엔진이 필요하다.

\noindent
\textbf{How:} OpenMOLE은 사용자가 지정한 워크로드를 실행할 수 있는 플랫폼(워크로드 실행엔진)과 도메인 전용 언어(DSL; Domain-Specific Language)를 제공한다.

\noindent
\textbf{What:} OpenMOLE은 오픈소스 프로젝트로 진행 중이며, 관련 프로젝트 웹사이트 (http://next.openmole.org)에 가면 소스 코드 및 실행가이드를 제공받을 수 있다. 그림 \ref{fig:openmole-code}은 OpenMOLE DSL를 이용한 매번 수행시 2씩 곱하는 간단한 응용프로그램이다. 이 계산은 로컬머신에서 멀티 쓰레드 환경에서 실행된다.

또 다른 실습으로 Monte-Carlo Pi estimation 예제와 Random Forest 예제 프로그램을 OpenMOLE에서 실행하는 실습도 함께 진행했다. 실습은 OpenMOLE 프로젝트 사이트 \cite{openmole:2015}에서 필요한 파일을 다운받아 설치 후, command line 또는 웹 GUI를 통해 비교적 쉽게 실습해 볼 수 있다.

\noindent
\textbf{Note}: \textit{Random Forests, Bagging and Boosting} --"Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees." — "Random Forest," Wikipedia, 3 November 2014.

\subsubsection{Science Gateways – Leveraging Modeling and Simulations in HPC Infrastructures via Increased Usability}
\textbf{Speakers:} Sandra Gesing, University of Notre Dame, Indiana, USA

과학 게이트웨이 (Science Gateways)는 분산되어 있는 계산 및 데이터 자원 (사이버 인프라)을 이용하는 과학 커뮤니티를 활성화함으로써 과학적 발견을 획기적으로 가속화하는 가상 환경을 의미한다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{teragrid-compute-cloud.pdf}
        \caption{TeraGrid Compute Cloud}
        \label{fig:teragrid-compute-cloud}
\end{figure}

\noindent
\textbf{Contributions:}  과학 게이트웨이 HPC 컴퓨터 및 스토리지 시스템에 액세스 할 수 있는 웹 기반 인터페이스다.
eScience를 통한 과학적 발견을 하는 데 있어서, 광범위한 과학 커뮤니티의 데이터를 공유하고 활용할 수 있도록 인프라를 제공한다.
게이트웨이는 과학 팀, 데이터에 액세스 공유 계산을 수행하고, 일반적으로 웹을 통해 자원과 상호 작용 할 수 있다. 

\noindent
\textbf{Why:}  많은 과학자들은 가지고 각자 가지고 있는 정보들을 쉽게 공유하고 활용하는 것이 매우 중요하다.

\noindent
\textbf{How:}  더 많은 과학자가 공유 리소스를 활용할 수 있도록 HPC에 사용 편의성을 향상 시키려고 노력한다.
그림 \ref{fig:teragrid-compute-cloud}는 TeraGrid 계산 클라우드를 이용한 HPC 사용 인프라 구조를 보여주고 있다.

\noindent
\textbf{What:}  \textit{Science Gateway Platform as a Service} (SciGaP) provides application programmer interfaces (APIs) to hosted generic infrastructure services that can be used by domain science communities to create Science Gateways.

\bi
\ii \textit{Apache Airavata}: Airavata is open source software to power the job and workflow management used by Science Gateways.

%\ii CIPRES Science Gateway: CIPRES is a popular Science Gateway for conducting online research into phylogenetics using supercomputers managed by the NSF XSEDE project.  Mark Miller is the CIPRES gateway’s project leader, and Terri Schwartz is the lead developer.

\ii \textit{European Grid Infrastructure}:  EGI is a federation of academic computing resources in the European Union that supports high throughput computing.

\ii \textit{Neuroscience Gateway}: NSG is a Science Gateway that provides a Web interface to XSEDE supercomputing resources for the neuroscience community. Amit Majumdar is the project leader for the NSG, Subhashini Sivagnanam and Kenneth Yoshimoto are the lead developers.

\ii \textit{Open Science Grid}: The OSG is a voluntary federation of academic computing systems that supports high throughput computing.

%\ii PRACE:  PRACE is the European Union’s federated high performance computing infrastructure.  PRACE in the EU and XSEDE in the US are roughly similar.
%
%\ii San Diego Supercomputing Center: SDSC is an XSEDE partner and home to the CIPRES and NSG science gateways.  It operates the Trestles and Gordon supercomputers.

\ii \textit{Science Gateways Group @ IU}: The IU Science Gateway Group develops software, participates in the Apache Software Foundation, and provides consulting support on the development and operation of Science Gateways.  Marlon Pierce leads the group and Suresh Marru is the group’s principal software architect.

\ii \textit{Science Gateway Institute} 
%: Nancy Wilkins-Diehr at SDSC leads this NSF Software Infrastructure for Sustained Innovation conceptualization award that includes members of the HUBzero, iPlant, and SciGaP Science Gateway projects.  The institute’s ultimate goal is to foster the Science Gateway community through a range of services, including incubator support, consultation services, gateway frameworks, community forums, and workflow development.

\ii \textit{Science Gateway Security}
%: Jim Basney (NCSA) is the principal investigator of this project, which includes Marlon Pierce and Suresh Marru from the SciGaP team, as well as Rion Dooley from iPlant.  The Science Gateway Security project is developing secure gateway services based on the OAuth standards to help gateways manage their user authentication needs.

%\ii UltraScan Science Gateway:  UltraScan is a comprehensive gateway for the high-resolution analysis of hydrodynamic data using remote supercomputing resources. The Gateway has been in existence since 2004 and was one of the first adopters of the Apache Airavata middleware infrastructure. It is connected to XSEDE supercomputers in SDSC, TACC, local resources at the University of Texas Health Science Center in San Antonio, and at the Jülich Supercomputing Center in Germany, it serves a worldwide user base with high-performance computing resources for biophysical, molecular biology, biomedical and material science applications.
%
%\ii Workbench Framework:  CIPRES and the Neuroscience Gateway are both based on the Workbench Framework.
%
%\ii XSEDE Science Gateways: The NSF XSEDE project integrates access to supercomputer centers across the country. Extended Collaborative Support Services (ECSS) are a major component of XSEDE, allocating time of  XSEDE staff members to work collaboratively with academic researchers. Within ECSS, the Science Gateway program provides experienced staff members who can help develop new gateways or help port existing gateways to use XSEDE resources.  SciGaP encourages researchers needing help with either task to apply for support.
\ei

\subsubsection{Getting Started with the AVX-512 on the Multicore and Manycore Platforms}
\textbf{Speakers:} \\
Elmoustapha Ould-ahmed-Vall, Intel Corporation, Phoenix, Arizona, USA\\ 
Shuo Li, Intel Corporation, Portland, Oregon, USA\\
Bob Valentine, Intel Corporation, Haifa, Israel\\
Jesus Corbal, Intel Corporation, Barcelona, Spain

고급 벡터 확장(Advanced Vector Extensions,약어:AVX)은 2008년 4월 춘계 인텔 개발자 포럼에서 발표된 x86 명령어 집합의 확장으로 SIMD명령어 집합중의 하나이다. SIMD 레지스터의 폭이 128비트에서 256비트로 확장돼서, 최대 2배까지 부동소수점 연산 처리 능력이 향상된다. 또한 기존의 2 피연산자 구조에서 3 피연산자 구조로 변경됨으로 인하여 프로그래밍이 더 효율적이고 성능이 더 뛰어나게 된다. 

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{intel-avx.pdf}
        \caption{Intel Advanced Vector Extensions}
        \label{fig:intel-avx}
\end{figure}

\noindent
\textbf{Contributions:} 그림 \ref{fig:intel-avx}는 Intel Advanced Vector Extensions (Intel AVX)에 대한 로드맵을 보여주고 있다.
향후 Intel의 새로운 제품은 512 비트 SIMD 지원할 예정이다. 이 제품은 단일 명령어 IntelAVX / AVX2 대비 두 배의 처리, 인텔 SSE 성능에 비해 4 배 성능 개선을 가능하게 한다. 특히, 빅데이터 분석에서 데이터레벨 병렬성 (data-level parallelism)을 갖는 문제에 대해 처리속도 개선에 큰 도움이 되리라 기대한다. 출시 예정인 AVX-512에 대한 세부 명령어 및 기능을 배울 수 있었던 유익한 튜토리얼이였다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{intel-table.pdf}
        \caption{AVX-512 ERI \& PRI Description}
        \label{fig:intel-avx-description}
\end{figure}

그림 \ref{fig:intel-avx-description}은 Intel AVX-512 Exponential and Reciprocal Instructions (ERI) 명령어를 보여주고 있다. 그림 \ref{fig:intel-avx-512}는 ERI 및 PRI 명령어의 제공 동기 (motivation)를 보여주고 있다.

\noindent
\textbf{Why:} 
인텔 AVX-512는 대부분 요구되는 계산중심 작업시에 고성능을 제공하기 때문에 중요하다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{intel-avx-512.pdf}
        \caption{AVX-512 ERI \& PRI Motivation}
        \label{fig:intel-avx-512}
\end{figure}

\noindent
\textbf{How:} 인텔 AVX-512 명령어는 명령 기능의 설계에서 전례없는 처리 성능과 최고 수준의 컴파일러 지원을 제공한다.

\noindent
\textbf{Note:} \textit{Intel AVX-512 features} include 32 vector registers each 512-bit wide and eight dedicated mask registers. Intel AVX-512 is a flexible instruction set that includes support for broadcast, embedded masking to enable predication, embedded floating point rounding control, embedded floating-point fault suppression, scatter instructions, high speed math instructions, and compact representation of large displacement values.

\noindent
\textbf{What:} CPUID에 따라 인텔 AVX-512 ERI 및 PRI 명령어를 제공한다.

\subsection{Keynotes}
슈퍼컴퓨팅 변화에 대한 소프트웨어 변화전략, GPU 기반 클라우드 가속화 기술, e-Science를 위한 복합 e-Infrastructure, 유럽지역의 컴퓨팅 인프라 및 로드맵인  EGI-Engage등 4개 주제로 키노트 발표가 있었다. 본 절에서는 슈퍼컴퓨팅 변화에 대한 소프트웨어 변화전략과 GPU 기반 클라우드 가속화 기술에 대한 키노트 발표에 대해 기술한다.

\subsubsection{Architecture-aware Algorithms and Software for Peta and Exascale Computing}
\textbf{Speaker:} Jack Dongarra\\University of Tennessee and Oak Ridge National Lab, Tennessee, USA; University of Manchester, U.K.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{hpc-keynote.png}
        \caption{Jack Dongarra}
        \label{fig:hpc_keynote}
\end{figure}

\noindent
\textbf{Contributions:} 본 키노트 발표에서는 지난 10년 동안 고성능 컴퓨팅 기술 변화에 대해 살펴보고, 주요 기술 동향에 따른 미래 예상되는 HPC 기술들에 대해 설명해 주었다. 그림 \ref{fig:hpc_20}은 지난 20년간 HPC 성능개발 변화 추이를 그래프이다. 현재 아이폰은 20년전 슈퍼컴퓨터의 성능과 동일함을 알 수 있다. 하지만, 발표자는 이러한 하드웨어 성능 개선속도에 비해 소프트웨어 변화가 더딘면이 있다고 지적했다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{hpc-20-years.pdf}
        \caption{Performance development of HPC over the last 20 years.}
        \label{fig:hpc_20}
\end{figure}
\noindent
\textbf{Why:}  지난 20년간 HPC 분야는 하드웨어 측면만을 지나치게 강조해서 연구개발 투자가 이루어져 왔다. HPC 기술 및 패러다임 변화는 우리가 개발하는 소프트웨어에 큰 영향을 미치기 때문에, HPC 기술이 어떻게 진화해 왔는지 그리고 앞으로 어떤 기술변화가 일어날 것인가 예측하는 것은 매우 중요하다. 현재 Top500의 99\% 시스템은 멀티코어 기반의 시스템이라고 한다. 따라서, 이러한 하드웨어 시스템 변화에 대응하는 소프트웨어 변화가 필요하다. 그림 \ref{fig:hpc_cores}는 슈퍼컴퓨터당 평균 코어수 변화 추이를 보여주고 있다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{hpc_cores.pdf}
        \caption{Average number of cores per supercomputer.}
        \label{fig:hpc_cores}
\end{figure}

\noindent
\textbf{How:}  소프트웨어 및 알고리즘 개발에 영향을 미칠 주요한 다섯가지 연구영역으로 나누어 기술변화를 예측하였다.
\bi
\ii 멀티코어 및 하이브리드 아키텍쳐에 맞게 소프트웨어 재설계 
\ii 자동 조정되는 응용 소프트웨어
\ii  성능을 위한 혼합 정밀도 활용하기
\ii 내고장성의 중요성
\ii 통신 회피 알고리즘
\ei

\noindent
\textbf{What:} Exascale computing으로 진화해 나가고 있는 HPC 하드웨어 및 소프트웨어 기술은 다음과 같다.
\bi
\ii Large-scale optics based interconnects
\ii Hardware and software based fault management
\ii Heterogeneous cores
\ii Another \textit{disruptive} technology
\bi
\ii Similar to what happened with \textit{cluster computing} and \textit{message passing}
\ei
\ei

\subsubsection{The Accelerated Cloud}
\textbf{Speaker:} Marc Hamilton\\Vice President, Solutions Architecture and Engineering, NVIDIA, California, USA
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.45\textwidth]{marc.png}
        \caption{Marc Hamilton, NVIDIA}
        \label{fig:marc}
\end{figure}

\noindent
\textbf{Contributions:}  NVIDIA의 Marc Hamilton 기술이사 (그림 \ref{fig:marc})가 발표한 키노트는 
클라우드 컴퓨팅 및 HPC 기술 변화에 대해 살펴보고, NVIDIA의 GPU기반 클라우드 컴퓨팅 기술들에 대해 설명해 주었다. 
그림 \ref{fig:accelerated-data-center}은 GPU가 클라우드 컴퓨팅에 필요한 현재와 미래 유즈케이스를 보여 주고 있다.
구글은 공개적으로 내부 클라우드에 있는 GPU 1000대를 이용하여 40개의 GPU 가속 어플리케이션을 실행했다고 주장했다.
또한, 기계학습의 전문가인 Andrew Ng는 \textit{"향후 5년 내에, 50\%의 질의를 위해 사용되는 입력은 음성 또는 이미지다"} 라고 말했다.
이와 같이, GPU 기반 클라우드 컴퓨팅을 가속하기 위하여 NVIDIA는 주요 병목이였던 PCIe 버스 속도를 5배 개선한 NVLink 기술을 제공하고 있다고 한다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{marc-ppt.png}
        \caption{The Accelerated Data Center - View}
        \label{fig:accelerated-data-center}
\end{figure}

\noindent
\textbf{Technology Trends:}  가트너 (Gartner)의 서버 기술에 대한 2015년 하이퍼 사이클 \cite{George:2015}에 따르면, 신경망 실리콘 (\textit{Neural network silicon})이 대규모 데이터 병렬성을 가지는 문제를 해결하는 GPU 기반 하드웨어 가속화 기술이 이와 연관된다. 신경망 실리콘은 하이퍼 사이클상에서 현재(2015년 7월) 뜨는(on rise) 기술이며, 5년에서 10년사이에 정체기(plateau)에 접어들 것으로 가트너는 예상했다.
\begin{figure*}[htb]
        \centering
        \includegraphics[width=1.0\textwidth]{gartner-server.pdf}
        \caption{Hype Cycle for Server Technologies, 2015 \cite{George:2015}}
        \label{fig:accelerated-data-center}
\end{figure*}
\bi
\ii \textbf{Neural Network Silicon:} 신경망 실리콘은 크게 깊은 신경망 (DNN; Deep Neural Network)을 이용한 기계학습 알고리즘의 성능을 향상 시킬수 있는 서버 기반 하드웨어 가속기를 의미한다. 
\ei

\noindent
\textbf{Note}  \textit{Deep Neural Networks}: "A \textit{deep neural network} (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. DNNs can model complex nonlinear relationships. The extra layers enable composition of features from lower layers, giving the potential of modeling complex data with fewer units than a similarly performing shallow network." — From the Deep Neural Networks section of "Deep Learning", Wikipedia, 3 November 2014.

2012년 구글의 연구진은 DNN 기계학습 알고리즘을 GPU에 적용하여, 대규모 데이터 셋에 대한 학습시간(training time)을 획기적으로 단축시켰다고 한다.
또한, 모바일 장치 음성 인식과 웹 이미지 태깅에서 최근 개선들은 이러한 혁신과 직결된다고 볼 수 있다.

\noindent
\textbf{Why:}  전력 소비량은 HPC 시스템 운영에 있어서 중요한 이슈중에 하나다. 딥 러닝 (Deep learning)과 같은 대규모 데이터 병렬성이 요구되는 문제에는 GPU 하드웨어 자원을 이용하는 것이 전력 소비량에서 효과적이다. 다시 말해, GPU 기반 클라우드 컴퓨팅 및 데이터 센터는 수행 성능을 개선하면서 전력 소모 비용을 감축시킬 수 있는 방법이다.

\noindent
\textbf{How:}  NVIDIA는 GPU를 활용하여 데이터센터 및 클라우드 컴퓨팅의 성능 개선에 필요한 두가지 방법을 제공하고 있다.
첫 번째 방법은 기존의 CPU와 GPU사이의 PCIe 버스 병목을 해소하기 위한 방법으로 NVLink 기술을 제공한다.
두 번째 방법으로 CUDA 프로그래머를 위해 통합 메모리 (unified memory) 접근 기법과 고수준의 병렬 코드를 위한 프로그래밍 도구를 제공하는 것이다. 예를 들어,  동일한 C++ 코드를 NVIDIA GPU 코드, x86, ARM 그리고 Power CPU 코드로 변환 할 수 있는 Thrust 라이브러리를 제공한다.
이러한 두가지 방법을 지원하는 새로운 GPU 아키텍쳐인 파스칼 (Pascal) 아키텍쳐를 소개했다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{gpu-platform.png}
        \caption{The industry's most versatile GPU-accelerated platform for the datacenter}
        \label{fig:gpu-accelerated-data-center}
\end{figure}

\noindent
\textbf{What:}  NVIDIA는 데이터 센터의 성능을 개선하기 위해 그림 \ref{fig:gpu-accelerated-data-center}과 같은 GPU 가속 기술을 적용한 플랫폼을 구축하여 운영하고 있다.
\bi
\ii Grid Workstation (Iray)
\ii GRID Virtualization (vGPU)
\ii Tesla HPC (CUDA)
\ii  Tesla Deep Learning (cuDNN)
\ii Tesla System Management and Communication
\ii  Flexible Pascal \& NVLink Architecture for the Datacenter: 파스칼 GPU는 170억개의 트랜지스터와 32GB VRAM을 갖추고 있다. (그림 \ref{fig:gpu-architecture})
\ei

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{gpu-architecture.png}
        \caption{A flexible architecture for the accelerated datacenter}
        \label{fig:gpu-architecture}
\end{figure}

NVIDIA의 파스칼 아키텍쳐의 통합 메모리, NVLink, 가상화 기술을 통한 멀티테넌시 (multi-tenancy) 기능들을 통해 기존 데이터 센터를 가속화 할 수 있을 것으로 기대한다. 
%국내 대학의 경우, 위탁연구 기관인 동명대학교에 GPU 클러스터가 구축되어 활용되고 있다.

\section{Paper Sessions}
HPCS 2015 논문 세션 중에 출장자의 논문 발표 세션과 세션 좌장 역할을 수행한 세션을 포함하여 총 6개에 세션에 참석했다. 참석한 세션은 현재 수행과제와 연관되는 처리성능 가속화를 위한 GPGPU, 데이터웨어하우스, HPC 시스템에서 자원할당,  스케쥴링, 로드 벨런싱과 연관된 셰션이였다.

\subsection{GENERAL PURPOSE GRAPHICS PROCESSING UNITS (GPGPU): : ACCELERATION, ALGORITHMS AND PERFORMANCE}

\subsubsection{A Runtime/Memory Trade-off of the Continous Ziggurat Method on GPUs}\textbf{Speaker:} Christoph Riesinger (Technische Universitat Munchen, Munich, Germany)

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{ziggurat.pdf}
        \caption{Definition of the Ziggurat: Four different regions}
        \label{fig:ziggurat}
\end{figure}

\noindent
\textbf{Contributions:} The Ziggurat method is a \textit{rejection} method to transform uniformly distributed random numbers to normally distributed random numbers.

\noindent
\textbf{Why:}  Random numbers are required in many fields.\bi\ii Cryptography\ii Monte Carlo methods\ii Simulating stochastic processes\ii Stochastic/Random differential equations
\ei

\noindent
\textbf{How:}  A \textit{transformation} function transforms this number to desired distribution, e.g. normal distribution $u^{normal}$ , by “imitating” the inverse CDF $\Rightarrow$ \textbf{Ziggurat} method 

\noindent
\textbf{What:}  A runtime/memory trade-off allows reducing the execution time of the transformation by spending more memory for larger lookup tables. Especially GPUs benefit from this trade-off since it leads to lower warp divergence. While an implementation for CPUs typically uses 128 or 256 strips, a flat recommendation for GPUs is not possible. This Ziggurat implementation is up to 19.9\% faster on Fermi and up to 24.3\% faster on Kepler.
%￼\subsubsection{Cph CT Toolbox: A Performance Evaluation}%\textbf{Speaker:} Jonas Bardino (University of Copenhagen, Copenhagen, Denmark)%%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  
\subsubsection{Acceleration of dRMSD Calculation and Efficient Usage of GPU Caches}\textbf{Speaker:} Jiri Filipovic (Masaryk University, Brno, Czech Republic)

\noindent
\textbf{Contributions:}  Using GPU caches, dRMSD is compute-bound on GPU.This research studies how the performance and code complexity change when GPU cache is used to maintain memory locality instead of (usually used) shared memory.

\noindent
\textbf{Why:}  멀티코어 CPU 환경에서 병렬 프로그래밍은 OpenMP를 이용한 컴파일러에 의존적인 방법을 이용한다. 많은 량의 데이터를 이용한 계산 집약적인 연산은 GPU를 이용하여 성능을 개선하는 것이 중요하다.

\noindent
\textbf{How:}  
RMSD (root-mean-square deviation) is used in computational chemistry to compare different structures of a molecule.
dRMSD is one possible variant of RMSD computation, it measures average difference between corresponding pairs of atoms of two molecules.
\begin{equation}
dRMSD = \sqrt{\frac{2}{n(n-1)} \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(d_{ij}^A-d_{ij}^B)^2}
\end{equation}
where $d_{ij}^A$ is the Euclidean distance between $i$-th and $j$-th atom in molecule $A$.

dRMSD algorithm used to compare different structures of a molecule. 
This paper introduces GPU acceleration, which significantly outperforms OpenMP CPU implementation.

\noindent
\textbf{What:}  
Cache can significantly improve naive implementation.However, to reach performance comparable with shared memory, several optimization steps have to be done, making the code similarly complex.
Main sources of lower performance when cache is used with compute-bound codes
\bi\ii integer instructions overhead on Maxwell 
\ii higher memory latency
\ei
\subsubsection{Optimizing Communications in multi-GPU Lattice Boltzmann Simulations}\textbf{Speaker:} Sebastiano Fabio Schifano (Universita di Ferrara and INFN, Ferrara, Italy)

다상류 혼합유체의 흐름을 파악하기 위한 시뮬레이션 방법 중 1980년대 중반에 소개된 Lattice Boltzmann Method (LB법)을 GPU상에 구현한 논문이다.

\bi
\ii Lattice Boltzmann method (LBM) is a class of computational fluid dynamics (CFD) methods\ii  simulation of synthetic dynamics described by the discrete \textit{Boltzmann} equation, instead of the \textit{Navier-Stokes} equations\ii a set of \textit{virtual particles} called \textit{populations} arranged at edges of a discrete and regular grid\ii  interacting by \textit{propagation} and \textit{collision} reproduce – after appropriate averaging – the dynamics of fluids
\ei
\noindent
\textbf{Contributions:}  
This research have discussed implementation issues of a multi-GPU LB code. This method combined use of CUDA-aware, GPUdirect and RDMA make communications among GPUs more efficient, and make code scaling over tens of GPUs.

\noindent
\textbf{Why:}  
LB법은 Navier-Stokes의 해법보다 비교적 손쉽게 상간의 경계면을 구분 지으며, 경계간의 열역학적 요소 를 동시에 시뮬레이션이 가능하게 되었다.
또한 시뮬레이션 진행은 “충돌”과 “전진” 이라는 단순개념이므로 사용자가 쉽게 프로그램 할 수 있는 장점이 있으며, 
복잡한 stencil을 사용하지 않으므로 MPI (Massage Passing Interface)화에 용이하다는 특징이 있다.
\bi
\ii All sites processed in parallel applying in sequence propagate and collide.
\ii Design efficient codes to exploit large fraction of available peak performance.
\ei

\noindent
\textbf{How:}  
Lattice 방법은 해당입자의 이동경로인 site를 정해 두고 주어진 모델에 의해 \textit{충돌}과 \textit{전진}을 반복해가며 해석을 수행하는데 본 micro적인 접근방법은 결국 macroscopic dynamic 모델화로 확장이 가능하게 된다. 
LB법은 입자 분포함수의 시간변화율을 나타내는 Boltzmann transport 방정식에 기반을 두고 있으며, 그 변화율이라고하는 것은 해당 site로 들어오는 입자수에 나가는입자수의 차로 표현하게 된다. 또한 이 방법은 완벽히 병렬화가 가능하다는 장점도 포함하고 있다. 
즉 모든 site에서는 똑 같은 계산이 주어지며, discrete 상에 주의의 node만이 계산에 참여하기 때문이다. 
따라서 대상 형상이 아무리 복잡하더라도 경계조건을 비교적 쉽게 적용 가능하며, 결국 전체적으로 빠른 계산능력을 확보할 수 있다.
\bi
\ii Lattice divided over a set of GPUs\ii GPUs virtually arranged in 1D-ring or 2D-grid\ii require an additional step PBC to update halo-columns and rows\ii  PBC is a GPU-to-GPU bi-directional memory copy\ii 1 MPI-rank per GPU
\ei

\noindent
\textbf{What:}  
MPI solutions for GPUs:
\bi\ii MPI standard libraries: OpenMPI, MVAPICH2, ...\ii CUDA-aware MPI: integration of CUDA support into MPI (use of GPU-memory pointers)\ii GPUDirect and RDMA: optimization of communications over IB-networks
\ei

\subsection{HIGH PERFORMANCE MEMORY SYSTEMS AND DISTRIBUTED STORAGE AND WAREHOUSING}
\subsubsection{Analysis of Asymmetric 3D DRAM Architecture in Combination with L2 Cache Size Reduction}\textbf{Speaker:} Alex Schoenberger (Technische Universität Darmstadt, Darmstadt, Germany)

메모리는 현재 시스템에서 이형적이며 복잡한 형태를 가지고 있다.
다이적층(die-stacking) 기술을 이용한 DRAM의 액세스 시간 및 대역폭 개선에 대한 평가는  기본적인 캐시, CPU 및 소프트웨어 구성 요소와 같은 하드웨어 구성 요소와의 상호작용에 의해 평가 될 수 있다. 본 연구에서는 이러한 평가를 위해, JPEG2000 알고리즘 실행의 인코딩과 디코딩 과정을 분석하였다. 그림 \ref{fig:sysarch}는 제안 시스템의 구조를 보여 주고 있다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{cache-arch.pdf}
        \caption{System Architecture}
        \label{fig:sysarch}
\end{figure}

\noindent
\textbf{Contributions:} 본 연구의 주요 기여한 점을 요약하면 아래와 같다.
\bi  
\ii bigger last level cache gains the system better than latency optimized DRAM
\ii fast 3D DRAM layer reduces runtime rise
\ii for particular input data amount cache accelerates the system read-ideally
\ei

\begin{figure*}[bth]
        \centering
        \includegraphics[width=1.0\textwidth]{in-memory.pdf}
        \caption{Hype Cycle for In-Memory Computing: In-memory computing (IMC) capabilities for the Hadoop와 \textit{IMC-Enabled Hybrid Transaction/Analytical Processing에 주목하자.}}
        \label{fig:in-memory}
\end{figure*}

\noindent
\textbf{Why:} 인-메모리 컴퓨팅 (in-memory computing; IMC) \cite{Massimo:2014}에서 대용량의 데이터처리가 요구되는 경우, 메모리 액세스 시간과 대역폭 개선은 중요한 이슈다. 
따라서, 본 연구는 in-memory 기반의 빅데이터 분석 플랫폼 운영시 활용 가능한 방법이다. 
그림 \ref{fig:in-memory}와 같이 IMC는 Peak of Inflated Expectations 구간에 와 있으며, 5년에서 10년사이에 정체기에 도달할 것으로 예상하고 있다. 

IMC enables architects and developers to overcome the technical issues that have so far prevented large-scale HTAP implementations. Technologies such as in-memory DBMSs (IMDBMSs) and in- memory data grids (IMDGs) make it possible to unify operational and analytical databases in a single low-latency-access, in-memory data store that can process high volumes of transactions while at the same time supporting, in real time, analytical queries at any level of granularity.

\noindent
\textbf{How:} 일반적으로 데이터 접근 속도를 향상시키기 위해 지역성 (locality)을 최대한
 활용한다. 또한, 데이터 저장위치를 관리하는 알고리즘을 적용함으로써 접근 성능을 최적화한다.
\bi   
\ii based on locality principle
\ii additionally usable with other concepts
\ii optimisation step is relocated into software
\ei

\noindent
\textbf{What:}  
최근 대두되고 있는 IMC-Enabled Hybrid Transaction/Analytical Processing 기술에도 큰 기여를 하리라 기대한다. 이와 관련된 벤더로는 Aerospike, GigaSpaces Technologies, GridGain, MemSQL, SAP, ScaleOut Software가 있다.

\noindent
\textbf{Technology Trends:} 기술 트렌드 측면에서 IMC는 기존 디스크기반 하둡 에코 시스템을 보완할 수 있는 기술분야 대두되고 있다. 또한, 실시간 트랜잭션에서 발생하는 이상현상 (anomaly)이나 부정사용 탐지 (fraud detection)는 스트림 컴퓨팅 기술 (stream computing)과 연계되어야 할 것이다.

\begin{Def}
\textbf{IMC-Enabled Hybrid Transaction/Analytical Processing} - An \textit{in-memory computing} (IMC)-enabled hybrid transaction/analytical processing (HTAP) architecture leverages IMC techniques and technologies to enable concurrent analytical and transaction processing on the same (typically in-memory) data store. By removing the latency of moving data from operational databases to data warehouses and data marts for analytical processing, this architecture enables real-time analytics and situation awareness on live transaction data as opposed to after-the-fact analysis on stale data (as in traditional approaches).
\end{Def}

또 하나 주목해야 할 것은 IMC-Enabled Capabilities for Hadoop 이다. 
그림 \ref{fig:in-memory}를 참조하면, 현재 IMC-Enabled Capabilities for Hadoop 분야는 뜨는 (on rising) 분야로 가트너에서는 분석하고 있다.
현재 수행하고 있는 KIWI 과제와의 연계성으로 볼 때, 차기 프로젝트는  \textbf{IMC-Enabled Capabilities for Hadoop} 기술과 관련된 프로젝트를 연결하는 것이 바람직하다고 생각한다. 

\begin{Def}
\textbf{In-memory computing (IMC) capabilities for the Hadoop} - \textit{In-memory computing (IMC) capabilities for the Hadoop} stack is a category that groups together all in-memory computing technologies complementing the Hadoop ecosystem. These various capabilities include in-memory analytics, in-memory data grid and in-memory SQL access, as well as in-memory support for streaming data.
\end{Def}
%\subsubsection{Tracing Long Running Applications: A Case Study Using Gromacs}%\textbf{Speaker:} Michael Wagner (Center for Information Services and High Performance Computing (ZIH), Technische Universitat Dresden, Dresden, Germany)

\subsubsection{Advanced Commands and Distributed Data Layout to Enhance the SSD Internal Parallelism}\textbf{Speaker:} Soraya Zertal (PRiSM, Universite de Versailles, Versailles, France)

대용량 저장능력 뿐만 아니라 고속 데이터 접근 성능도 제공해야 하는 것이 데이터 집약적인 응용프로그램이나 고성능 컴퓨팅에서 중요한 요구사항이다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{ssd_arch.pdf}
        \caption{SSD Hierarchical Architecture}
        \label{fig:ssdarch}
\end{figure}

\noindent
\textbf{Contributions:}  SSD는 고성능 컴퓨팅에 적절하면서 경쟁력 있는 기반을 만들어 준다. 또한, 데이터 레이아웃 방식은 SSD 내부 병렬처리를 위한 자본이다. 다음은 본 연구의 주요한 기여 포인트다.
\bi
\ii A “carefull” use of advanced commands improve performance and preserve endurance\ii Considering the external SSD parallelism in very large SSD-based storage systems\ii Considering real tests and comparison with simulation results
\ei

\noindent
\textbf{Why:}  본 논문의 주요한 연구동기 (motivation)는 다음과 같다.
\bi
\ii HDD optimizations are obsolete for SSDs (\textit{no moving part, no-write-in-place, erase before write, write cycles limitation})\ii SSD density is inversely proportional to its \textit{performance} $\Rightarrow$ use the SSD external and \textit{internal parallelism}\ii  reduced SSD \textit{endurance} (1000 write cycles), to preserve!
\ei

\noindent
\textbf{How:} SSD 데이터 액세스 속도 개선을 위해, 다음과 같이 진보된 명령어를 추가하였다.
\bi 
\ii \textbf{Copy back:} data page migration within a plane.\\
Different \#page but the same parity $\rightarrow$ \textbf{CBrule}\ii \textbf{Multi-plane:} operations in parallel on planes within a die.\\Only the \#plane are different $\rightarrow$ \textbf{MPrule}\ii \textbf{Interleave:} multiple basic operations in parallel on dies within a chip.\\ 
Same \#chip and different \#die $\rightarrow$ \textbf{Irule}
\ei
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{ssd-commands.pdf}
        \caption{SSD Advanced Commands}
        \label{fig:ssdcomm}
\end{figure}

\noindent
\textbf{What:} 본 연구의 산출물은 새로운 명령어가 추가된 SSD 및 HPC 응용프로그램으로 다음과 같다.
\bi  
\ii SSD: Hierarchy of independent components: channels, packages, dies, chips, planes\ii HPC applications: set of parallel tasks/processes running in parallel \\$\Rightarrow$ Excellent matching between the SSD internal structure and the concurrency nature of HPC applications
\ei

\subsection{HPCS 2015 POSTER PAPERS}
호텔 로비에서 포스터 세션이 진행되었고, 빅데이터 스트림 처리, 네트워크 라우팅, GPGPU, 분산/클라우드 컴퓨팅을 주제로 한 포스터는 아래와 같이 6개의 논문이 발표되었다.  
\be
\ii Real-time Signal Identification in Big Data Streams Bragg–Spot Localization in Photon Science\ii A Resilient Routing Approach for Mobil Ad Hoc Networks\ii Efficient Asian Option Pricing with CUDA\ii GPGPU Performance Evaluation of Some Basic Molecular Dynamics Algorithms\ii Cookery: A Framework for Developing Cloud Applications\ii Subordination: Cluster Management without Distributed Consensus
\ee

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{posters.png}
        \caption{HPCS 2015 Poster Papers}
        \label{fig:posters}
\end{figure}

\subsection{HIGH PERFORMANCE MOBILE ARCHITECTURES, WIRELESS NETWORKS AND APPLICATIONS}
%\subsubsection{GPU Accelerated Ray Launching for High-Fidelity Virtual Test Drives of VANET Applications}%\textbf{Speaker:} Manuel Schiller, (Lehrstuhl fur Echtzeitsysteme und Robotik, Technische Universitat Munchen, Germany)
%
%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  
출장자의 논문은 오전 8시부터 시작하는 이번 세션에서 발표하였다.  발표전, 세션 좌장들과 발표자들이 함께 만나, 아침식사를 함께하며 이야기를 나누는 것이 인상에 남는다.
\subsubsection{A Collaboration Middleware for Service Scalability in Peer-to-Peer Systems}\textbf{Speaker:} Sung-Soo Kim (ETRI, Daejeon, South Korea)

필자가 발표한 논문으로 피어투피어 시스템에서 다양한 응용 어플리케이션간의 협업을 지원할 수 있는 미들웨어에 대해 발표했다.
%발표 후, 아래와 같이 2가지 질문이 있었다.
%
%\bi
%\ii \textbf{Question 1:} 미들웨어에서 관리하고 있는 논리적 앱 라이프 사이클이 안드로이드의 Activity 라이프 사이클과는 어떻게 다른가?\\
%\textbf{Answer 1:} 논리적 앱 라이프 사이클은 동일 네트워크에 있는 모든 디바이스들이 함께 공유하는 것이다. 하지만, 안드로이드 액티비티 라이프 사이클은 각각의 디바이스에서 독립적으로 관리된다. 논리적 앱에서 앱이 특정 디바이스에서 다른 디바이스로의 이주 (Migration)를 하는 경우, 이주에 대한 라이프 사이클 상태가 있다. 하지만, 안드로이드 액티비티 라이프 사이클에는 단일 디바이스에 대한 상태이므로, 이주 (Migration) 상태가 없다.
%\ii \textbf{Question 2:} 앱간 연관성 정보 및 앱간 메세지 정보에 대한 보안은 어떻게 처리했는가?\\
%\textbf{Answer 1:} 메세지 암호화 및 정보 보안에 관련된 내용은 현재 구현되어 있지 않지만, 향후 연구로 계획되어 있다. 메세지 암호화는 AES (Advanced Encryption System)을 적용하여 구현할 예정이다.
%\ei

\noindent
\textbf{Contributions:} 피어투피어 시스템에 존재하는 멀티스크린 디바이스의 앱관 연관성, 콘텐츠간 연관성 스키마 정의언어를 이용하여, 협업을 제공하는 서비스를 구성할 수 있으며, 대칭형 협업 (symmetric collaboration)을 제공하는 최초의 미들웨어를 설계/구현하였다. 그림 \ref{fig:middleware-architecture}d은 우리 논문에서 제안한 협업미들웨어 구조와 서비스 플랫폼 구조를 보여주고 있다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{middleware.pdf}
        \caption{A architecture for the collaboration middleware}
        \label{fig:middleware-architecture}
\end{figure}

\noindent
\textbf{Why:} 기존 스마트 디바이스간 협업을 위해서는 특정 디바이스(예; 스마트 TV)를 중심으로 한 마스터 슬레이브 형태의 계층적 협업만을 제공할 수 있었다. 스마트 디바이스가 각각이 동등한 역할로 피어투피어 형태의 대칭협 협업과 서비스 확장성을 제공하는 것이 중요하다. 

\noindent
\textbf{How:}  논문의 협업 미들웨어는 세가지 구성요소인 서비스 개발자를 위한 N-스크린 어플리케이션 라이브러리, 최종 사용자를 위한 협업 위젯, 그리고 각 스마트 디바이스에 설치되는 협업 에이전트를 포함하고 있다. 미들웨어는 협업 기반의 N-스크린 서비스를 개발에 필요한 공통적인 기능들; 스마트 디바이스 관리, 응용프로그램 및 컨텍스트 관리, 협업세션 관리, 앱 생명주기 관리 등의 기능을 제공한다.

\noindent
\textbf{What:} 미들웨어는 크게 두가지 계층으로 나누어 진다. 서비스 개발자가 새로운 서비스 개발을 위한 N-스크린 어플리케이션 라이브러리 (NSAL)과 디바이스 협업 미들웨어 (collaboration agent)를 설계하고 개발했다. 각 디바이스의 어플리케이션 및 어플리케이션 컨텍스트 정보는 N-스크린 서비스 클라우드 (N-Screen service cloud)가 관리한다. 본 연구의 주요 산출물은 아래와 같다.
\bi
\ii N-Screen Service Cloud
\ii N-Screen Application Library
\ii Collaboration Agent
\ei

그림 \ref{fig:session}은 개발한 미들웨어를 이용하여, 끊김없는 협업 서비스를 제공하기 위해 필요한 협업세션 관리 기능을 보여주고 있다.
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{session.pdf}
        \caption{Collaboration Session Management}
        \label{fig:session}
\end{figure}

\noindent
\textbf{Q \& A:} 발표 후, 아래와 같이 2가지 질문이 있었다.
\bi
\ii \textbf{Question 1:} 미들웨어에서 관리하고 있는 논리적 앱 라이프 사이클이 안드로이드의 Activity 라이프 사이클과는 어떻게 다른가?\\
\textbf{Answer 1:} 논리적 앱 라이프 사이클은 동일 네트워크에 있는 모든 디바이스들이 함께 공유하는 것이다. 하지만, 안드로이드 액티비티 라이프 사이클은 각각의 디바이스에서 독립적으로 관리된다. 논리적 앱에서 앱이 특정 디바이스에서 다른 디바이스로의 이주 (Migration)를 하는 경우, 이주에 대한 라이프 사이클 상태가 있다. 하지만, 안드로이드 액티비티 라이프 사이클에는 단일 디바이스에 대한 상태이므로, 이주 (Migration) 상태가 없다.
\ii \textbf{Question 2:} 앱간 연관성 정보 및 앱간 메세지 정보에 대한 보안은 어떻게 처리했는가?\\
\textbf{Answer 1:} 메세지 암호화 및 정보 보안에 관련된 내용은 현재 구현되어 있지 않지만, 향후 연구로 계획되어 있다. 메세지 암호화는 AES (Advanced Encryption System)을 적용하여 구현할 예정이다.
\ei
%\subsubsection{Experiments in Fair Scheduling in 4G WiMAX and LTE}%\textbf{Speaker:} Junaid Ahmed Zubairi (State University of New York at Fredonia, New York, USA)
%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  

\subsection{IMAGE PROCESSING, MACHINE LEARNING, PATTERN RECOGNITION \& APPLICATIONS}

이 세션은 이번 학회에서 출장자가 세션 좌장을 맡았던 세션이다 (그림 \ref{fig:chair}). 개인적으로 국제학회의 좌장 역할을 처음 수행해 봤고, 좋은 경험을 했다고 생각한다. 본 세션에서는 총 4편의 논문이 발표되었고, 그 중 딥 러닝 (deep learning)을 연구 주제로 한 논문에 대해 정리한다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{chair.png}
        \caption{Session Chair 역할은 좋은 경험이었다.}
        \label{fig:chair}
\end{figure}

%\subsubsection{A Reduced Complexity Instruction Set Architecture for Low Cost Embedded Processors}%\textbf{Speaker:} Mabo Ito (The University of British Columbia - Vancouver, British Columbia, Canada)
%%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  
\subsubsection{Deep Learning with Shallow Architecture for Image Classification}\textbf{Speaker:} Asma ElAdel (National School of Engineers of Sfax, Sfax, Tunisia)

\noindent
\textbf{Contributions:}  본 논문은 딥러닝에서 다중 해상도 분석 (MRA)과 Adaboost 알고리즘에 기반한  \textit{deep shallow neural network} 구조를 제안하였다.  신경망의 입력 레이어와 히든 레이어 사이의 가중치를 MRA를 이용하여 서로다른 스케일로 계산한다. 또한, 히든 레이어와 출력 레이어 사이의 가중치는 Adaboost 알고리즘을 사용함으로써 계산한다. 본 논문 결과를 통해 그림 \ref{fig:neuralnetwork}와 같이 신경망의 깊이를 단순화 하면서도 좋은 이미지 분류 결과를 얻을 수 있다고 주장한다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{neural-network.png}
        \caption{Huge Neural Network vs. Simplified Neural Network}
        \label{fig:neuralnetwork}
\end{figure}

\noindent
\textbf{Why:}  본 논문에서 학습단계에서 Adaboost를 이용하여 특징을 선택 (feature selection)하였다. Adaboost 방법을 다음과 같은 이유 때문에 딥러닝의 학습단계에서 중요하다.
\bi
\ii Improves classification accuracy
\ii Can be used with many different classifiers
\ii Commonly used in many areas
\ii Simple to implement
\ei

\noindent
\textbf{How:}  Adaboot 방법은 다음과 같이 절차를 통해 처리된다.
\be
\ii Each training sample uses a weight to determine the probability of being selected for
    a training set.
\ii Adaboost is an algorithm for constructing a \textit{strong} classifier as linear combination of \textit{weak} classifiers.
\ii Final classification based on weighted vote of weak classifiers
\ee

\noindent
\textbf{What:} 다중 해상도 분석 (MRA)과 Adaboost 알고리즘에 기반한  deep shallow neural network 구조를 제시하고 테스트 이미지 데이터베이스에 대한 성능실험 결과를 제시하였다.

%\subsubsection{An Efficient Implementation of Fuzzy Edge Detection Using GPUs in MATLAB}%\textbf{Speaker:} Farnaz Hoseini (Islamic Azad University, Rasht, Iran; University of Guilan, Rasht, Iran)
%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  

%\subsection{RESOURCE ALLOCATION, SCHEDULING, AND LOAD BALANCING IN HPC SYSTEMS}
%\subsubsection{Market-inspired Dynamic Resource Allocation in Many-core High Performance Computing Systems}%\textbf{Speaker:} Amit Kumar Singh (University of York, York, U.K.)%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  
%\subsubsection{234 Scheduling of 3-2 and 2-1 Eliminations for Parallel Image Compositing using Non-Power-of-Two Number of Processes}%\textbf{Speaker:} Jorji Nonaka (RIKEN Advanced Institute of Computational Science, Kobe, Japan; Light Transport Entertainment, Inc., Tokyo, Japan)
%%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  
%%\subsubsection{In Search of the Best MPI-OpenMP Distribution for Optimum Intel-MIC Cluster Performance}%\textbf{Speaker:} Gladys Utrera (Universitat Politecnica de Catalunya-Barcelona, Barcelona, Spain)
%
%\noindent
%\textbf{Contributions:}  
%
%\noindent
%\textbf{Why:}  
%
%\noindent
%\textbf{How:}  
%
%\noindent
%\textbf{What:}  

% Example Figure
%%%%%%%%%%%%%
%\begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.48\textwidth]{system-overview.pdf}
%        \caption{A system overview with CPU and a discrete GPU.}
%        \label{fig:system_overview}
%\end{figure}


\section{People to People}
금번 학회 논문 채택률은 37\%로 45편의 full paper가  채택되었고, 6개 관련워크샵 참석자들을 합쳐 180여명이 참석하였다. 논문을 제출한 국가별 비율을 살펴보면 다음과 같다. 모두 5개 대륙에서 41 개국에서 논문 심사를 위해 투고했다고 한다. 그 중 논문 투고수가 많은 상위 5개국은 이탈리아 11.76\%, 독일 10.00\%, 인도 7.65\%, 미국 7.05\%, 영국 6.47\% 순이라고 한다.

한국에서 참석한 사람은 총 3명이지만, 파키스탄에서 온 2명의 UST 학생 (ETRI, KISTI)을 빼면 순수 한국인은 출장자 뿐이였다. ETRI 휴먼컴퓨팅연구실 소속인 UST 학생 사자드도 논문 발표를 위해 참석하였다 (그림 \ref{fig:people} 오른쪽 위 이미지).

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{people.png}
        \caption{People in the HPCS 2015}
        \label{fig:people}
\end{figure}

학회 중간 coffee break, 리셉션에서 학회에 참석한 연구자들과의 교류도 즐거웠다. 미국 NSF 소속인 Chtchelkanova 박사를 만나 이야기를 나누었다 (그림 \ref{fig:namecards00}). 그녀는 현재 미국의 컴퓨팅과 통신 인프라와 관련된 연구 주제선정과 펀딩을 담당하고 있었다. 이와 관련하여 현재 미국에서는 저전력 고성능을 제공하는 데이터센터에 대한 투자를 본격화하고 있다고 했다. 특히, 이번 IBM, NVIDIA, 그리고 Stanford, UC.Berkeley와 같은 미국의 유수 대학이 함께 추진하는 대형 프로젝트에 대해서도 열정적으로 설명해 주는 모습이 인상적이었다.
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.48\textwidth]{nc01.png}
        \caption{교류한 연구자 명함 [NSF(미국)]}
        \label{fig:namecards00}
\end{figure}

학회 개최지인 암스테르담 교수, 이탈리아 교수, 미국 교수, 크로아티아 연구원, 아랍 에미레이트 교수, 이란 교수, 독일 박사과정 학생 등 총 30여명과 학회 기간 동안 만나 이야기를 나누었다 (그림 \ref{fig:namecards01}, Appendix 그림 \ref{fig:namecards02}).

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.4\textwidth]{nc02.png}
        \includegraphics[width=0.4\textwidth]{nc03.png}
        \caption{교류한 연구자 명함 [교수(네델란드, 이탈리아)]}
        \label{fig:namecards01}
\end{figure}

%\subsection{학회장소 및 숙소}
HPCS 2015 학회 장소인 힐튼 호텔 (그림 \ref{fig:map})은 암스테르담 도시 중심부에서 약간 떨어진 위치에 있지만, 트램, 버스와 같은 대중 교통을 이용하면 불편없이 다닐 수 있는 위치였다. 숙소는 담 광장 근처에 있는 호텔 암스테르담 (Hotel Amsterdam)에서 보냈다.

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.4\textwidth]{map.png}
        \caption{학회장소 (Hilton Amsterdam Hotel) 위치}
        \label{fig:map}
\end{figure}

\section{결론}
이번이 13번째 개최된 HPCS 2015 학회는 규모는 크지 않지만, HPC, 클라우드 컴퓨팅, 빅데이터와 관련된 다양한 분야의 알고리즘과 시스템에 대한 연구결과를 접할 수 있었다.
이번 학회에서 과제 연구개발 수행시 참고가 되는 HPC와 인-메모리 컴퓨팅 기반 기술을 획득하는데 도움을 얻을 수 있었다. 출장을 통해 아래 주요한 기술 키워드들은 얻을 수 있었다.
\bi
\ii In-memory computing (IMC) capabilities for the Hadoop
\ii IMC-Enabled Hybrid Transaction/Analytical Processing
\ii Science Gateways
\ii Accelerated Cloud
\ei

출장자의 연구관심분야와 다른 분야인 라우팅 알고리즘 등과 같은 네트워크 기술에 관한 연구는 다소 지루하고 이해하기 힘들었던 부분이 없잖아 있었지만, 반면에 HPC 역사, GPU 가속 데이터 센터, 그래프이론을 접목한 Graph Data, GPU를 활용한 시뮬레이션 처리 가속화 등에 대한 연구결과가 여럿 눈에 띄어서 새로운 연구주제를 파악할 수 있었다.

유전자 시스템에 대한 수학적 연구에서 피셔(R. A. Fisher)는 피셔의 기본 정리 (Fisher's Fundamental Theorem)라는 법칙을 만들었다. 사실, 이 법칙은 피셔가 만든 다른 이론들의 토대가 되는 매우 중요한 법칙이다. 아무튼, 이 법칙에 따르면, 한 유전자 시스템이 특정 환경에 많이 적응되어 있을 수록 다른 새로운 환경에 적응하기는 더 어렵다. 우리 ETRI 구성원도 주어진 연구 현실에 급급해 내부 연구 환경에만에 적응하는 오류를 범하지 않았으면 하는 것이 필자의 바람이다. 학회참석을 위한 국외출장이 논문발표든 단순참석이든 연관된 학회에 적극 참여하고 다양한 생각들을 열정과 열린 자세로 받아들이는 것이 평범한 일상이 되리라 기대해 본다.

끝으로, 이번 학회참석을 통해 다양한 분야의 국제 전문가 및 교수님들을 함께 이야기 나누게 되어 즐거웠고, 
HPC, 대규모 분산 컴퓨팅과 빅데이터 관련 연구분야의 다양한 연구교류를 갖게 되어 좋았다. 
열네번째인 다음번 학회 HPCS 2016은 오스트리아 빈에서 개최될 예정이다.
\bibliographystyle{abbrv}
\bibliography{sqlonhadoop}
\section*{Appendix}
\begin{figure}[htb]
        \centering
        \includegraphics[width=0.36\textwidth]{nc04.png}
        \includegraphics[width=0.36\textwidth]{nc05.png}
        \includegraphics[width=0.36\textwidth]{nc06.png}
        \includegraphics[width=0.36\textwidth]{nc07.png}
        \caption{교류한 연구자 명함}
        \label{fig:namecards02}
\end{figure}
\end{document}
