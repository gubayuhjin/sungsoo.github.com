
\documentclass{sig-alternate-05-2015}

%%%%%%%%%%%%%%%%%%%%%%%%%
%                              My Commands
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\ii}{\item}
\newtheorem{Def}{Definition}
\newtheorem{Lem}{Lemma}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{graphicx}
\graphicspath{%
        {converted_graphics/}
        {./images/}
}
\usepackage{times}

% 				End of My Commands
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{BIGDAS 2015}{Jeju Island, Republic of Korea}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Flying KIWI: Design of Approximate Query Processing Engine for Interactive Data Analytics at Scale }

\numberofauthors{1} 

\author{
% 1st. author
\alignauthor
Sung-Soo Kim, Taewhi Lee, Moonyoung Chung and Jongho Won \\
       \affaddr{Electronics and Telecommunications Research Institute (ETRI)}\\
       \affaddr{218 Gajeong-ro, Yuseong-gu, Daejeon}\\
       \affaddr{South Korea}\\
       \email{\{sungsoo, taewhi, mchung, jhwon\}@etri.re.kr}
}

% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
This paper introduces the design of hybrid SQL-on-Hadoop system, which supports \textit{dual-mode} (interactive and deep) analytics. %: \textit{KIWI}.
We present an architecture of approximate query processing engine using \textit{horizontal} and \textit{vertical} sampling of the original database for interactive big data analytics. 
A key novelty of our approach is that we can support interactive analytics as well as deep analytics at scale in the \textit{unified} manner.
%The experimental results demonstrate that our system can provide improved performance in terms of  query processing on large scale datasets.
\end{abstract}


%\ccsdesc[500]{Information systems~Data management systems}
%\ccsdesc[300]{Data management systems~Query optimization}
\ccsdesc[200]{Data management systems engines~Parallel and distributed DBMSs}
\ccsdesc[100]{Parallel and distributed DBMSs~Relational parallel and distributed DBMSs}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{SQL-on-Hadoop, Approximate Query Processing; Big Data Analytics; Random Sampling}

\section{Introduction}
A \textit{data warehouse} (DW) is the collection of processes and data whose overarching purpose is to support the business with it analysis and decision-making. 
The term "Big Data" refers to the continuing massive expansion in the data \textit{volume} and \textit{variety} as well as the \textit{velocity} and \textit{veracity} of data processing.
SQL-on-Hadoop is a class of "Big Data" analytics systems that combine established SQL-style querying with Hadoop-based data warehouse.
SQL-on-Hadoop systems
include Impala, Presto,  Tajo, Impala, Drill, Hadapt, HAWQ, BigSQL, Stinger, and Hive on Tez \cite{Floratou:2014}.

For \textit{interactive queries} that require a few seconds (or even milliseconds) of response time, MapReduce (MR) is the wrong choice, since MR is often suitable for \textit{batch processing}. 
In order to provide an effective environment for big data analysis, we believe that SQL-on-Hadoop systems will need to support both \textit{interactive} analytics and \textit{deep} analytics based on massively \textit{batch} processing. 
\textit{KIWI} is a  SQL-on-Hadoop system, which runs on hundreds of machines in existing Hadoop cluster.
It is decoupled from the underlying storage engine, unlike traditional database management systems.

Given a database $\mathcal{D}$ and a query $\mathcal{Q}$, let $\mathcal{Q}(\mathcal{D})$ denote the exact answer of evaluating $\mathcal{Q}$ on $\mathcal{D}$. 
In approximate query processing (AQP) \cite{Agarwal:2014, Agarwal:2013}, an approximate answer can be obtained by (i) extracting from $\mathcal{D}$ a random sample $\mathcal{D}_s$, % \cite{Babcock:2003}, 
(ii) evaluating a potentially modified version of $\mathcal{Q}$ (say $q$) on $\mathcal{D}_s$, and (iii) using $q(\mathcal{D}_s)$ with error bars as an approximation of $\mathcal{Q}(\mathcal{D})$.
Generally, error estimation in AQP can be categorized into two main approaches, such as \textit{closed-form} and \textit{bootstrap} \cite{Chaudhuri:2007}.
Closed-form error estimation is often used for common aggregate functions in a SQL query, such as \texttt{COUNT, SUM, AVG}, and so on. 
Bootstrap consists in a simple Monte Carlo procedure, however, this requires the computational overhead. 
\textit{Analytical bootstrap} has been proven equivalent to the simulation-based bootstrap, but avoids the overhead. 

The ultimate goal of our work is to provide a SQL-on-Hadoop system, which can support interactive analytics as well as deep (batch) analytics. \textit{Flying KIWI} is an AQP engine in order to support interactive analytics. Flying KIWI provides an approximate answer with an error bar, which is calculated by error estimation. \\

\noindent
\textbf{Main contributions:} 
The contributions of our work can be summarized as follows. 
\bi
\ii \textit{Dual-Mode Analytics}: 
The proposed SQL-on-Hadoop system supports \textit{interactive} analytics as well as MR-based \textit{batch} processing in the unified KIWI architecture.
\ii \textit{Approximate Query Processing}: 
We introduce a progressive error estimation method in order to support online aggregation using the \textit{Flying KIWI}.
\ei

\section{System Architecture}
This section focuses on the overall architecture for approximate query processing engine 
and describes two main components in the system architecture. 
Figure \ref{fig:architecture} shows the high-level architecture of our system, which can be divided into two main blocks, such as \textit{query compilation} block and \textit{query execution} block.\\

\noindent
\textbf{Query Compilation Block:} 
This block performs transparently compiling, checking and rewriting the query to support error estimation using closed forms or analytical bootstrap. The compiler generates a query plan expressed in relational algebra from the a given SQL query. 
The query rewriter takes into consideration the user-specified quality measures, and rewrites the plan into a new logical query plan according to the logical optimization rules. 

Sampling refers to the commonly used technique of evaluating the queries from a small random sample of the original database. In order to create samples, the sampling construction module exploits two sampling methods, such as, \textit{horizontal} sampling (or row sampling) and \textit{vertical} sampling (or column sampling).

\noindent
\textbf{Query Execution Block:} 
When a query arrives at runtime, it is re-written to run against the sample tables instead of the original database.
The execution block evaluates the query augmented with sample selection operations at runtime, 
and providing the accuracy measures in user-specified metrics. 

\begin{figure}[htb]
        \centering
        \includegraphics[width=0.412\textwidth]{sys-architecture.pdf}
        \caption{Flying KIWI Architecture.}
        \label{fig:architecture}
\end{figure}

\subsection{Sample Construction and Selection}
Given a table $T$ with $r$ rows $R_1, ..., R_n$ and $c$ columns $C_1, ..., C_m$, 
in horizontal sampling, let $S_h=\{R_i, R_{i+1}, ..., R_{i+l}\}$, where $ i \leq i+l \leq r$, denote a \textit{row set}  that consists of $l$ rows in $T$. Our system can construct various row sets according to  sampling algorithms such as stratified sampling \cite{Chaudhuri:2007} or Monte-Carlo sampling with user-specified sampling rate.
In vertical sampling, let $S_v=\{C_j, C_{j+1}, ..., C_{j+k}\}$, where $ j \leq j+k \leq c$, denote a \textit{column set} that consists of $k$ columns in $T$. 
A query $q$ often need to scan fully or partially all data items in a row set or a column set $S_q$ of $T$. 
If data items in $S_q$ have been materialized, for $q$, need to scan only materialized items instead of full table $T$. 
In case of column sampling, because the number of columns in $S_q$ is often much smaller than $c$, scanning would be done much faster.

When queries are issued at runtime, the query execution block rewrites the queries to run against sample tables rather than the original tables referenced in the queries.
Let $\xi(T, S)$ be the memory space needed to store all data items in a column set $S$ of a table $T$.
Let $\varphi$ be the storage system's space limit for materialized column sets.
Let $\omega$ be possible column sets of table $T$. 
The sum of the memory space of possible column sets, $\sum_{\forall S_i \in \omega} \xi(T, S)$ is exponentially large.
Let $Q_p$ is the set of queries issued in the past.
Let $\mathcal{V}(T, S_i)$ be the value obtained for future queries if $S_i$ is materialized.\\

\noindent
\textbf{Problem Definition:}  Given a table $T$ and a query $Q$, find a collection of optimal column sets, $S_{opt} = \{S_1, ..., S_k\}$ consisting of $k$ column sets, such that $\sum_{\forall S_i \in \omega} \xi(T, S_i) \leq \omega $ and $\mathcal{V}_{opt} = \sum_{\forall S_i \in \omega} \mathcal{V}(T, S_i) $ is maximized.\\

\noindent
\textbf{Our Approach:}
From the set of historical queries $Q_h$ extracts a set of distinct column sets $S_{a}$ that appear in $Q_h$.
$\forall S_i \in S_{a}$, compute $\xi(T, S_i)$, remove from $S_{a}$ if $\xi(T, S_i) > \omega$.
$\forall S_i \in S_{a}$, compute the appearance frequencies $f(S_i)$, remove from $S_{a}$ if $\xi(T, S_i) > \omega$.
Let $n$ be the number of column sets in $S_a$.
For an arbitrary column set $S$, $\xi(T, S)$ can be approximated as:
\begin{equation}
\xi(T, S) = r \times \sum_{i=1}^{|S|} \mathcal{I}(C_i)
\end{equation}
where $r$ denotes the number of rows in $T$, 
$|S|$ denotes the number of columns in $S$ 
and $\mathcal{I}(C_i)$ denotes the average size of a data item in $C_i$ 
(e.g., if data type of $C_i$ is double, then $\mathcal{I}(C_i)$ is 8 bytes).
\begin{algorithm}
\caption{Find optimal column sets ($S_{opt}$).}
\label{algo:columnset}
\begin{algorithmic}[1]
	\Procedure{FindOptimalColumnSets}{$S_a$, $Q$}
	\State List<$S$> $L_{s}$ = \textit{constructColumnSets}($S_a$, $Q$);
	\State $L_{s}$.\textit{sortByAppreanceFrequency}(DESENDING);
	\For{each node $S_j \in L_{s}$ } 
	\If {$\sum_{\forall S_i \in S_{opt}} \xi(T, S_i)  + \xi(T, S_j) > \omega $}
	\Return
	\Else 	
	\State $S_{opt}$.\textit{add}($S_j$); 
	\State $S_a$.\textit{remove}($S_j$); 
	\EndIf
	\EndFor
	\EndProcedure
\end{algorithmic}
\end{algorithm}

The appropriate sample table(s) to use for a given query $Q$ are determined by comparing $Q$ with the metadata annotations for the samples.  \textbf{Algorithm \ref{algo:columnset}} shows how optimal column sets, $S_{opt}$, can be evaluated progressively for a given query $Q$. The time complexity of this algorithm is $O (n \log n)$, where $n$ is the size of the database.

\section{Conclusion}
We present a hybrid SQL-on-Hadoop system architecture, which supports approximate query processing. 
Our main contribution in this work has been to propose a new unified approach for supporting \textit{dual-mode} (interactive and deep) analytics at scale.

Our work concludes with the following take-away messages:
(1) It is beneficial to have an \textit{unified} query processing engine in the KIWI SQL-on-Hadoop system,
(2) \textit{Flying KIWI} is a general purpose system that implements error estimation features of approximate answers for interactive analytics, and (3) the sampling methods are intuitive to use.

\section*{Acknowledgments}
This work was supported by ETRI R\&D program ("\textit{Development of Big Data Platform for Dual Mode Batch Query Analysis}, 15ZS1400") funded by the government of South Korea.

\small{
\bibliographystyle{abbrv}
\bibliography{sqlonhadoop}
}

\end{document}
